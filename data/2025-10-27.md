<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 3]
- [cs.NI](#cs.NI) [Total: 1]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Lincoln AI Computing Survey (LAICS) and Trends](https://arxiv.org/abs/2510.20931)
*Albert Reuther,Peter Michaleas,Michael Jones,Vijay Gadepally,Jeremy Kepner*

Main category: cs.DC

TL;DR: Annual update of the Lincoln AI Computing Survey (LAICS) cataloging publicly announced commercial AI accelerators with peak performance and power numbers, plotted on performance-vs-power scatter plots, with market-segment zooms, short descriptions of new devices, and a new architecture categorization.


<details>
  <summary>Details</summary>
Motivation: Recent surge of interest in generative AI has increased focus on training and inference hardware; the authors aim to update a multi-year survey to reflect new commercial accelerators and architectural trends.

Method: Collect peak performance and peak power consumption for publicly announced commercial accelerators; plot these on scatter graphs; highlight market segments and produce zoomed views; add brief device descriptions and introduce a new classification of computing architectures used by the accelerators.

Result: An expanded dataset of accelerators for the past year, updated scatter plots showing device trends across performance and power dimensions, segmented/zoomed plots for market niches, and a new taxonomy of accelerator architectures; qualitative observations and analysis of trends.

Conclusion: Provides an updated, visual comparative resource for AI accelerators, useful to researchers and practitioners tracking hardware trends for GenAI training and inference; introduces a new architecture categorization to aid comparison.

Abstract: In the past year, generative AI (GenAI) models have received a tremendous
amount of attention, which in turn has increased attention to computing systems
for training and inference for GenAI. Hence, an update to this survey is due.
This paper is an update of the survey of AI accelerators and processors from
past seven years, which is called the Lincoln AI Computing Survey -- LAICS
(pronounced "lace"). This multi-year survey collects and summarizes the current
commercial accelerators that have been publicly announced with peak performance
and peak power consumption numbers. In the same tradition of past papers of
this survey, the performance and power values are plotted on a scatter graph,
and a number of dimensions and observations from the trends on this plot are
again discussed and analyzed. Market segments are highlighted on the scatter
plot, and zoomed plots of each segment are also included. A brief description
of each of the new accelerators that have been added in the survey this year is
included, and this update features a new categorization of computing
architectures that implement each of the accelerators.

</details>


### [2] [Towards Straggler-Resilient Split Federated Learning: An Unbalanced Update Approach](https://arxiv.org/abs/2510.21155)
*Dandan Liang,Jianing Zhang,Evan Chen,Zhe Li,Rui Li,Haibo Yang*

Main category: cs.DC

TL;DR: Proposes MU-SplitFed, a straggler-resilient Split Federated Learning algorithm that lets the split server perform τ local updates per client round (unbalanced updates) under zeroth-order optimization; achieves convergence O(√(d/(τT))) for non-convex objectives and shows empirical gains under straggler conditions.


<details>
  <summary>Details</summary>
Motivation: SFL faces severe delays from stragglers because the server update depends on client activations; synchronization causes latency and limits scalability. The work aims to decouple server progress from slow clients to mitigate straggler effects.

Method: Introduce an unbalanced update mechanism where the Split Server runs τ local updates each client round, combined with a zeroth-order optimization approach suitable for cases where gradients are unavailable. This decouples server training progress from straggler-induced waits.

Result: Theoretically proves a convergence rate of O(√(d/(τT))) for non-convex objectives, implying linear speedup in communication rounds with larger τ. Empirical evaluations show MU-SplitFed outperforms baselines under straggler scenarios and that adaptive tuning of τ effectively reduces straggler impact. Code is provided.

Conclusion: MU-SplitFed is an effective, theoretically-grounded method to reduce straggler bottlenecks in Split Federated Learning by allowing unbalanced server updates; it yields provable speedups and practical improvements in experiments.

Abstract: Split Federated Learning (SFL) enables scalable training on edge devices by
combining the parallelism of Federated Learning (FL) with the computational
offloading of Split Learning (SL). Despite its great success, SFL suffers
significantly from the well-known straggler issue in distributed learning
systems. This problem is exacerbated by the dependency between Split Server and
clients: the Split Server side model update relies on receiving activations
from clients. Such synchronization requirement introduces significant time
latency, making straggler a critical bottleneck to the scalability and
efficiency of the system. To mitigate this problem, we propose MU-SplitFed, a
straggler-resilient SFL algorithm in zeroth-order optimization that decouples
training progress from straggler delays via a simple yet effective unbalanced
update mechanism.
  By enabling the server to perform $\tau$ local updates per client round,
MU-SplitFed achieves a convergence rate of $O(\sqrt{d/(\tau T)})$ for
non-convex objectives, demonstrating a linear speedup of $\tau$ in
communication rounds. Experiments demonstrate that MU-SplitFed consistently
outperforms baseline methods with the presence of stragglers and effectively
mitigates their impact through adaptive tuning of $\tau$. The code for this
project is available at https://github.com/Johnny-Zip/MU-SplitFed.

</details>


### [3] [Generative Federated Learning for Smart Prediction and Recommendation Applications](https://arxiv.org/abs/2510.21183)
*Anwesha Mukherjee,Rajkumar Buyya*

Main category: cs.DC

TL;DR: Proposes Generative Federated Learning (GAN + federated learning) for low-latency, privacy-preserving heart health prediction; uses GANs to generate synthetic data and implements centralized and decentralized FL on edge. Reports +12% accuracy and −73% response time versus conventional baselines.


<details>
  <summary>Details</summary>
Motivation: Smart prediction/recommendation systems face high response latency, privacy risks when centralizing health data, and data scarcity/class imbalance. The paper targets these by combining generative models with federated learning at the edge.

Method: Introduce Generative Federated Learning (GFL): train a GAN to create realistic synthetic heart-health data for augmentation and class-balance, then perform federated learning in two modes: centralized (edge nodes ↔ central server) and decentralized (peer-to-peer model update exchanges). Implement the system on an edge computing paradigm and evaluate on the heart-monitoring case study.

Result: The proposed framework outperforms prior heart-health monitoring approaches in the experiments: prediction accuracy improved by 12% over a conventional framework and response time reduced by 73% compared with a cloud-only system.

Conclusion: GFL can mitigate data scarcity and privacy concerns while lowering latency and improving accuracy for edge-based health monitoring. Further work should validate synthetic-data fidelity, privacy guarantees, communication costs, and robustness to adversarial or non-iid conditions.

Abstract: This paper proposes a generative adversarial network and federated
learning-based model to address various challenges of the smart prediction and
recommendation applications, such as high response time, compromised data
privacy, and data scarcity. The integration of the generative adversarial
network and federated learning is referred to as Generative Federated Learning
(GFL). As a case study of the proposed model, a heart health monitoring
application is considered. The realistic synthetic datasets are generated using
the generated adversarial network-based proposed algorithm for improving data
diversity, data quality, and data augmentation, and remove the data scarcity
and class imbalance issues. In this paper, we implement the centralized and
decentralized federated learning approaches in an edge computing paradigm. In
centralized federated learning, the edge nodes communicate with the central
server to build the global and personalized local models in a collaborative
manner. In the decentralized federated learning approach, the edge nodes
communicate among themselves to exchange model updates for collaborative
training. The comparative study shows that the proposed framework outperforms
the existing heart health monitoring applications. The results show that using
the proposed framework (i) the prediction accuracy is improved by 12% than the
conventional framework, and (ii) the response time is reduced by 73% than the
conventional cloud-only system.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [4] [A Confidence-Constrained Cloud-Edge Collaborative Framework for Autism Spectrum Disorder Diagnosis](https://arxiv.org/abs/2510.21130)
*Qi Deng,Yinghao Zhang,Yalin Liu,Bishenghui Tao*

Main category: cs.NI

TL;DR: C3EKD is a hierarchical cloud-edge knowledge distillation framework for ASD facial-image diagnosis in schools: edge models handle most inferences and send only low-confidence cases to the cloud, which returns temperature-scaled soft labels and distils improvements back to edges using a global aggregated loss, improving accuracy (87.4%) while reducing privacy/latency trade-offs.


<details>
  <summary>Details</summary>
Motivation: Address privacy and latency issues of cloud-only ASD diagnosis from IoT cameras, and the limited accuracy of edge-only inference, by combining cloud knowledge and edge deployment without centralizing raw data.

Method: Edge devices perform inference and send only low-confidence samples to the cloud. The cloud computes temperature-scaled soft labels and aggregates a global loss across participating schools; distilled knowledge is sent back to edge models to improve generalization. This selective upload and global distillation enable improved edge accuracy without sharing raw data.

Result: On two public ASD facial-image datasets, C3EKD achieved 87.4% accuracy, outperforming baselines (edge-only and cloud-only) while reducing data transfer and maintaining privacy by avoiding centralization of raw images.

Conclusion: C3EKD provides a scalable compromise between privacy, latency, and accuracy for ASD diagnosis in school IoT settings by leveraging selective cloud assistance and aggregated knowledge distillation to enhance edge models without raw-data centralization.

Abstract: Autism Spectrum Disorder (ASD) diagnosis systems in school environments
increasingly relies on IoT-enabled cameras, yet pure cloud processing raises
privacy and latency concerns while pure edge inference suffers from limited
accuracy. We propose Confidence-Constrained Cloud-Edge Knowledge Distillation
(C3EKD), a hierarchical framework that performs most inference at the edge and
selectively uploads only low-confidence samples to the cloud. The cloud
produces temperature-scaled soft labels and distils them back to edge models
via a global loss aggregated across participating schools, improving
generalization without centralizing raw data. On two public ASD facial-image
datasets, the proposed framework achieves a superior accuracy of 87.4\%,
demonstrating its potential for scalable deployment in real-world applications.

</details>
