{"id": "2510.16144", "categories": ["cs.NI", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.16144", "abs": "https://arxiv.org/abs/2510.16144", "authors": ["Sukhdeep Singh", "Avinash Bhat", "Shweta M", "Subhash K Singh", "Moonki Hong", "Madhan Raj K", "Kandeepan Sithamparanathan", "Sunder A. Khowaja", "Kapal Dev"], "title": "Agentic AI for Ultra-Modern Networks: Multi-Agent Framework for RAN Autonomy and Assurance", "comment": null, "summary": "The increasing complexity of Beyond 5G and 6G networks necessitates new\nparadigms for autonomy and assur- ance. Traditional O-RAN control loops rely\nheavily on RIC- based orchestration, which centralizes intelligence and exposes\nthe system to risks such as policy conflicts, data drift, and unsafe actions\nunder unforeseen conditions. In this work, we argue that the future of\nautonomous networks lies in a multi-agentic architecture, where specialized\nagents collaborate to perform data collection, model training, prediction,\npolicy generation, verification, deployment, and assurance. By replacing\ntightly- coupled centralized RIC-based workflows with distributed agents, the\nframework achieves autonomy, resilience, explainability, and system-wide\nsafety. To substantiate this vision, we design and evaluate a traffic steering\nuse case under surge and drift conditions. Results across four KPIs: RRC\nconnected users, IP throughput, PRB utilization, and SINR, demonstrate that a\nnaive predictor-driven deployment improves local KPIs but destabilizes\nneighbors, whereas the agentic system blocks unsafe policies, preserving global\nnetwork health. This study highlights multi- agent architectures as a credible\nfoundation for trustworthy AI- driven autonomy in next-generation RANs.", "AI": {"tldr": "Proposes replacing centralized RIC workflows with a distributed multi-agent architecture for B5G/6G RAN autonomy; shows in a traffic-steering case that agentic safety checks prevent policies that would destabilize neighbors, preserving global KPIs.", "motivation": "Centralized O-RAN RIC orchestration concentrates intelligence and is vulnerable to policy conflicts, data drift, and unsafe decisions under unforeseen conditions; next-generation networks require autonomous, explainable, resilient assurance mechanisms.", "method": "Designs a multi-agentic framework where specialized agents handle data collection, model training, prediction, policy generation, verification, deployment, and assurance. Implements a traffic-steering use case under surge and drift, comparing a naive predictor-driven deployment against the agentic system across four KPIs (RRC connected users, IP throughput, PRB utilization, SINR).", "result": "The naive predictor improves local KPIs but produces negative externalities that destabilize neighboring cells. The agentic system detects and blocks unsafe policies, maintaining global network health and better overall KPI balance.", "conclusion": "Multi-agent architectures are presented as a credible foundation for trustworthy AI-driven autonomy in next-generation RANs; distributed verification and safety enforcement are key to avoiding local optimizations that harm system-wide performance."}}
{"id": "2510.16415", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.16415", "abs": "https://arxiv.org/abs/2510.16415", "authors": ["Rizhen Hu", "Yutong He", "Ran Yan", "Mou Sun", "Binghang Yuan", "Kun Yuan"], "title": "MeCeFO: Enhancing LLM Training Robustness via Fault-Tolerant Optimization", "comment": "NeurIPS 2025 poster", "summary": "As distributed optimization scales to meet the demands of Large Language\nModel (LLM) training, hardware failures become increasingly non-negligible.\nExisting fault-tolerant training methods often introduce significant\ncomputational or memory overhead, demanding additional resources. To address\nthis challenge, we propose Memory- and Computation-efficient Fault-tolerant\nOptimization (MeCeFO), a novel algorithm that ensures robust training with\nminimal overhead. When a computing node fails, MeCeFO seamlessly transfers its\ntraining task to a neighboring node while employing memory- and\ncomputation-efficient algorithmic optimizations to minimize the extra workload\nimposed on the neighboring node handling both tasks. MeCeFO leverages three key\nalgorithmic designs: (i) Skip-connection, which drops the multi-head attention\n(MHA) module during backpropagation for memory- and computation-efficient\napproximation; (ii) Recomputation, which reduces activation memory in\nfeedforward networks (FFNs); and (iii) Low-rank gradient approximation,\nenabling efficient estimation of FFN weight matrix gradients. Theoretically,\nMeCeFO matches the convergence rate of conventional distributed training, with\na rate of $\\mathcal{O}(1/\\sqrt{nT})$, where n is the data parallelism size and\nT is the number of iterations. Empirically, MeCeFO maintains robust performance\nunder high failure rates, incurring only a 4.18% drop in throughput,\ndemonstrating 5.0$\\times$ to 6.7$\\times$ greater resilience than previous SOTA\napproaches. Codes are available at https://github.com/pkumelon/MeCeFO.", "AI": {"tldr": "\u63d0\u51fa MeCeFO\uff0c\u4e00\u7a2e\u5728\u7bc0\u9ede\u6545\u969c\u6642\u5c07\u8a13\u7df4\u5de5\u4f5c\u7121\u7e2b\u79fb\u5230\u9130\u8fd1\u7bc0\u9ede\uff0c\u4e26\u4ee5\u8a18\u61b6\u9ad4\u8207\u904b\u7b97\u6548\u7387\u70ba\u8a2d\u8a08\u76ee\u6a19\u7684\u5bb9\u932f\u512a\u5316\u6f14\u7b97\u6cd5\uff1b\u900f\u904e\u8df3\u63a5\uff08\u65bc\u53cd\u5411\u50b3\u64ad\u7565\u904e MHA\uff09\u3001\u91cd\u8a08\u7b97\uff08recomputation\uff09\u8207\u4f4e\u79e9\u68af\u5ea6\u8fd1\u4f3c\u4e09\u9805\u6280\u8853\u6e1b\u5c11\u984d\u5916\u8ca0\u64d4\uff0c\u7406\u8ad6\u4e0a\u6536\u6582\u7387\u8207\u5e38\u898f\u5206\u6563\u8a13\u7df4\u76f8\u7576\uff08O(1/\u221a{nT})\uff09\uff0c\u5be6\u9a57\u986f\u793a\u5728\u9ad8\u5931\u6548\u7387\u4e0b\u50c5\u6709\u7d04 4.18% \u541e\u5410\u91cf\u4e0b\u964d\uff0c\u4e14\u6bd4\u6700\u5148\u9032\u65b9\u6cd5\u6709 5.0\u00d7\u20136.7\u00d7 \u7684\u97cc\u6027\u63d0\u5347\u3002", "motivation": "\u96a8\u8457 LLM \u8a13\u7df4\u8d70\u5411\u5927\u898f\u6a21\u5206\u6563\u5316\uff0c\u786c\u9ad4\u7bc0\u9ede\u6545\u969c\u983b\u7387\u4e0d\u53ef\u5ffd\u8996\u3002\u65e2\u6709\u5bb9\u932f\u65b9\u6cd5\u5f80\u5f80\u5f15\u5165\u986f\u8457\u7684\u8a08\u7b97\u6216\u8a18\u61b6\u9ad4\u958b\u92b7\uff0c\u9700\u8981\u984d\u5916\u8cc7\u6e90\u3002\u8a2d\u8a08\u4e00\u5957\u5728\u767c\u751f\u7bc0\u9ede\u6545\u969c\u6642\uff0c\u80fd\u4ee5\u6700\u5c0f\u984d\u5916\u6210\u672c\u7e7c\u7e8c\u7a69\u5065\u8a13\u7df4\u7684\u6f14\u7b97\u6cd5\uff0c\u662f\u4e3b\u8981\u52d5\u6a5f\u3002", "method": "MeCeFO \u5728\u6545\u969c\u767c\u751f\u6642\u5c07\u5931\u6548\u7bc0\u9ede\u7684\u4efb\u52d9\u79fb\u4ea4\u7d66\u9130\u8fd1\u7bc0\u9ede\uff0c\u4e26\u4ee5\u4e09\u9805\u6f14\u7b97\u6cd5\u512a\u5316\u964d\u4f4e\u9130\u5c45\u7684\u984d\u5916\u8ca0\u64d4\uff1a(i) Skip-connection\uff1a\u65bc\u53cd\u5411\u50b3\u64ad\u6642\u7565\u904e\u591a\u982d\u6ce8\u610f\u529b\uff08MHA\uff09\u6a21\u7d44\uff0c\u4ee5\u7bc0\u7701\u8a18\u61b6\u9ad4\u8207\u904b\u7b97\u91cf\uff0c\u4f5c\u70ba\u8fd1\u4f3c\u7b56\u7565\uff1b(ii) Recomputation\uff1a\u5728 FFN \u4e2d\u900f\u904e\u91cd\u8a08\u7b97\u4ee5\u964d\u4f4e\u6fc0\u6d3b\u503c\u8a18\u61b6\u9ad4\u9700\u6c42\uff1b(iii) Low-rank gradient approximation\uff1a\u5c0d FFN \u6b0a\u91cd\u77e9\u9663\u7684\u68af\u5ea6\u63a1\u4f4e\u79e9\u8fd1\u4f3c\u4ee5\u964d\u4f4e\u68af\u5ea6\u4f30\u8a08\u6210\u672c\u3002\u6574\u9ad4\u8a2d\u8a08\u517c\u9867\u6548\u7387\u8207\u7a69\u5b9a\u6027\u3002", "result": "\u7406\u8ad6\u5206\u6790\u8868\u660e MeCeFO \u53ef\u4fdd\u6301\u8207\u5e38\u898f\u5206\u6563\u8a13\u7df4\u76f8\u540c\u7b49\u968e\u7684\u6536\u6582\u901f\u7387 O(1/\u221a{nT})\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\u5728\u9ad8\u5931\u6548\u7387\u689d\u4ef6\u4e0b\uff0cMeCeFO \u50c5\u9020\u6210\u7d04 4.18% \u7684\u541e\u5410\u91cf\u4e0b\u964d\uff0c\u4e14\u5728\u97cc\u6027\u4e0a\u6bd4\u5148\u524d\u6700\u5148\u9032\u65b9\u6cd5\u9ad8\u51fa 5.0\u00d7 \u5230 6.7\u00d7\u3002\u539f\u59cb\u7a0b\u5f0f\u78bc\u5df2\u516c\u958b\u3002", "conclusion": "MeCeFO \u80fd\u5728\u4e0d\u986f\u8457\u589e\u52a0\u8cc7\u6e90\u9700\u6c42\u7684\u60c5\u6cc1\u4e0b\uff0c\u70ba\u5927\u898f\u6a21 LLM \u8a13\u7df4\u63d0\u4f9b\u9ad8\u6548\u7684\u5bb9\u932f\u80fd\u529b\u3002\u5176\u6f14\u7b97\u6cd5\u6027\u8fd1\u4f3c\u8207\u91cd\u8a08\u7b97\u7b56\u7565\u5728\u5be6\u52d9\u4e0a\u9054\u5230\u826f\u597d\u6b0a\u8861\uff0c\u4f46\u4ecd\u9700\u9032\u4e00\u6b65\u8a55\u4f30\u8fd1\u4f3c\u5c0d\u6700\u7d42\u6a21\u578b\u7cbe\u5ea6\u8207\u4e0d\u540c\u62d3\u6a38\u3001\u5931\u6548\u6a21\u578b\u4e0b\u7684\u6cdb\u5316\u6027\u3002"}}
{"id": "2510.16418", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.16418", "abs": "https://arxiv.org/abs/2510.16418", "authors": ["Jian Ma", "Xinchen Lyu", "Jun Jiang", "Longhao Zou", "Chenshan Ren", "Qimei Cui", "Xiaofeng Tao"], "title": "FourierCompress: Layer-Aware Spectral Activation Compression for Efficient and Accurate Collaborative LLM Inference", "comment": null, "summary": "Collaborative large language model (LLM) inference enables real-time,\nprivacy-preserving AI services on resource-constrained edge devices by\npartitioning computational workloads between client devices and edge servers.\nHowever, this paradigm is severely hindered by communication bottlenecks caused\nby the transmission of high-dimensional intermediate activations, exacerbated\nby the autoregressive decoding structure of LLMs, where bandwidth consumption\nscales linearly with output length. Existing activation compression methods\nstruggle to simultaneously achieve high compression ratios, low reconstruction\nerror, and computational efficiency. This paper proposes FourierCompress, a\nnovel, layer-aware activation compression framework that exploits the\nfrequency-domain sparsity of LLM activations. We rigorously demonstrate that\nactivations from the first Transformer layer exhibit strong smoothness and\nenergy concentration in the low-frequency domain, making them highly amenable\nto near-lossless compression via the Fast Fourier Transform (FFT).\nFourierCompress transforms activations into the frequency domain, retains only\na compact block of low-frequency coefficients, and reconstructs the signal at\nthe server using conjugate symmetry, enabling seamless hardware acceleration on\nDSPs and FPGAs. Extensive experiments on Llama 3 and Qwen2.5 models across 10\ncommonsense reasoning datasets demonstrate that FourierCompress preserves\nperformance remarkably close to the uncompressed baseline, outperforming Top-k,\nQR, and SVD. FourierCompress bridges the gap between communication efficiency\n(an average 7.6x reduction in activation size), near-lossless inference (less\nthan 0.3% average accuracy loss), and significantly faster compression\n(achieving over 32x reduction in compression time compared to Top-k via\nhardware acceleration) for edge-device LLM inference.", "AI": {"tldr": "\u63d0\u51fa FourierCompress\uff0c\u4e00\u7a2e\u57fa\u65bc\u983b\u57df\u7a00\u758f\u6027\u7684\u5c64\u611f\u77e5\u6fc0\u6d3b\u58d3\u7e2e\u6846\u67b6\uff0c\u5229\u7528 FFT \u4fdd\u7559\u4f4e\u983b\u4fc2\u6578\u4e26\u7528\u5171\u8edb\u5c0d\u7a31\u91cd\u5efa\uff0c\u4ee5\u5728\u908a\u7de3\u8a2d\u5099\u4e0a\u5be6\u73fe\u9ad8\u58d3\u7e2e\u6bd4\u3001\u8fd1\u7121\u640d\u63a8\u7406\u8207\u5feb\u901f\u786c\u9ad4\u52a0\u901f\u3002", "motivation": "\u5354\u4f5c\u5f0f LLM \u63a8\u7406\u5728\u8cc7\u6e90\u53d7\u9650\u7684\u908a\u7de3\u8a2d\u5099\u4e0a\u9700\u8981\u5206\u64d4\u904b\u7b97\u8207\u4fdd\u8b77\u96b1\u79c1\uff0c\u4f46\u4e2d\u9593\u6fc0\u6d3b\u5411\u91cf\u9ad8\u7dad\u4e14\u96a8\u751f\u6210\u9577\u5ea6\u7dda\u6027\u589e\u9577\uff0c\u9020\u6210\u901a\u8a0a\u74f6\u9838\uff1b\u73fe\u6709\u58d3\u7e2e\u65b9\u6cd5\u96e3\u4ee5\u517c\u9867\u58d3\u7e2e\u7387\u3001\u91cd\u5efa\u8aa4\u5dee\u8207\u8a08\u7b97\u6548\u7387\u3002", "method": "\u89c0\u5bdf Transformer \u9996\u5c64\u6fc0\u6d3b\u5728\u983b\u57df\u5177\u6709\u5e73\u6ed1\u6027\u8207\u80fd\u91cf\u4f4e\u983b\u96c6\u4e2d\u6027\uff0c\u5c0d\u6fc0\u6d3b\u505a FFT\uff0c\u50c5\u4fdd\u7559\u4e00\u500b\u4f4e\u983b\u4fc2\u6578\u584a\u4e26\u5229\u7528\u5171\u8edb\u5c0d\u7a31\u6027\u5728\u4f3a\u670d\u5668\u7aef\u91cd\u5efa\uff1b\u8a2d\u8a08\u70ba\u5c64\u611f\u77e5\u4e26\u53ef\u5728 DSP/FPU/FPGA \u4e0a\u786c\u9ad4\u52a0\u901f\u3002", "result": "\u5728 Llama 3 \u8207 Qwen2.5 \u4e26\u8de8 10 \u500b\u5e38\u8b58\u63a8\u7406\u6578\u64da\u96c6\u7684\u5be6\u9a57\u4e2d\uff0c\u5e73\u5747\u58d3\u7e2e\u6bd4\u7d04 7.6\u00d7\uff0c\u5e73\u5747\u7cbe\u5ea6\u640d\u5931 <0.3%\uff0c\u4e14\u5728\u786c\u9ad4\u52a0\u901f\u4e0b\u58d3\u7e2e\u6642\u9593\u6bd4 Top-k \u5feb >32\u00d7\uff0c\u5728\u6548\u80fd\u4e0a\u512a\u65bc Top-k\u3001QR\u3001SVD\u3002", "conclusion": "FourierCompress \u5728\u908a\u7de3 LLM \u5354\u4f5c\u63a8\u7406\u5834\u666f\u4e2d\u6210\u529f\u5e73\u8861\u4e86\u901a\u8a0a\u6548\u7387\u3001\u8fd1\u7121\u640d\u63a8\u7406\u8207\u58d3\u7e2e\u901f\u5ea6\uff0c\u70ba\u5be6\u6642\u3001\u96b1\u79c1\u4fdd\u8b77\u7684\u908a\u7de3 AI \u63d0\u4f9b\u53ef\u884c\u89e3\u3002"}}
{"id": "2510.17147", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.17147", "abs": "https://arxiv.org/abs/2510.17147", "authors": ["Linhan Xia", "Mingzhan Yang", "Jingjing Wang", "Ziwei Yan", "Yakun Ren", "Guo Yu", "Kai Lei"], "title": "Mamba4Net: Distilled Hybrid Mamba Large Language Models For Networking", "comment": null, "summary": "Transformer-based large language models (LLMs) are increasingly being adopted\nin networking research to address domain-specific challenges. However, their\nquadratic time complexity and substantial model sizes often result in\nsignificant computational overhead and memory constraints, particularly in\nresource-constrained environments. Drawing inspiration from the efficiency and\nperformance of the Deepseek-R1 model within the knowledge distillation\nparadigm, this paper introduces Mamba4Net, a novel cross-architecture\ndistillation framework. Mamba4Net transfers networking-specific knowledge from\ntransformer-based LLMs to student models built on the Mamba architecture, which\nfeatures linear time complexity. This design substantially enhances\ncomputational efficiency compared to the quadratic complexity of\ntransformer-based models, while the reduced model size further minimizes\ncomputational demands, improving overall performance and resource utilization.\nTo evaluate its effectiveness, Mamba4Net was tested across three diverse\nnetworking tasks: viewport prediction, adaptive bitrate streaming, and cluster\njob scheduling. Compared to existing methods that do not leverage LLMs,\nMamba4Net demonstrates superior task performance. Furthermore, relative to\ndirect applications of transformer-based LLMs, it achieves significant\nefficiency gains, including a throughput 3.96 times higher and a storage\nfootprint of only 5.48% of that required by previous LLM-based approaches.\nThese results highlight Mamba4Net's potential to enable the cost-effective\napplication of LLM-derived knowledge in networking contexts. The source code is\nopenly available to support further research and development.", "AI": {"tldr": "\u63d0\u51fa Mamba4Net\uff0c\u4e00\u500b\u5c07 transformer-based LLM \u7684\u7db2\u8def\u9818\u57df\u77e5\u8b58\u84b8\u993e\u5230 Mamba \u67b6\u69cb\uff08\u7dda\u6027\u6642\u9593\u8907\u96dc\u5ea6\uff09\u7684\u8de8\u67b6\u69cb\u84b8\u993e\u6846\u67b6\uff0c\u4ee5\u5728\u4e09\u500b\u7db2\u8def\u4efb\u52d9\u4e0a\u4fdd\u6709\u6548\u80fd\u540c\u6642\u5927\u5e45\u964d\u4f4e\u8cc7\u6e90\u6d88\u8017\uff08\u541e\u5410 3.96\u00d7\uff0c\u5132\u5b58\u50c5 5.48%\uff09\u3002\u539f\u59cb\u78bc\u516c\u958b\u3002", "motivation": "Transformers \u5728\u7db2\u8def\u7814\u7a76\u4e2d\u8868\u73fe\u826f\u597d\u4f46\u8a08\u7b97\u8207\u8a18\u61b6\u9ad4\u958b\u92b7\u9ad8\u6602\uff1b\u9700\u8981\u4e00\u7a2e\u80fd\u5728\u8cc7\u6e90\u53d7\u9650\u74b0\u5883\u4e2d\u4fdd\u7559 LLM \u77e5\u8b58\u4f46\u66f4\u6709\u6548\u7387\u7684\u6a21\u578b\u3002", "method": "\u5f9e transformer-based LLM \u4ee5\u77e5\u8b58\u84b8\u993e\u65b9\u5f0f\u5c07\u7db2\u8def\u5c08\u5c6c\u77e5\u8b58\u8f49\u79fb\u81f3\u57fa\u65bc Mamba \u7684\u5b78\u751f\u6a21\u578b\uff1bMamba \u63a1\u7dda\u6027\u6642\u9593\u8907\u96dc\u5ea6\u8207\u8f03\u5c0f\u6a21\u578b\u5c3a\u5bf8\u4ee5\u6e1b\u5c11\u8a08\u7b97\u8207\u5132\u5b58\u6210\u672c\uff1b\u5728\u8996\u7a97\u9810\u6e2c\u3001\u9069\u61c9\u6027\u78bc\u7387\u4e32\u6d41\u8207\u53e2\u96c6\u6392\u7a0b\u4e09\u500b\u4efb\u52d9\u4e0a\u9032\u884c\u8a55\u4f30\u3002", "result": "\u76f8\u8f03\u65bc\u672a\u5229\u7528 LLM \u7684\u50b3\u7d71\u65b9\u6cd5\uff0cMamba4Net \u5728\u4efb\u52d9\u8868\u73fe\u4e0a\u66f4\u4f73\uff1b\u76f8\u8f03\u65bc\u76f4\u63a5\u63a1\u7528 transformer LLM\uff0cMamba4Net \u5728\u6548\u7387\u4e0a\u5927\u5e45\u63d0\u5347\uff08\u541e\u5410\u7d04 3.96 \u500d\u3001\u5132\u5b58\u50c5 5.48%\uff09\u3002", "conclusion": "Mamba4Net \u5c55\u793a\u4e86\u8de8\u67b6\u69cb\u84b8\u993e\u53ef\u5728\u4fdd\u7559\u6216\u63d0\u5347\u4efb\u52d9\u6548\u80fd\u7684\u540c\u6642\uff0c\u986f\u8457\u964d\u4f4e\u8a08\u7b97\u8207\u5132\u5b58\u9700\u6c42\uff0c\u5177\u5099\u5728\u8cc7\u6e90\u53d7\u9650\u7db2\u8def\u5834\u666f\u4e2d\u5be6\u7528\u7684\u6f5b\u529b\u3002"}}
{"id": "2510.16497", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16497", "abs": "https://arxiv.org/abs/2510.16497", "authors": ["Pacome Simon Mbonimpa", "Diane Tuyizere", "Azizuddin Ahmed Biyabani", "Ozan K. Tonguz"], "title": "Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages", "comment": null, "summary": "This paper presents a novel framework for speech transcription and synthesis,\nleveraging edge-cloud parallelism to enhance processing speed and accessibility\nfor Kinyarwanda and Swahili speakers. It addresses the scarcity of powerful\nlanguage processing tools for these widely spoken languages in East African\ncountries with limited technological infrastructure. The framework utilizes the\nWhisper and SpeechT5 pre-trained models to enable speech-to-text (STT) and\ntext-to-speech (TTS) translation. The architecture uses a cascading mechanism\nthat distributes the model inference workload between the edge device and the\ncloud, thereby reducing latency and resource usage, benefiting both ends. On\nthe edge device, our approach achieves a memory usage compression of 9.5% for\nthe SpeechT5 model and 14% for the Whisper model, with a maximum memory usage\nof 149 MB. Experimental results indicate that on a 1.7 GHz CPU edge device with\na 1 MB/s network bandwidth, the system can process a 270-character text in less\nthan a minute for both speech-to-text and text-to-speech transcription. Using\nreal-world survey data from Kenya, it is shown that the cascaded edge-cloud\narchitecture proposed could easily serve as an excellent platform for STT and\nTTS transcription with good accuracy and response time.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u5957\u4ee5\u908a\u7de3\u2014\u96f2\u7aef\u5e73\u884c\u8655\u7406\u70ba\u6838\u5fc3\u7684\u8a9e\u97f3\u8f49\u5beb\u8207\u5408\u6210\u6846\u67b6\uff0c\u4f7f\u7528 Whisper \u8207 SpeechT5 \u4f5c\u70ba\u9810\u8a13\u7df4\u6a21\u578b\uff0c\u900f\u904e\u4e32\u6d41/\u5206\u6563\u63a8\u8ad6\u964d\u4f4e\u5ef6\u9072\u8207\u8a18\u61b6\u9ad4\u4f7f\u7528\uff0c\u91dd\u5c0d\u80af\u4e9e\u5be6\u969b\u554f\u5377\u8cc7\u6599\u5728\u4f4e\u8cc7\u6e90\u88dd\u7f6e\uff081.7 GHz CPU\u30011 MB/s\uff09\u4e0a\u80fd\u65bc\u4e00\u5206\u9418\u5167\u5b8c\u6210 270 \u5b57\u7684 STT \u8207 TTS\uff0c\u4e26\u5c07 SpeechT5 \u8207 Whisper \u8a18\u61b6\u9ad4\u5206\u5225\u58d3\u7e2e\u7d04 9.5% \u548c 14%\uff0c\u6700\u5927\u8a18\u61b6\u9ad4\u4f7f\u7528 149 MB\u3002", "motivation": "\u89e3\u6c7a\u6771\u975e\u5ee3\u6cdb\u4f7f\u7528\u4f46\u8cc7\u6e90\u5331\u4e4f\u8a9e\u8a00\uff08\u5982 Kinyarwanda \u8207 Swahili\uff09\u7f3a\u4e4f\u5f37\u5927\u8a9e\u8a00\u8655\u7406\u5de5\u5177\uff0c\u4ee5\u53ca\u7576\u5730\u8a2d\u5099\u8207\u7db2\u8def\u57fa\u790e\u8a2d\u65bd\u6709\u9650\uff0c\u9700\u8a2d\u8a08\u80fd\u5728\u4f4e\u898f\u683c\u908a\u7de3\u88dd\u7f6e\u4e0a\u63d0\u4f9b\u53ef\u63a5\u53d7\u5ef6\u9072\u8207\u6e96\u78ba\u5ea6\u7684 STT/TTS \u89e3\u6c7a\u65b9\u6848\u3002", "method": "\u63a1\u7528 Whisper\uff08STT\uff09\u8207 SpeechT5\uff08TTS\uff09\u4e4b\u9810\u8a13\u7df4\u6a21\u578b\uff0c\u8a2d\u8a08\u4e00\u500b\u4e32\u63a5/\u5c64\u7d1a\u5f0f\uff08cascading\uff09\u67b6\u69cb\u5c07\u63a8\u8ad6\u5de5\u4f5c\u5728\u908a\u7de3\u88dd\u7f6e\u8207\u96f2\u7aef\u9593\u5206\u914d\u4ee5\u5e73\u8861\u5ef6\u9072\u8207\u8cc7\u6e90\u6d88\u8017\uff1b\u91dd\u5c0d\u908a\u7de3\u7aef\u9032\u884c\u6a21\u578b\u58d3\u7e2e\u4e26\u9650\u5236\u8a18\u61b6\u9ad4\u4f7f\u7528\uff0c\u4e0a\u50b3/\u4e0b\u8f09\u91cf\u53d7\u7db2\u8def\u983b\u5bec\u5f71\u97ff\uff0c\u5be6\u9a57\u5728 1.7 GHz CPU \u8207 1 MB/s \u7db2\u8def\u689d\u4ef6\u4e0b\u9032\u884c\u6548\u80fd\u6e2c\u8a66\uff1b\u4f7f\u7528\u4f86\u81ea\u80af\u4e9e\u7684\u5be6\u52d9\u554f\u5377\u8a9e\u97f3/\u6587\u672c\u4f5c\u70ba\u9a57\u8b49\u8cc7\u6599\u3002", "result": "\u5728\u76ee\u6a19\u786c\u9ad4\u8207\u7db2\u8def\u689d\u4ef6\u4e0b\uff0c\u53ef\u65bc\u4e00\u5206\u9418\u5167\u5b8c\u6210 270 \u5b57\u7684 STT \u8207 TTS \u4efb\u52d9\uff1b\u5c0d SpeechT5 \u8207 Whisper \u5206\u5225\u9054\u5230\u7d04 9.5% \u8207 14% \u7684\u8a18\u61b6\u9ad4\u4f7f\u7528\u58d3\u7e2e\uff0c\u6700\u5927\u8a18\u61b6\u9ad4\u5cf0\u503c\u70ba 149 MB\uff1b\u5be6\u9a57\u6578\u64da\uff08\u57fa\u65bc\u80af\u4e9e\u554f\u5377\uff09\u986f\u793a\u7cfb\u7d71\u5728\u56de\u61c9\u6642\u9593\u8207\u6e96\u78ba\u5ea6\u4e0a\u8868\u73fe\u826f\u597d\uff0c\u9069\u5408\u4f5c\u70ba\u4f4e\u8cc7\u6e90\u74b0\u5883\u7684\u8a9e\u97f3\u8655\u7406\u89e3\u6c7a\u65b9\u6848\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u908a\u7de3\u2014\u96f2\u7aef\u4e32\u63a5\u67b6\u69cb\u80fd\u5728\u53d7\u9650\u786c\u9ad4\u8207\u983b\u5bec\u4e0b\u63d0\u4f9b\u53ef\u7528\u7684 STT \u8207 TTS \u670d\u52d9\uff0c\u964d\u4f4e\u5ef6\u9072\u8207\u8cc7\u6e90\u6d88\u8017\uff0c\u5c0d Kinyarwanda \u8207 Swahili \u7b49\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u5177\u6709\u5be6\u52d9\u61c9\u7528\u6f5b\u529b\u3002\u7136\u800c\u4ecd\u9700\u88dc\u5145\u8a73\u76e1\u7684\u8a55\u4f30\u6307\u6a19\uff08\u5982 WER\u3001MOS\uff09\u3001\u8207\u57fa\u6e96\u6bd4\u8f03\u3001\u53ef\u64f4\u5c55\u6027\u8207\u7db2\u8def\u65b7\u7dda\u60c5\u5883\u4e0b\u7684\u8655\u7406\u7b56\u7565\u4ee5\u9a57\u8b49\u7a69\u5065\u6027\u3002"}}
{"id": "2510.16890", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.16890", "abs": "https://arxiv.org/abs/2510.16890", "authors": ["Ji\u0159\u00ed Klepl", "Martin Kruli\u0161", "Maty\u00e1\u0161 Brabec"], "title": "Layout-Agnostic MPI Abstraction for Distributed Computing in Modern C++", "comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Recent Advances in the Message Passing Interface (EuroMPI 2025),\n  and is available online at https://doi.org/10.1007/978-3-032-07194-1_3", "summary": "Message Passing Interface (MPI) has been a well-established technology in the\ndomain of distributed high-performance computing for several decades. However,\none of its greatest drawbacks is a rather ancient pure-C interface. It lacks\nmany useful features of modern languages (namely C++), like basic type-checking\nor support for generic code design. In this paper, we propose a novel\nabstraction for MPI, which we implemented as an extension of the C++ Noarr\nlibrary. It follows Noarr paradigms (first-class layout and traversal\nabstraction) and offers layout-agnostic design of MPI applications. We also\nimplemented a layout-agnostic distributed GEMM kernel as a case study to\ndemonstrate the usability and syntax of the proposed abstraction. We show that\nthe abstraction achieves performance comparable to the state-of-the-art MPI C++\nbindings while allowing for a more flexible design of distributed applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u500b\u4ee5 C++ Noarr \u64f4\u5145\u70ba\u57fa\u790e\u7684 MPI \u62bd\u8c61\u5c64\uff0c\u63d0\u4f9b layout-agnostic \u7684\u5206\u6563\u5f0f\u61c9\u7528\u8a2d\u8a08\uff0c\u4e26\u4ee5\u5206\u6563\u5f0f GEMM \u4f5c\u70ba\u6848\u4f8b\u9a57\u8b49\uff1a\u8a9e\u610f\u66f4\u73fe\u4ee3\u4e14\u6027\u80fd\u8207\u73fe\u6709 MPI C++ \u7d81\u5b9a\u76f8\u7576\u3002", "motivation": "MPI \u539f\u751f\u4ecb\u9762\u70ba\u53e4\u8001\u7684\u7d14 C API\uff0c\u7f3a\u4e4f C++ \u7684\u578b\u5225\u6aa2\u67e5\u8207\u6cdb\u578b\u652f\u63f4\uff0c\u5c0e\u81f4\u4f7f\u7528\u8005\u5728\u958b\u767c\u53ef\u91cd\u7528\u8207\u9748\u6d3b\u7684\u5206\u6563\u5f0f\u9ad8\u6548\u80fd\u7a0b\u5f0f\u6642\u53d7\u9650\u3002\u9700\u8981\u4e00\u7a2e\u80fd\u4fdd\u7559 MPI \u6027\u80fd\u540c\u6642\u63d0\u4f9b\u73fe\u4ee3\u8a9e\u8a00\u62bd\u8c61\u7684\u89e3\u6c7a\u65b9\u6848\u3002", "method": "\u5728 Noarr\uff08\u5f37\u8abf\u8cc7\u6599\u4f48\u5c40\u8207\u904d\u6b77\u7684\u7b2c\u4e00\u985e\u62bd\u8c61\uff09\u57fa\u790e\u4e0a\u5be6\u4f5c\u4e00\u500b MPI \u62bd\u8c61\u64f4\u5145\uff0c\u8b93\u901a\u4fe1 API \u8207\u8cc7\u6599\u4f48\u5c40\u5206\u96e2\uff0c\u652f\u63f4 layout-agnostic \u7684\u7a0b\u5f0f\u8a2d\u8a08\u3002\u4ee5\u4e00\u500b layout-agnostic \u7684\u5206\u6563\u5f0f GEMM kernel \u4f5c\u70ba\u6848\u4f8b\uff0c\u5c55\u793a\u8a9e\u6cd5\u8207\u53ef\u7528\u6027\uff0c\u4e26\u8207\u73fe\u6709 MPI C++ \u7d81\u5b9a\u9032\u884c\u6548\u80fd\u6bd4\u8f03\u3002", "result": "\u5be6\u9a57\u986f\u793a\u63d0\u51fa\u7684\u62bd\u8c61\u5728\u6548\u80fd\u4e0a\u53ef\u8207\u6700\u5148\u9032\u7684 MPI C++ \u7d81\u5b9a\u76f8\u7576\uff0c\u540c\u6642\u63d0\u4f9b\u66f4\u9748\u6d3b\u8207\u53ef\u6cdb\u5316\u7684\u7a0b\u5f0f\u8a2d\u8a08\u4ecb\u9762\u3002\u6848\u4f8b\u7684\u5206\u6563\u5f0f GEMM \u6210\u529f\u5c55\u793a\u4e86\u8a72\u62bd\u8c61\u7684\u9069\u7528\u6027\u8207\u6613\u7528\u6027\u3002", "conclusion": "\u8a72 Noarr \u64f4\u5145\u7684 MPI \u62bd\u8c61\u80fd\u5728\u4e0d\u72a7\u7272\u6548\u80fd\u7684\u60c5\u6cc1\u4e0b\uff0c\u628a\u73fe\u4ee3 C++ \u7684\u578b\u5225\u8207\u6cdb\u578b\u512a\u52e2\u5e36\u5165 MPI \u61c9\u7528\uff0c\u4fc3\u9032\u66f4\u5177\u5f48\u6027\u8207\u53ef\u91cd\u7528\u7684\u5206\u6563\u5f0f\u9ad8\u6548\u80fd\u7a0b\u5f0f\u8a2d\u8a08\u3002"}}
{"id": "2510.17410", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.17410", "abs": "https://arxiv.org/abs/2510.17410", "authors": ["Dmitry Bankov", "Artem Krasilov", "Artem Otmakhov", "Pavel Savlukovich", "Evgeny Khorov"], "title": "Is It Worth to Use Feedback Channel in 5G V2X Platoon Scenarios?", "comment": null, "summary": "5G Vehicle-to-Everything (V2X) is a new technology developed by 3GPP to\nsupport inter-vehicle communication. In contrast to 4G V2X which allows only\nbroadcast communication, 5G V2X enables groupcast and unicast communication.\nSuch types of communication are needed for new V2X scenarios: platooning,\nextended sensors, remote driving, etc. To improve the data transmission\nreliability and assist in the selection of the transmission parameters in these\nscenarios, 5G V2X introduces a feedback channel that allows receivers to send\nacknowledgments in response to data packets. However, some part of the overall\nresource shall be allocated for the feedback channel, which reduces the amount\nof channel resources available for data transmission. In this paper, we\nconsider a scenario with a platoon, which generates groupcast traffic, and\nsurrounding vehicles, which generate legacy broadcast traffic. Using extensive\nsimulations in NS-3, we analyze how the usage of the feedback channel\ninfluences the overall system capacity. Our results show that depending on the\nplatoon size, groupcast, and broadcast traffic intensities, and their quality\nof service requirements, the usage of the feedback channel can in some cases\nsignificantly increase the system capacity (up to 2x), while in other cases it\nalmost halves the system capacity. We explain the reasons for such effects and\ndiscuss how to adaptively select the feedback channel parameters.", "AI": {"tldr": "This paper evaluates how introducing a feedback channel in 5G V2X (to support groupcast/unicast) affects system capacity in a mixed scenario with a platoon (groupcast) and surrounding vehicles (broadcast). Using NS-3 simulations they find feedback can either nearly double capacity or nearly halve it, depending on platoon size, traffic load and QoS; adaptive feedback parameter selection is proposed.", "motivation": "5G V2X adds groupcast/unicast and receiver feedback to improve reliability and transmission parameter selection. However feedback consumes radio resources, so the net effect on overall system capacity in mixed traffic scenarios is unclear and must be quantified.", "method": "Build a mixed traffic scenario (platoon producing groupcast + surrounding broadcast) in NS-3. Sweep variables: platoon size, groupcast and broadcast traffic intensities, QoS requirements, and feedback-channel parameterization. Measure system capacity and analyze trade-offs.", "result": "Simulations show a context-dependent effect: in some parameter regimes feedback increases capacity up to ~2x (by improving reliability and enabling more aggressive transmission settings), while in others it reduces capacity by up to ~50% (because feedback consumes scarce resources and interferes with broadcast flows). The paper analyzes root causes and interactions.", "conclusion": "The benefit of a feedback channel in 5G V2X is not universal; it depends on traffic mix, platoon size and QoS. Adaptive selection of feedback resources/parameters is necessary to realize gains and avoid performance degradation."}}
{"id": "2510.16933", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16933", "abs": "https://arxiv.org/abs/2510.16933", "authors": ["Maty\u00e1\u0161 Brabec", "Ji\u0159\u00ed Klepl", "Michal T\u00f6pfer", "Martin Kruli\u0161"], "title": "Tutoring LLM into a Better CUDA Optimizer", "comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Euro-Par 2025: Parallel Processing, Part II, and is available\n  online at https://doi.org/10.1007/978-3-031-99857-7_18", "summary": "Recent leaps in large language models (LLMs) caused a revolution in\nprogramming tools (like GitHub Copilot) that can help with code generation,\ndebugging, and even performance optimization. In this paper, we focus on the\ncapabilities of the most recent reasoning models to generate optimized CUDA\ncode for predefined, well-known tasks. Our objective is to determine which\ntypes of code optimizations and parallel patterns the LLMs can perform by\nthemselves and whether they can be improved by tutoring (providing more\ndetailed hints and guidelines in the prompt). The generated solutions were\nevaluated both automatically (for correctness and speedup) and manually (code\nreviews) to provide a more detailed perspective. We also tried an interactive\napproach where the LLM can fix its previous mistakes within a session. The\nresults indicate that LLMs are quite skilled coders; however, they require\ntutoring to reach optimized solutions provided by parallel computing experts.", "AI": {"tldr": "LLMs \u80fd\u751f\u6210 CUDA \u7a0b\u5f0f\u4e26\u505a\u90e8\u5206\u512a\u5316\uff0c\u4f46\u8981\u9054\u5230\u4e26\u884c\u904b\u7b97\u5c08\u5bb6\u7684\u6c34\u6e96\u4ecd\u9700\u63d0\u793a\uff08tutoring\uff09\u8207\u4e92\u52d5\u4fee\u6b63\u3002", "motivation": "\u8a55\u4f30\u6700\u65b0\u63a8\u7406\u578b LLM \u5728\u70ba\u5df2\u77e5\u4efb\u52d9\u7522\u751f\u512a\u5316 CUDA \u7a0b\u5f0f\u3001\u8fa8\u8b58\u4e26\u884c\u6a21\u5f0f\u8207\u61c9\u7528\u6700\u4f73\u5316\u6280\u5de7\u4e0a\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u63d0\u793a\uff08prompt tutoring\uff09\u8207\u4e92\u52d5\u4fee\u6b63\u5c0d\u7d50\u679c\u7684\u5f71\u97ff\u3002", "method": "\u8b93 LLM \u751f\u6210\u91dd\u5c0d\u9810\u5b9a\u7bc4\u4f8b\u4efb\u52d9\u7684 CUDA \u7a0b\u5f0f\uff1b\u4f7f\u7528\u81ea\u52d5\u5316\u6e2c\u8a66\uff08\u6b63\u78ba\u6027\u3001\u52a0\u901f\u6bd4\uff09\u8207\u4eba\u5de5\u7a0b\u5f0f\u78bc\u5be9\u67e5\u8a55\u4f30\uff1b\u6bd4\u8f03\u539f\u59cb\u63d0\u793a\u3001\u7d93\u904e\u8a73\u7d30\u63d0\u793a\uff08tutoring\uff09\u8207\u4e92\u52d5\u4fee\u6b63\u7b56\u7565\u7684\u7d50\u679c\u3002", "result": "LLMs \u5728\u7522\u751f\u529f\u80fd\u6b63\u78ba\u7684 CUDA \u7a0b\u5f0f\u65b9\u9762\u8868\u73fe\u826f\u597d\uff0c\u80fd\u5920\u63a1\u7528\u90e8\u5206\u4e26\u884c\u6a21\u5f0f\u8207\u57fa\u672c\u512a\u5316\uff1b\u900f\u904e\u63d0\u793a\u8207\u4e92\u52d5\u53ef\u9032\u4e00\u6b65\u6539\u5584\u6548\u80fd\uff0c\u4f46\u4ecd\u5e38\u843d\u5f8c\u65bc\u4e26\u884c\u904b\u7b97\u5c08\u5bb6\u6240\u63d0\u4f9b\u7684\u6700\u4f73\u5316\u7248\u672c\u3002", "conclusion": "LLMs \u662f\u6709\u7528\u7684\u7a0b\u5f0f\u5354\u52a9\u5de5\u5177\uff0c\u4f46\u82e5\u8981\u9054\u5230\u5c08\u5bb6\u7d1a\u7684 CUDA \u512a\u5316\uff0c\u9700\u8981\u5c08\u5bb6\u4ecb\u5165\u3001\u7cbe\u7dfb\u5316\u63d0\u793a\u6216\u91dd\u5c0d\u6027\u5fae\u8abf\u3002\u5efa\u8b70\u64f4\u5927\u4efb\u52d9\u96c6\u3001\u7814\u7a76\u81ea\u52d5\u63d0\u793a\u751f\u6210\u8207\u4eba\u6a5f\u4e92\u52d5\u6d41\u7a0b\u4f86\u7e2e\u5c0f\u5dee\u8ddd\u3002"}}
