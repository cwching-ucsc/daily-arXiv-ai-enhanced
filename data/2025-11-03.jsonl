{"id": "2510.26913", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.26913", "abs": "https://arxiv.org/abs/2510.26913", "authors": ["Junyi Shen", "Noppanat Wadlom", "Lingfeng Zhou", "Dequan Wang", "Xu Miao", "Lei Fang", "Yao Lu"], "title": "FlowMesh: A Service Fabric for Composable LLM Workflows", "comment": null, "summary": "AI deployment increasingly resembles a pipeline of data transformation,\nfine-tuning, and agent interactions rather than a monolithic LLM job; recent\nexamples include RLHF/RLAIF training and agentic workflows. To cope with this\nshift, we propose FlowMesh, a multi-tenant service fabric that executes and\noptimizes these workloads as one shared service instead of isolated pipelines.\nIt decomposes workflows into fine-grained operators with recorded lineage,\nenabling de-duplication of work across users and batching requests on the same\nhardware while preserving per-workflow provenance. A global control plane\nmaintains a cluster-wide pool of ready operators and uses a single utility\nfunction to pick both the batch and the worker, balancing throughput, cost, and\ndata locality on heterogeneous GPUs. The data plane is an elastic fleet of\nstateless workers backed by a content-addressable store, enabling rapid,\nautomatic scale-out, safe retry after preemption, and portability across\nmanaged clusters such as Kubernetes and geo-distributed GPU marketplaces such\nas Vast.ai. Compared with baseline solutions, FlowMesh achieves up to 3.8x cost\nreduction and 2.0x lower energy usage, provides a similar or better latency\nprofile, and remains efficient under dynamic and failure-prone conditions."}
{"id": "2510.27039", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.27039", "abs": "https://arxiv.org/abs/2510.27039", "authors": ["Zhuo Zheng", "Lingran Meng", "Ziyu Lin"], "title": "A Cloud-Based Spatio-Temporal GNN-Transformer Hybrid Model for Traffic Flow Forecasting with External Feature Integration", "comment": null, "summary": "Accurate traffic flow forecasting is essential for the development of\nintelligent transportation systems (ITS), supporting tasks such as traffic\nsignal optimization, congestion management, and route planning. Traditional\nmodels often fail to effectively capture complex spatial-temporal dependencies\nin large-scale road networks, especially under the influence of external\nfactors such as weather, holidays, and traffic accidents. To address this\nchallenge, this paper proposes a cloud-based hybrid model that integrates\nSpatio-Temporal Graph Neural Networks (ST-GNN) with a Transformer architecture\nfor traffic flow prediction. The model leverages the strengths of GNNs in\nmodeling spatial correlations across road networks and the Transformers'\nability to capture long-term temporal dependencies. External contextual\nfeatures are incorporated via feature fusion to enhance predictive accuracy.\nThe proposed model is deployed on a cloud computing platform to achieve\nscalability and real-time adaptability. Experimental evaluation of the dataset\nshows that our model outperforms baseline methods (LSTM, TCN, GCN, pure\nTransformer) with an RMSE of only 17.92 and a MAE of only 10.53. These findings\nsuggest that the hybrid GNN-Transformer approach provides an effective and\nscalable solution for cloud-based ITS applications, offering methodological\nadvancements for traffic flow forecasting and practical implications for\ncongestion mitigation."}
{"id": "2510.27257", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.27257", "abs": "https://arxiv.org/abs/2510.27257", "authors": ["Mengshi Qi", "Jiaxuan Peng", "Jie Zhang", "Juan Zhu", "Yong Li", "Huadong Ma"], "title": "Synergistic Tensor and Pipeline Parallelism", "comment": null, "summary": "In the machine learning system, the hybrid model parallelism combining tensor\nparallelism (TP) and pipeline parallelism (PP) has become the dominant solution\nfor distributed training of Large Language Models~(LLMs) and Multimodal LLMs\n(MLLMs). However, TP introduces significant collective communication overheads,\nwhile PP suffers from synchronization inefficiencies such as pipeline bubbles.\nExisting works primarily address these challenges from isolated perspectives,\nfocusing either on overlapping TP communication or on flexible PP scheduling to\nmitigate pipeline bubbles. In this paper, we propose a new synergistic tensor\nand pipeline parallelism schedule that simultaneously reduces both types of\nbubbles. Our proposed schedule decouples the forward and backward passes in PP\ninto fine-grained computation units, which are then braided to form a composite\ncomputation sequence. This compositional structure enables near-complete\nelimination of TP-related bubbles. Building upon this structure, we further\ndesign the PP schedule to minimize PP bubbles. Experimental results demonstrate\nthat our approach improves training throughput by up to 12% for LLMs and 16%\nfor MLLMs compared to existing scheduling methods. Our source code is avaiable\nat https://github.com/MICLAB-BUPT/STP."}
{"id": "2510.27289", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.27289", "abs": "https://arxiv.org/abs/2510.27289", "authors": ["Zhengchang Hua", "Panagiotis Oikonomou", "Karim Djemame", "Nikos Tziritas", "Georgios Theodoropoulos"], "title": "A Digital Twin-based Multi-Agent Reinforcement Learning Framework for Vehicle-to-Grid Coordination", "comment": "16 pages, 8 figures. Accepted by the 25th International Conference on\n  Algorithms and Architectures for Parallel Processing (ICA3PP'25)", "summary": "The coordination of large-scale, decentralised systems, such as a fleet of\nElectric Vehicles (EVs) in a Vehicle-to-Grid (V2G) network, presents a\nsignificant challenge for modern control systems. While collaborative Digital\nTwins have been proposed as a solution to manage such systems without\ncompromising the privacy of individual agents, deriving globally optimal\ncontrol policies from the high-level information they share remains an open\nproblem. This paper introduces Digital Twin Assisted Multi-Agent Deep\nDeterministic Policy Gradient (DT-MADDPG) algorithm, a novel hybrid\narchitecture that integrates a multi-agent reinforcement learning framework\nwith a collaborative DT network. Our core contribution is a simulation-assisted\nlearning algorithm where the centralised critic is enhanced by a predictive\nglobal model that is collaboratively built from the privacy-preserving data\nshared by individual DTs. This approach removes the need for collecting\nsensitive raw data at a centralised entity, a requirement of traditional\nmulti-agent learning algorithms. Experimental results in a simulated V2G\nenvironment demonstrate that DT-MADDPG can achieve coordination performance\ncomparable to the standard MADDPG algorithm while offering significant\nadvantages in terms of data privacy and architectural decentralisation. This\nwork presents a practical and robust framework for deploying intelligent,\nlearning-based coordination in complex, real-world cyber-physical systems."}
{"id": "2510.27027", "categories": ["cs.NI", "C.2.1; C.2.m"], "pdf": "https://arxiv.org/pdf/2510.27027", "abs": "https://arxiv.org/abs/2510.27027", "authors": ["Martin Ottens", "Kai-Steffen Hielscher", "Reinhard German"], "title": "Trace-driven Path Emulation of Satellite Networks using Hypatia", "comment": "Preprint, 15 pages, 15 figures, 6 tables, 1 listing", "summary": "The increasing prevalence LEO satellite mega-constellations for global\nInternet coverage requires new approaches to evaluate the behavior of existing\nInternet protocols and applications. Traditional discrete event simulators like\nHypatia allow for modeling these environments but fall short in evaluating real\napplications. This paper builds upon our previous work, in which we proposed a\nsystem design for trace-driven emulation of such satellite networks, bridging\nthe gab between simulations and real-time testbeds. By extending the Hypatia\nframework, we record network path characteristics, e.g., delay and bandwidth,\nbetween two endpoints in the network during non-real-time simulations. Path\ncharacteristics are exported to Trace Files, which are replayed in real-time\nemulation environments on real systems, enabling evaluations with real software\nand human interaction. An advantage of our approach is its easy adaptability to\nexisting simulation models. Our extensive evaluation involves multiple\nscenarios with different satellite constellations, illustrating the approach's\naccuracy in reproducing the behavior of satellite networks. Between full\nsimulation, which serves as a baseline for our evaluation, and emulation runs,\nwe observe high correlation metrics of up to 0.96, validating the approach's\neffectiveness. Challenges such as the lack of emulation-to-simulation feedback\nand synchronization issues are discussed."}
{"id": "2510.27317", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.27317", "abs": "https://arxiv.org/abs/2510.27317", "authors": ["Shuyi Chen", "Panagiotis Oikonomou", "Zhengchang Hua", "Nikos Tziritas", "Karim Djemame", "Nan Zhang", "Georgios Theodoropoulos"], "title": "Dynamic Service Scheduling and Resource Management in Energy-Harvesting Multi-access Edge Computing", "comment": "Accepted by the 21st IEEE International Conference on Green Computing\n  and Communications (GreenCom 2025)", "summary": "Multi-access Edge Computing (MEC) delivers low-latency services by hosting\napplications near end-users. To promote sustainability, these systems are\nincreasingly integrated with renewable Energy Harvesting (EH) technologies,\nenabling operation where grid electricity is unavailable. However, balancing\nthe intermittent nature of harvested energy with dynamic user demand presents a\nsignificant resource allocation challenge. This work proposes an online\nstrategy for an MEC system powered exclusively by EH to address this trade-off.\nOur strategy dynamically schedules computational tasks with dependencies and\ngoverns energy consumption through real-time decisions on server frequency\nscaling and service module migration. Experiments using real-world datasets\ndemonstrate our algorithm's effectiveness in efficiently utilizing harvested\nenergy while maintaining low service latency."}
{"id": "2510.27057", "categories": ["cs.NI", "C.2.m; D.4.4"], "pdf": "https://arxiv.org/pdf/2510.27057", "abs": "https://arxiv.org/abs/2510.27057", "authors": ["Martin Ottens", "Kai-Steffen Hielscher", "Reinhard German"], "title": "TheaterQ: A Qdisc for Dynamic Network Emulation", "comment": "Preprint, 5 pages, 5 figures, 2 listings", "summary": "TheaterQ is a Linux qdisc designed for dynamic network emulation, addressing\nthe limitations of static parameters in traditional tools like NetEm. By\nutilizing Trace Files containing timelines with network characteristics,\nTheaterQ achieves high-accuracy emulation of dynamic networks without involving\nthe userspace and allows for resolutions of characteristic updates of up to 1\nmicrosecond. Features include synchronization across mutliple qdisc instances\nand handling of delays, bandwidth, packet loss, duplication, and reordering.\nEvaluations show TheaterQ's accuracy and its comparable performance to existing\ntools, offering a flexible solution for modern communication protocol\ndevelopment. TheaterQ is available as open-source software under the GPLv2\nlicense."}
{"id": "2510.27351", "categories": ["cs.DC", "65Y05, 65Y10, 90C59, 68T20"], "pdf": "https://arxiv.org/pdf/2510.27351", "abs": "https://arxiv.org/abs/2510.27351", "authors": ["Milena Veneva"], "title": "ML-Based Optimum Sub-system Size Heuristic for the GPU Implementation of the Tridiagonal Partition Method", "comment": "10 pages, 6 figures, 4 tables, DLCP conference 2025, Moscow, Russia", "summary": "This paper presents a machine learning (ML)-based heuristic for finding the\noptimum sub-system size for the CUDA implementation of the parallel partition\nalgorithm. Computational experiments for different system of linear algebraic\nequation (SLAE) sizes are conducted, and the optimum sub-system size for each\nof them is found empirically. To estimate a model for the sub-system size, we\nperform the k-nearest neighbors (kNN) classification method. Statistical\nanalysis of the results is done. By comparing the predicted values with the\nactual data, the algorithm is deemed to be acceptably good. Next, the heuristic\nis expanded to work for the recursive parallel partition algorithm as well. An\nalgorithm for determining the optimum sub-system size for each recursive step\nis formulated. A kNN model for predicting the optimum number of recursive steps\nfor a particular SLAE size is built."}
{"id": "2510.27108", "categories": ["cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.27108", "abs": "https://arxiv.org/abs/2510.27108", "authors": ["Shuo Zhu", "Siyu Lin"], "title": "Analytical Model of NR-V2X Mode 2 with Re-Evaluation Mechanism", "comment": "6 pages, 7 figures, conference", "summary": "Massive message transmissions, unpredictable aperiodic messages, and\nhigh-speed moving vehicles contribute to the complex wireless environment,\nresulting in inefficient resource collisions in Vehicle to Everything (V2X). In\norder to achieve better medium access control (MAC) layer performance, 3GPP\nintroduced several new features in NR-V2X. One of the most important is the\nre-evaluation mechanism. It allows the vehicle to continuously sense resources\nbefore message transmission to avoid resource collisions. So far, only a few\narticles have studied the re-evaluation mechanism of NR-V2X, and they mainly\nfocus on network simulator that do not consider variable traffic, which makes\nanalysis and comparison difficult. In this paper, an analytical model of NR-V2X\nMode 2 is established, and a message generator is constructed by using discrete\ntime Markov chain (DTMC) to simulate the traffic pattern recommended by 3GPP\nadvanced V2X services. Our study shows that the re-evaluation mechanism\nimproves the reliability of NR-V2X transmission, but there are still local\nimprovements needed to reduce latency."}
{"id": "2510.27656", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.27656", "abs": "https://arxiv.org/abs/2510.27656", "authors": ["Nandor Licker", "Kevin Hu", "Vladimir Zaytsev", "Lequn Chen"], "title": "RDMA Point-to-Point Communication for LLM Systems", "comment": null, "summary": "Emerging Large Language Model (LLM) system patterns, such as disaggregated\ninference, Mixture-of-Experts (MoE) routing, and asynchronous reinforcement\nfine-tuning, require flexible point-to-point communication beyond simple\ncollectives. Existing implementations are locked to specific Network Interface\nControllers (NICs), hindering integration into inference engines and\nportability across hardware providers. We present TransferEngine, which bridges\nthe functionality of common NICs to expose a uniform interface. TransferEngine\nexposes one-sided WriteImm operations with a ImmCounter primitive for\ncompletion notification, without ordering assumptions of network transport,\ntransparently managing multiple NICs per GPU. We demonstrate peak throughput of\n400 Gbps on both NVIDIA ConnectX-7 and AWS Elastic Fabric Adapter (EFA). We\nshowcase TransferEngine through three production systems: (1) KvCache transfer\nfor disaggregated inference with dynamic scaling, (2) RL weight updates\nachieving 1.3 seconds for trillion-parameter models, and (3) MoE\ndispatch/combine implementation exceeding DeepEP decode latency on ConnectX-7,\nwith the first viable latencies on EFA. We demonstrate that our portable\npoint-to-point communication complements collectives while avoiding lock-in."}
{"id": "2510.27111", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.27111", "abs": "https://arxiv.org/abs/2510.27111", "authors": ["Yunfeng Jiang", "Zhiming Huang", "Jianping Pan"], "title": "Stochastic Geometry of Cylinders: Characterizing Inter-Nodal Distances for 3D UAV Networks", "comment": "8 pages, 6 figures", "summary": "The analytical characterization of coverage probability in finite\nthree-dimensional wireless networks has long remained an open problem, hindered\nby the loss of spatial independence in finite-node settings and the coupling\nbetween link distances and interference in bounded geometries. This paper\ncloses this gap by presenting the first exact analytical framework for coverage\nprobability in finite 3D networks modeled by a binomial point process within a\ncylindrical region. To bypass the intractability that has long hindered such\nanalyses, we leverage the independence structure, convolution geometry, and\nderivative properties of Laplace transforms, yielding a formulation that is\nboth mathematically exact and computationally efficient. Extensive Monte Carlo\nsimulations verify the analysis and demonstrate significant accuracy gains over\nconventional Poisson-based models. The results generalize to any confined 3D\nwireless system, including aerial, underwater, and robotic networks."}
{"id": "2510.27121", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.27121", "abs": "https://arxiv.org/abs/2510.27121", "authors": ["Luis Antonio L. F. da Costa", "Rodrigo C. de Lamare", "Rafael Kunst", "Edison Pignaton de Freitas"], "title": "Study of Cluster-Based Routing Based on Machine Learning for UAV Networks in 6G", "comment": "12 pages, 9 figures", "summary": "The sixth generation (6G) wireless networks are envisioned to deliver\nultra-low latency, massive connectivity, and high data rates, enabling advanced\napplications such as autonomous {unmaned aerial vehicles (UAV)} swarms and\naerial edge computing. However, realizing this vision in Flying Ad Hoc Networks\n(FANETs) requires intelligent and adaptive clustering mechanisms to ensure\nefficient routing and resource utilization. This paper proposes a novel machine\nlearning-driven framework for dynamic cluster formation and cluster head\nselection in 6G-enabled FANETs. The system leverages mobility prediction using\n{Extreme Gradient Boosting (XGBoost)} and a composite optimization strategy\nbased on signal strength and spatial proximity to identify optimal cluster\nheads. To evaluate the proposed method, comprehensive simulations were\nconducted in both centralized (5G) and decentralized (6G) topologies using\nrealistic video traffic patterns. Results show that the proposed model achieves\nsignificant improvements in delay, jitter, and throughput in decentralized\nscenarios. These findings demonstrate the potential of combining machine\nlearning with clustering techniques to enhance scalability, stability, and\nperformance in next-generation aerial networks."}
{"id": "2510.27137", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.27137", "abs": "https://arxiv.org/abs/2510.27137", "authors": ["Minh Phu Vuong", "Chul-Ho Lee", "Do Young Eun"], "title": "Effective Delayed Patching for Transient Malware Control on Networks", "comment": "IEEE MASS 2025", "summary": "Patching nodes is an effective network defense strategy for malware control\nat early stages, and its performance is primarily dependent on how accurately\nthe infection propagation is characterized. In this paper, we aim to design a\nnovel patching policy based on the susceptible-infected epidemic network model\nby incorporating the influence of patching delay--the type of delay that has\nbeen largely overlooked in designing patching policies in the literature, while\nbeing prevalent in practice. We first identify 'critical edges' that form a\nboundary to separate the most likely infected nodes from the nodes which would\nstill remain healthy after the patching delay. We next leverage the critical\nedges to determine which nodes to be patched in light of limited patching\nresources at early stages. To this end, we formulate a constrained graph\npartitioning problem and use its solution to identify a set of nodes to patch\nor vaccinate under the limited resources, to effectively prevent malware\npropagation from getting through the healthy region. We numerically validate\nthat our patching policy significantly outperforms other baseline policies in\nprotecting the healthy nodes under limited patching resources and in the\npresence of patching delay."}
{"id": "2510.27325", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.27325", "abs": "https://arxiv.org/abs/2510.27325", "authors": ["Marius Feldmann", "Tobias NÃ¶thlich", "Felix Walter", "Maximilian Nitsch", "Juan A. Fraire", "Georg A. Murzik", "Fiona Fuchs"], "title": "Selected Results from the REDMARS2 Project: Recursive Delay-Tolerant Networking using Bundle-in-Bundle Encapsulation", "comment": null, "summary": "This whitepaper presents parts of the results of the REDMARS2 project\nconducted in 2021-2022, exploring the integration of Recursive Internetwork\nArchitecture (RINA) concepts into Delay- and Disruption-Tolerant Networking\n(DTN) protocols. Using Bundle-in-Bundle Encapsulation (BIBE), we implemented\nscope-based separation mechanisms resulting in scalable DTNs. A key\ncontribution of this work is the demonstration of practical BIBE-based use\ncases, including a realistic Solar System Internet communication scenario\ninvolving unmanned aerial vehicles (UAVs) and satellite relays. The evaluation,\nsupported by field tests in collaboration with the European Space Agency (ESA),\nconfirmed the viability of BIBE as a foundation for scalable, recursive, and\ninteroperable DTN architectures."}
{"id": "2510.27500", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.27500", "abs": "https://arxiv.org/abs/2510.27500", "authors": ["Dennis Trautwein", "Cornelius Ihle", "Moritz Schubotz", "Bela Gipp"], "title": "Challenging Tribal Knowledge -- Large Scale Measurement Campaign on Decentralized NAT Traversal", "comment": null, "summary": "The promise of decentralized peer-to-peer (P2P) systems is fundamentally\ngated by the challenge of Network Address Translation (NAT) traversal, with\nexisting solutions often reintroducing the very centralization they seek to\navoid. This paper presents the first large-scale, longitudinal measurement\nstudy of a fully decentralized NAT traversal protocol, Direct Connection\nUpgrade through Relay (DCUtR), within the production libp2p-based IPFS network.\nDrawing on over 4.4 million traversal attempts from 85,000+ distinct networks\nacross 167 countries, we provide a definitive empirical analysis of modern P2P\nconnectivity. We establish a contemporary baseline success rate of $70\\% \\pm\n7.1\\%$ for the hole-punching stage, providing a crucial new benchmark for the\nfield. Critically, we empirically refute the long-held 'tribal knowledge' of\nUDP's superiority for NAT traversal, demonstrating that DCUtR's high-precision,\nRTT-based synchronization yields statistically indistinguishable success rates\nfor both TCP and QUIC ($\\sim70\\%$). Our analysis further validates the\nprotocol's design for permissionless environments by showing that success is\nindependent of relay characteristics and that the mechanism is highly\nefficient, with $97.6\\%$ of successful connections established on the first\nattempt. Building on this analysis, we propose a concrete roadmap of protocol\nenhancements aimed at achieving universal connectivity and contribute our\ncomplete dataset to foster further research in this domain."}
{"id": "2510.27506", "categories": ["cs.NI", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.27506", "abs": "https://arxiv.org/abs/2510.27506", "authors": ["Ke He", "Thang X. Vu", "Le He", "Lisheng Fan", "Symeon Chatzinotas", "Bjorn Ottersten"], "title": "Asynchronous Risk-Aware Multi-Agent Packet Routing for Ultra-Dense LEO Satellite Networks", "comment": null, "summary": "The rise of ultra-dense LEO constellations creates a complex and asynchronous\nnetwork environment, driven by their massive scale, dynamic topologies, and\nsignificant delays. This unique complexity demands an adaptive packet routing\nalgorithm that is asynchronous, risk-aware, and capable of balancing diverse\nand often conflicting QoS objectives in a decentralized manner. However,\nexisting methods fail to address this need, as they typically rely on\nimpractical synchronous decision-making and/or risk-oblivious approaches. To\ntackle this gap, we introduce PRIMAL, an event-driven multi-agent routing\nframework designed specifically to allow each satellite to act independently on\nits own event-driven timeline, while managing the risk of worst-case\nperformance degradation via a principled primal-dual approach. This is achieved\nby enabling agents to learn the full cost distribution of the targeted QoS\nobjectives and constrain tail-end risks. Extensive simulations on a LEO\nconstellation with 1584 satellites validate its superiority in effectively\noptimizing latency and balancing load. Compared to a recent risk-oblivious\nbaseline, it reduces queuing delay by over 70%, and achieves a nearly 12 ms\nend-to-end delay reduction in loaded scenarios. This is accomplished by\nresolving the core conflict between naive shortest-path finding and congestion\navoidance, highlighting such autonomous risk-awareness as a key to robust\nrouting."}
{"id": "2510.27664", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.27664", "abs": "https://arxiv.org/abs/2510.27664", "authors": ["Niloy Saha", "Noura Limam", "Yang Xiao", "Raouf Boutaba"], "title": "Rethinking Telemetry Design for Fine-Grained Anomaly Detection in 5G User Planes", "comment": null, "summary": "Detecting QoS anomalies in 5G user planes requires fine-grained per-flow\nvisibility, but existing telemetry approaches face a fundamental trade-off.\nCoarse per-class counters are lightweight but mask transient and per-flow\nanomalies, while per-packet telemetry postcards provide full visibility at\nprohibitive cost that grows linearly with line rate. Selective postcard schemes\nreduce overhead but miss anomalies that fall below configured thresholds or\noccur during brief intervals. We present Kestrel, a sketch-based telemetry\nsystem for 5G user planes that provides fine-grained visibility into key metric\ndistributions such as latency tails and inter-arrival times at a fraction of\nthe cost of per-packet postcards. Kestrel extends Count-Min Sketch with\nhistogram-augmented buckets and per-queue partitioning, which compress\nper-packet measurements into compact summaries while preserving\nanomaly-relevant signals. We develop formal detectability guarantees that\naccount for sketch collisions, yielding principled sizing rules and binning\nstrategies that maximize anomaly separability. Our evaluations on a 5G testbed\nwith Intel Tofino switches show that Kestrel achieves 10% better detection\naccuracy than existing selective postcard schemes while reducing export\nbandwidth by 10x."}
