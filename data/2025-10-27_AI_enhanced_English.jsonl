{"id": "2510.21130", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.21130", "abs": "https://arxiv.org/abs/2510.21130", "authors": ["Qi Deng", "Yinghao Zhang", "Yalin Liu", "Bishenghui Tao"], "title": "A Confidence-Constrained Cloud-Edge Collaborative Framework for Autism Spectrum Disorder Diagnosis", "comment": "10 pages, 2 figures", "summary": "Autism Spectrum Disorder (ASD) diagnosis systems in school environments\nincreasingly relies on IoT-enabled cameras, yet pure cloud processing raises\nprivacy and latency concerns while pure edge inference suffers from limited\naccuracy. We propose Confidence-Constrained Cloud-Edge Knowledge Distillation\n(C3EKD), a hierarchical framework that performs most inference at the edge and\nselectively uploads only low-confidence samples to the cloud. The cloud\nproduces temperature-scaled soft labels and distils them back to edge models\nvia a global loss aggregated across participating schools, improving\ngeneralization without centralizing raw data. On two public ASD facial-image\ndatasets, the proposed framework achieves a superior accuracy of 87.4\\%,\ndemonstrating its potential for scalable deployment in real-world applications.", "AI": {"tldr": "C3EKD is a hierarchical cloud-edge knowledge distillation framework for ASD facial-image diagnosis in schools: edge models handle most inferences and send only low-confidence cases to the cloud, which returns temperature-scaled soft labels and distils improvements back to edges using a global aggregated loss, improving accuracy (87.4%) while reducing privacy/latency trade-offs.", "motivation": "Address privacy and latency issues of cloud-only ASD diagnosis from IoT cameras, and the limited accuracy of edge-only inference, by combining cloud knowledge and edge deployment without centralizing raw data.", "method": "Edge devices perform inference and send only low-confidence samples to the cloud. The cloud computes temperature-scaled soft labels and aggregates a global loss across participating schools; distilled knowledge is sent back to edge models to improve generalization. This selective upload and global distillation enable improved edge accuracy without sharing raw data.", "result": "On two public ASD facial-image datasets, C3EKD achieved 87.4% accuracy, outperforming baselines (edge-only and cloud-only) while reducing data transfer and maintaining privacy by avoiding centralization of raw images.", "conclusion": "C3EKD provides a scalable compromise between privacy, latency, and accuracy for ASD diagnosis in school IoT settings by leveraging selective cloud assistance and aggregated knowledge distillation to enhance edge models without raw-data centralization."}}
{"id": "2510.20931", "categories": ["cs.DC", "cs.AR", "C.1.4; C.4"], "pdf": "https://arxiv.org/pdf/2510.20931", "abs": "https://arxiv.org/abs/2510.20931", "authors": ["Albert Reuther", "Peter Michaleas", "Michael Jones", "Vijay Gadepally", "Jeremy Kepner"], "title": "Lincoln AI Computing Survey (LAICS) and Trends", "comment": "12 pages, 7 figures, 2025 IEEE High Performance Extreme Computing\n  (HPEC) conference, September 2025", "summary": "In the past year, generative AI (GenAI) models have received a tremendous\namount of attention, which in turn has increased attention to computing systems\nfor training and inference for GenAI. Hence, an update to this survey is due.\nThis paper is an update of the survey of AI accelerators and processors from\npast seven years, which is called the Lincoln AI Computing Survey -- LAICS\n(pronounced \"lace\"). This multi-year survey collects and summarizes the current\ncommercial accelerators that have been publicly announced with peak performance\nand peak power consumption numbers. In the same tradition of past papers of\nthis survey, the performance and power values are plotted on a scatter graph,\nand a number of dimensions and observations from the trends on this plot are\nagain discussed and analyzed. Market segments are highlighted on the scatter\nplot, and zoomed plots of each segment are also included. A brief description\nof each of the new accelerators that have been added in the survey this year is\nincluded, and this update features a new categorization of computing\narchitectures that implement each of the accelerators.", "AI": {"tldr": "Annual update of the Lincoln AI Computing Survey (LAICS) cataloging publicly announced commercial AI accelerators with peak performance and power numbers, plotted on performance-vs-power scatter plots, with market-segment zooms, short descriptions of new devices, and a new architecture categorization.", "motivation": "Recent surge of interest in generative AI has increased focus on training and inference hardware; the authors aim to update a multi-year survey to reflect new commercial accelerators and architectural trends.", "method": "Collect peak performance and peak power consumption for publicly announced commercial accelerators; plot these on scatter graphs; highlight market segments and produce zoomed views; add brief device descriptions and introduce a new classification of computing architectures used by the accelerators.", "result": "An expanded dataset of accelerators for the past year, updated scatter plots showing device trends across performance and power dimensions, segmented/zoomed plots for market niches, and a new taxonomy of accelerator architectures; qualitative observations and analysis of trends.", "conclusion": "Provides an updated, visual comparative resource for AI accelerators, useful to researchers and practitioners tracking hardware trends for GenAI training and inference; introduces a new architecture categorization to aid comparison."}}
{"id": "2510.21155", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21155", "abs": "https://arxiv.org/abs/2510.21155", "authors": ["Dandan Liang", "Jianing Zhang", "Evan Chen", "Zhe Li", "Rui Li", "Haibo Yang"], "title": "Towards Straggler-Resilient Split Federated Learning: An Unbalanced Update Approach", "comment": null, "summary": "Split Federated Learning (SFL) enables scalable training on edge devices by\ncombining the parallelism of Federated Learning (FL) with the computational\noffloading of Split Learning (SL). Despite its great success, SFL suffers\nsignificantly from the well-known straggler issue in distributed learning\nsystems. This problem is exacerbated by the dependency between Split Server and\nclients: the Split Server side model update relies on receiving activations\nfrom clients. Such synchronization requirement introduces significant time\nlatency, making straggler a critical bottleneck to the scalability and\nefficiency of the system. To mitigate this problem, we propose MU-SplitFed, a\nstraggler-resilient SFL algorithm in zeroth-order optimization that decouples\ntraining progress from straggler delays via a simple yet effective unbalanced\nupdate mechanism.\n  By enabling the server to perform $\\tau$ local updates per client round,\nMU-SplitFed achieves a convergence rate of $O(\\sqrt{d/(\\tau T)})$ for\nnon-convex objectives, demonstrating a linear speedup of $\\tau$ in\ncommunication rounds. Experiments demonstrate that MU-SplitFed consistently\noutperforms baseline methods with the presence of stragglers and effectively\nmitigates their impact through adaptive tuning of $\\tau$. The code for this\nproject is available at https://github.com/Johnny-Zip/MU-SplitFed.", "AI": {"tldr": "Proposes MU-SplitFed, a straggler-resilient Split Federated Learning algorithm that lets the split server perform \u03c4 local updates per client round (unbalanced updates) under zeroth-order optimization; achieves convergence O(\u221a(d/(\u03c4T))) for non-convex objectives and shows empirical gains under straggler conditions.", "motivation": "SFL faces severe delays from stragglers because the server update depends on client activations; synchronization causes latency and limits scalability. The work aims to decouple server progress from slow clients to mitigate straggler effects.", "method": "Introduce an unbalanced update mechanism where the Split Server runs \u03c4 local updates each client round, combined with a zeroth-order optimization approach suitable for cases where gradients are unavailable. This decouples server training progress from straggler-induced waits.", "result": "Theoretically proves a convergence rate of O(\u221a(d/(\u03c4T))) for non-convex objectives, implying linear speedup in communication rounds with larger \u03c4. Empirical evaluations show MU-SplitFed outperforms baselines under straggler scenarios and that adaptive tuning of \u03c4 effectively reduces straggler impact. Code is provided.", "conclusion": "MU-SplitFed is an effective, theoretically-grounded method to reduce straggler bottlenecks in Split Federated Learning by allowing unbalanced server updates; it yields provable speedups and practical improvements in experiments."}}
{"id": "2510.21183", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.21183", "abs": "https://arxiv.org/abs/2510.21183", "authors": ["Anwesha Mukherjee", "Rajkumar Buyya"], "title": "Generative Federated Learning for Smart Prediction and Recommendation Applications", "comment": null, "summary": "This paper proposes a generative adversarial network and federated\nlearning-based model to address various challenges of the smart prediction and\nrecommendation applications, such as high response time, compromised data\nprivacy, and data scarcity. The integration of the generative adversarial\nnetwork and federated learning is referred to as Generative Federated Learning\n(GFL). As a case study of the proposed model, a heart health monitoring\napplication is considered. The realistic synthetic datasets are generated using\nthe generated adversarial network-based proposed algorithm for improving data\ndiversity, data quality, and data augmentation, and remove the data scarcity\nand class imbalance issues. In this paper, we implement the centralized and\ndecentralized federated learning approaches in an edge computing paradigm. In\ncentralized federated learning, the edge nodes communicate with the central\nserver to build the global and personalized local models in a collaborative\nmanner. In the decentralized federated learning approach, the edge nodes\ncommunicate among themselves to exchange model updates for collaborative\ntraining. The comparative study shows that the proposed framework outperforms\nthe existing heart health monitoring applications. The results show that using\nthe proposed framework (i) the prediction accuracy is improved by 12% than the\nconventional framework, and (ii) the response time is reduced by 73% than the\nconventional cloud-only system.", "AI": {"tldr": "Proposes Generative Federated Learning (GAN + federated learning) for low-latency, privacy-preserving heart health prediction; uses GANs to generate synthetic data and implements centralized and decentralized FL on edge. Reports +12% accuracy and \u221273% response time versus conventional baselines.", "motivation": "Smart prediction/recommendation systems face high response latency, privacy risks when centralizing health data, and data scarcity/class imbalance. The paper targets these by combining generative models with federated learning at the edge.", "method": "Introduce Generative Federated Learning (GFL): train a GAN to create realistic synthetic heart-health data for augmentation and class-balance, then perform federated learning in two modes: centralized (edge nodes \u2194 central server) and decentralized (peer-to-peer model update exchanges). Implement the system on an edge computing paradigm and evaluate on the heart-monitoring case study.", "result": "The proposed framework outperforms prior heart-health monitoring approaches in the experiments: prediction accuracy improved by 12% over a conventional framework and response time reduced by 73% compared with a cloud-only system.", "conclusion": "GFL can mitigate data scarcity and privacy concerns while lowering latency and improving accuracy for edge-based health monitoring. Further work should validate synthetic-data fidelity, privacy guarantees, communication costs, and robustness to adversarial or non-iid conditions."}}
