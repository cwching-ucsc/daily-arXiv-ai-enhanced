{"id": "2511.03866", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PF", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.03866", "abs": "https://arxiv.org/abs/2511.03866", "authors": ["Arijit Bhattacharjee", "Ali TehraniJamsaz", "Le Chen", "Niranjan Hasabnis", "Mihai Capota", "Nesreen Ahmed", "Ali Jannesari"], "title": "OMPILOT: Harnessing Transformer Models for Auto Parallelization to Shared Memory Computing Paradigms", "comment": null, "summary": "Recent advances in large language models (LLMs) have significantly\naccelerated progress in code translation, enabling more accurate and efficient\ntransformation across programming languages. While originally developed for\nnatural language processing, LLMs have shown strong capabilities in modeling\nprogramming language syntax and semantics, outperforming traditional rule-based\nsystems in both accuracy and flexibility. These models have streamlined\ncross-language conversion, reduced development overhead, and accelerated legacy\ncode migration. In this paper, we introduce OMPILOT, a novel domain-specific\nencoder-decoder transformer tailored for translating C++ code into OpenMP,\nenabling effective shared-memory parallelization. OMPILOT leverages custom\npre-training objectives that incorporate the semantics of parallel constructs\nand combines both unsupervised and supervised learning strategies to improve\ncode translation robustness. Unlike previous work that focused primarily on\nloop-level transformations, OMPILOT operates at the function level to capture a\nwider semantic context. To evaluate our approach, we propose OMPBLEU, a novel\ncomposite metric specifically crafted to assess the correctness and quality of\nOpenMP parallel constructs, addressing limitations in conventional translation\nmetrics.", "AI": {"tldr": "Proposes OMPILOT, a domain-specific encoder\u2013decoder transformer for translating C++ to OpenMP parallel code using custom pretraining that encodes parallel semantics and combined unsupervised/supervised training; introduces OMPBLEU, a metric for evaluating OpenMP constructs. Claims function-level translation and improved robustness over prior loop-level approaches.", "motivation": "Automate and improve C++\u2192OpenMP translation to accelerate shared-memory parallelization and legacy code modernization, leveraging LLM strengths in code understanding while addressing limitations of rule-based and loop-focused translators.", "method": "Design a domain-specific encoder\u2013decoder transformer (OMPILOT) with custom pretraining objectives that encode semantics of parallel constructs; use a mixture of unsupervised and supervised learning; operate at function granularity to capture broader context beyond loop-level transforms.", "result": "Presents OMPILOT and the OMPBLEU composite metric tailored to assess correctness and quality of OpenMP parallel constructs. The abstract claims improved robustness and broader semantic capture, but provides no quantitative results or dataset/benchmark details.", "conclusion": "The approach is promising\u2014specialized modeling plus a targeted metric could improve automated parallelization\u2014but the abstract lacks experimental validation, ablation studies, dataset description, and comparison baselines; these are needed to substantiate claims."}}
{"id": "2511.04477", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.04477", "abs": "https://arxiv.org/abs/2511.04477", "authors": ["Rongxiang Wang", "Kangyuan Shu", "Felix Xiaozhu Lin"], "title": "Enabling Dynamic Sparsity in Quantized LLM Inference", "comment": null, "summary": "Deploying large language models (LLMs) on end-user devices is gaining\nimportance due to benefits in responsiveness, privacy, and operational cost.\nYet the limited memory and compute capability of mobile and desktop GPUs make\nefficient execution difficult. Recent observations suggest that the internal\nactivations of LLMs are often dynamically sparse, meaning that for each input,\nonly part of the network contributes significantly to the output. Such sparsity\ncould reduce computation, but it interacts poorly with group-wise quantization,\nwhich remains the dominant approach for fitting LLMs onto resource-constrained\nhardware. To reconcile these two properties, this study proposes a set of\ntechniques that realize dynamic sparse inference under low-bit quantization.\nThe method features: (1) a zigzag-patterned quantization layout that organizes\nweights in a way consistent with activation sparsity and improves GPU memory\nlocality; (2) a specialized GEMV kernel designed for this layout to fully\nutilize parallel compute units; and (3) a compact runtime mechanism that\ngathers sparse indices with minimal overhead. Across several model scales and\nhardware configurations, the approach achieves up to 1.55x faster decoding\nthroughput while maintaining accuracy comparable to dense quantized inference,\nshowing that structured sparsity and quantization can effectively coexist on\ncommodity GPUs.", "AI": {"tldr": "Introduces methods to combine dynamic activation sparsity with low-bit, group-wise quantization for LLM inference on commodity GPUs by reordering weight layout (zigzag), a custom GEMV kernel, and a lightweight sparse-index gatherer, yielding up to 1.55x decoding throughput with similar accuracy to dense quantized baselines.", "motivation": "LLM inference on end-user devices is constrained by memory and compute; dynamic activation sparsity offers a route to reduce work but conflicts with dominant group-wise quantization layouts used to fit models into GPU memory. The goal is to reconcile these to get faster, memory-efficient inference.", "method": "Proposes a zigzag-patterned weight layout aligned with activation sparsity to improve memory locality under quantization, a specialized GPU GEMV kernel to exploit the layout, and a compact runtime to collect sparse activation indices with minimal overhead.", "result": "Across multiple models and hardware setups, they report up to 1.55x faster decoding throughput while maintaining accuracy comparable to dense quantized inference.", "conclusion": "Structured/dynamic sparsity can be made compatible with group-wise low-bit quantization on commodity GPUs via layout, kernel, and runtime co-design, enabling practical speedups without accuracy loss."}}
