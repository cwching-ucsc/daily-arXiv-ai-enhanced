<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 1]
- [cs.DC](#cs.DC) [Total: 2]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [NetBurst: Event-Centric Forecasting of Bursty, Intermittent Time Series](https://arxiv.org/abs/2510.22397)
*Satyandra Guthula,Jaber Daneshamooz,Charles Fleming,Ashish Kundu,Walter Willinger,Arpit Gupta*

Main category: cs.NI

TL;DR: NetBurst reframes forecasting of highly bursty, intermittent network telemetry as an event prediction task (when bursts occur and how large they are) using quantile-based codebooks and dual autoregressors, yielding large MASE improvements and better-structured embeddings versus strong baselines.


<details>
  <summary>Details</summary>
Motivation: Standard forecasting benchmarks favor smooth/seasonal series; production network telemetry is bursty, intermittent, and heavy-tailed (Mandelbrot regimes). Modern AI models underperform there, motivating an event-centric formulation tailored to bursts.

Method: Introduce NetBurst: an event-centric forecasting framework that predicts burst timing and magnitude via quantile-based codebooks and two autoregressive components. Outputs are quantile-coded burst sizes and event times rather than dense-value forecasts, enabling preservation of burstiness and compact embeddings.

Result: On large-scale production network telemetry, NetBurst reduces MASE by 13–605× on service-level series relative to strong baselines (e.g., Chronos), preserves burst statistics, and produces embeddings that cluster ~5× more cleanly than Chronos.

Conclusion: Leveraging Mandelbrot-style thinking for heavy-tailed, intermittent time series improves forecasting and representation quality in operational network telemetry; the event-centric quantile/codebook approach appears particularly effective in these regimes.

Abstract: Forecasting on widely used benchmark time series data (e.g., ETT,
Electricity, Taxi, and Exchange Rate, etc.) has favored smooth, seasonal
series, but network telemetry time series -- traffic measurements at service,
IP, or subnet granularity -- are instead highly bursty and intermittent, with
heavy-tailed bursts and highly variable inactive periods. These properties
place the latter in the statistical regimes made famous and popularized more
than 20 years ago by B.~Mandelbrot. Yet forecasting such time series with
modern-day AI architectures remains underexplored. We introduce NetBurst, an
event-centric framework that reformulates forecasting as predicting when bursts
occur and how large they are, using quantile-based codebooks and dual
autoregressors. Across large-scale sets of production network telemetry time
series and compared to strong baselines, such as Chronos, NetBurst reduces Mean
Average Scaled Error (MASE) by 13--605x on service-level time series while
preserving burstiness and producing embeddings that cluster 5x more cleanly
than Chronos. In effect, our work highlights the benefits that modern AI can
reap from leveraging Mandelbrot's pioneering studies for forecasting in bursty,
intermittent, and heavy-tailed regimes, where its operational value for
high-stakes decision making is of paramount interest.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Rethinking Inference Placement for Deep Learning across Edge and Cloud Platforms: A Multi-Objective Optimization Perspective and Future Directions](https://arxiv.org/abs/2510.22909)
*Zongshun Zhang,Ibrahim Matta*

Main category: cs.DC

TL;DR: Survey of model partitioning/offloading for edge intelligent applications, examining trade-offs among inference latency, data privacy, and monetary cost; covers techniques like model compression, distillation, transmission compression, and internal classifiers.


<details>
  <summary>Details</summary>
Motivation: Edge and mobile devices are resource-constrained yet must support large DL models for VR/AR and chatbots; partitioning/offloading among device, edge, and cloud can leverage complementary resources but introduces latency, bandwidth, and privacy trade-offs.

Method: A literature survey that categorizes and contextualizes state-of-the-art offloading and model adaptation techniques, and analyzes their implications within a multi-objective optimization framework balancing latency, privacy, and cost.

Result: Provides a taxonomy of approaches (offloading strategies, compression/distillation, architecture adaptations), highlights transmission bottlenecks and privacy risks, and synthesizes how methods trade accuracy, delay, and cost.

Conclusion: Concludes that existing methods partially address trade-offs but gaps remain; calls for multi-objective optimization, standardized benchmarks, and research into adaptive, privacy-preserving, and cost-aware partitioning strategies.

Abstract: Edge intelligent applications like VR/AR and language model based chatbots
have become widespread with the rapid expansion of IoT and mobile devices.
However, constrained edge devices often cannot serve the increasingly large and
complex deep learning (DL) models. To mitigate these challenges, researchers
have proposed optimizing and offloading partitions of DL models among user
devices, edge servers, and the cloud. In this setting, users can take advantage
of different services to support their intelligent applications. For example,
edge resources offer low response latency. In contrast, cloud platforms provide
low monetary cost computation resources for computation-intensive workloads.
However, communication between DL model partitions can introduce transmission
bottlenecks and pose risks of data leakage. Recent research aims to balance
accuracy, computation delay, transmission delay, and privacy concerns. They
address these issues with model compression, model distillation, transmission
compression, and model architecture adaptations, including internal
classifiers. This survey contextualizes the state-of-the-art model offloading
methods and model adaptation techniques by studying their implication to a
multi-objective optimization comprising inference latency, data privacy, and
resource monetary cost.

</details>


### [3] [Bayes-Split-Edge: Bayesian Optimization for Constrained Collaborative Inference in Wireless Edge Systems](https://arxiv.org/abs/2510.23503)
*Fatemeh Zahra Safaeipour,Jacob Chakareski,Morteza Hashemi*

Main category: cs.DC

TL;DR: Presents Bayes-Split-Edge, a constrained Bayesian optimization framework that jointly selects neural-network split point and transmission power for wireless collaborative inference under energy and deadline constraints; shows sample-efficient, constraint-aware optimization with strong empirical gains (≤20 evaluations, up to 2.4× lower cost vs standard BO) on VGG19/ResNet101 and real wireless traces.


<details>
  <summary>Details</summary>
Motivation: Edge devices (AR/VR, etc.) have tight compute/energy budgets yet must meet inference latency deadlines. Splitting inference between device and edge server over wireless links can trade computation and communication, but finding the optimal split and transmit power under energy and delay constraints is nontrivial and costly to evaluate directly.

Method: Formulates an inference-utility optimization with energy and delay constraints. Proposes Bayes-Split-Edge: a constrained Bayesian optimization that jointly optimizes transmit power and network split point. Introduces a hybrid acquisition function to balance task utility, sample efficiency, and penalties for constraint violations.

Result: Evaluated on ImageNet-Mini with VGG19 and Tiny-ImageNet with ResNet101 using real mobile wireless channel traces. Reported up to 2.4× reduction in evaluation cost vs standard BO, near-linear convergence, outperformance of CMA-ES, DIRECT, exhaustive search, and PPO, and matching exhaustive search under tight constraints, while requiring at most 20 function evaluations.

Conclusion: Bayes-Split-Edge is a sample-efficient, constraint-aware approach for wireless split inference that can find near-optimal split-and-power settings with few evaluations and robust performance under realistic channel conditions.

Abstract: Mobile edge devices (e.g., AR/VR headsets) typically need to complete timely
inference tasks while operating with limited on-board computing and energy
resources. In this paper, we investigate the problem of collaborative inference
in wireless edge networks, where energy-constrained edge devices aim to
complete inference tasks within given deadlines. These tasks are carried out
using neural networks, and the edge device seeks to optimize inference
performance under energy and delay constraints. The inference process can be
split between the edge device and an edge server, thereby achieving
collaborative inference over wireless networks. We formulate an inference
utility optimization problem subject to energy and delay constraints, and
propose a novel solution called Bayes-Split-Edge, which leverages Bayesian
optimization for collaborative split inference over wireless edge networks. Our
solution jointly optimizes the transmission power and the neural network split
point. The Bayes-Split-Edge framework incorporates a novel hybrid acquisition
function that balances inference task utility, sample efficiency, and
constraint violation penalties. We evaluate our approach using the VGG19 model
on the ImageNet-Mini dataset, and Resnet101 on Tiny-ImageNet, and real-world
mMobile wireless channel datasets. Numerical results demonstrate that
Bayes-Split-Edge achieves up to 2.4x reduction in evaluation cost compared to
standard Bayesian optimization and achieves near-linear convergence. It also
outperforms several baselines, including CMA-ES, DIRECT, exhaustive search, and
Proximal Policy Optimization (PPO), while matching exhaustive search
performance under tight constraints. These results confirm that the proposed
framework provides a sample-efficient solution requiring maximum 20 function
evaluations and constraint-aware optimization for wireless split inference in
edge computing systems.

</details>
