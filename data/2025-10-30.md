<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 3]
- [cs.NI](#cs.NI) [Total: 1]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [MoEntwine: Unleashing the Potential of Wafer-scale Chips for Large-scale Expert Parallel Inference](https://arxiv.org/abs/2510.25258)
*Xinru Tang,Jingxiang Hou,Dingcheng Jiang,Taiquan Wei,Jiaxin Liu,Jinyi Deng,Huizheng Wang,Qize Yang,Haoran Shang,Chao Li,Yang Hu,Shouyi Yin*

Main category: cs.DC

TL;DR: Paper studies running mixture-of-experts (MoE) LLMs on wafer-scale chips (WSCs). It proposes ER-Mapping to co-map attention and MoE layers to balance mesh-network communication and NI-Balancer to split and interleave expert migration using complementary "cold" links, hiding migration overhead. Evaluations show up to 62% communication reduction, NI-Balancer improves MoE computation by 54% and communication by 22%, and WSCs outperform an NVL72 supernode by ~39% per-device MoE performance.


<details>
  <summary>Details</summary>
Motivation: MoE models need expert parallelism (EP) which requires costly all-to-all communication. Traditional GPU clusters suffer high cross-node overhead, limiting EP. WSCs offer a unified high-performance on-wafer network but with mesh topology and no on-wafer disk, causing imbalanced link pressure and costly expert migration. The paper aims to unlock WSCs for MoE by addressing these constraints.

Method: Proposes Entwined Ring Mapping (ER-Mapping) that co-designs mapping of attention and MoE layers so their communication patterns' hot/cold links complement each other, reducing contention. Proposes Non-invasive Balancer (NI-Balancer) that decomposes expert migration into multiple steps and alternately uses the cold links of attention and MoE layers to hide migration overhead.

Result: ER-Mapping reduces communication up to 62%. NI-Balancer yields 54% and 22% improvements in MoE computation and communication respectively. Overall, WSC platform achieves ~39% higher per-device MoE performance vs a state-of-the-art NVL72 supernode due to better scalability of EP.

Conclusion: By co-designing layer mapping and migration scheduling (ER-Mapping + NI-Balancer), the paper mitigates mesh-network imbalance and migration overhead on WSCs, enabling more efficient and scalable MoE execution compared to current supernode GPU setups.

Abstract: As large language models (LLMs) continue to scale up, mixture-of-experts
(MoE) has become a common technology in SOTA models. MoE models rely on expert
parallelism (EP) to alleviate memory bottleneck, which introduces all-to-all
communication to dispatch and combine tokens across devices. However, in
widely-adopted GPU clusters, high-overhead cross-node communication makes
all-to-all expensive, hindering the adoption of EP. Recently, wafer-scale chips
(WSCs) have emerged as a platform integrating numerous devices on a wafer-sized
interposer. WSCs provide a unified high-performance network connecting all
devices, presenting a promising potential for hosting MoE models. Yet, their
network is restricted to a mesh topology, causing imbalanced communication
pressure and performance loss. Moreover, the lack of on-wafer disk leads to
high-overhead expert migration on the critical path.
  To fully unleash this potential, we first propose Entwined Ring Mapping
(ER-Mapping), which co-designs the mapping of attention and MoE layers to
balance communication pressure and achieve better performance. We find that
under ER-Mapping, the distribution of cold and hot links in the attention and
MoE layers is complementary. Therefore, to hide the migration overhead, we
propose the Non-invasive Balancer (NI-Balancer), which splits a complete expert
migration into multiple steps and alternately utilizes the cold links of both
layers. Evaluation shows ER-Mapping achieves communication reduction up to 62%.
NI-Balancer further delivers 54% and 22% improvements in MoE computation and
communication, respectively. Compared with the SOTA NVL72 supernode, the WSC
platform delivers an average 39% higher per-device MoE performance owing to its
scalability to larger EP.

</details>


### [2] [A Privacy-Preserving Ecosystem for Developing Machine Learning Algorithms Using Patient Data: Insights from the TUM.ai Makeathon](https://arxiv.org/abs/2510.25277)
*Simon Süwer,Mai Khanh Mai,Christoph Klein,Nicola Götzenberger,Denis Dalić,Andreas Maier,Jan Baumbach*

Main category: cs.DC

TL;DR: A multi-stage privacy-preserving workflow uses a simulated clinical knowledge graph and a federated learning framework (FeatureCloud) to design and deploy models trained inside hospital environments, validated in a Makeathon where 50 students developed models without accessing real patient data.


<details>
  <summary>Details</summary>
Motivation: To enable development of predictive medical AI while complying with GDPR and protecting sensitive patient data, especially for small cohorts and rare diseases where data sharing is restricted; high-quality structured data and privacy-preserving training are needed.

Method: (1) Model architecture and pipeline designed using a simulated clinical knowledge graph (cKG) that mirrors real graph structure without real data. (2) Model packaged into the FeatureCloud federated learning framework in a protected single-client setup. (3) Training executed inside hospitals on the real cKG under staff supervision or via an automated hospital-controlled pipeline. (4) Verified evaluation scripts run locally and return only aggregated performance metrics to prevent leakage of individual data. The cKG organizes multi-omics and patient data for realistic hospital deployment.

Result: Validated during TUM.ai Makeathon 2024 (TUMaiM24) with Dr. von Hauner Children's Hospital challenge: 50 students built patient classification/diagnosis models without access to real data; framework provided secure algorithm deployment and feedback via aggregated metrics.

Conclusion: The proposed approach demonstrates a practical pathway for privacy-preserving AI in healthcare by combining simulated knowledge graphs and federated frameworks, enabling model development without exposing sensitive patient-level data.

Abstract: The integration of clinical data offers significant potential for the
development of personalized medicine. However, its use is severely restricted
by the General Data Protection Regulation (GDPR), especially for small cohorts
with rare diseases. High-quality, structured data is essential for the
development of predictive medical AI. In this case study, we propose a novel,
multi-stage approach to secure AI training: (1) The model is designed on a
simulated clinical knowledge graph (cKG). This graph is used exclusively to
represent the structural characteristics of the real cKG without revealing any
sensitive content. (2) The model is then integrated into the FeatureCloud (FC)
federated learning framework, where it is prepared in a single-client
configuration within a protected execution environment. (3) Training then takes
place within the hospital environment on the real cKG, either under the direct
supervision of hospital staff or via a fully automated pipeline controlled by
the hospital. (4) Finally, verified evaluation scripts are executed, which only
return aggregated performance metrics. This enables immediate performance
feedback without sensitive patient data or individual predictions, leaving the
clinic. A fundamental element of this approach involves the incorporation of a
cKG, which serves to organize multi-omics and patient data within the context
of real-world hospital environments. This approach was successfully validated
during the TUM.ai Makeathon 2024 (TUMaiM24) challenge set by the Dr. von Hauner
Children's Hospital (HCH-LMU): 50 students developed models for patient
classification and diagnosis without access to real data. Deploying secure
algorithms via federated frameworks, such as the FC framework, could be a
practical way of achieving privacy-preserving AI in healthcare.

</details>


### [3] [Holon Streaming: Global Aggregations with Windowed CRDTs](https://arxiv.org/abs/2510.25757)
*Jonas Spenger,Kolya Krafeld,Ruben van Gemeren,Philipp Haller,Paris Carbone*

Main category: cs.DC

TL;DR: Holon Streaming introduces Windowed CRDTs — a deterministic, decentralized approach for scalable, exactly-once global aggregations in stream processing — yielding substantially lower latency and higher throughput than an existing system, especially during failures.


<details>
  <summary>Details</summary>
Motivation: Global aggregations are hard to scale in exactly-once stream processors: single-task aggregation or static aggregation trees create bottlenecks, increase end-to-end latency (bounded by slowest tree path), and centralized coordination worsens latency spikes under failures and reconfiguration.

Method: Design Holon Streaming with a deterministic programming model based on Windowed CRDTs (conflict-free replicated data types scoped to windows) to represent shared replicated state. Use determinism and CRDT convergence guarantees to enable decentralized coordination and efficient failure-recovery algorithms, avoiding centralized bottlenecks.

Result: Compared to an existing stream processing system on global aggregation workloads, Holon Streaming achieves ~5x lower latency and ~2x higher throughput in normal operation, and an ~11x latency reduction during failure scenarios.

Conclusion: Deterministic, CRDT-based abstractions (Windowed CRDTs) plus decentralized coordination materially improve scalability, latency, and failure behavior for global aggregations in exactly-once stream processing.

Abstract: Scaling global aggregations is a challenge for exactly-once stream processing
systems. Current systems implement these either by computing the aggregation in
a single task instance, or by static aggregation trees, which limits
scalability and may become a bottleneck. Moreover, the end-to-end latency is
determined by the slowest path in the tree, and failures and reconfiguration
cause large latency spikes due to the centralized coordination. Towards these
issues, we present Holon Streaming, an exactly-once stream processing system
for global aggregations. Its deterministic programming model uses windowed
conflict-free replicated data types (Windowed CRDTs), a novel abstraction for
shared replicated state. Windowed CRDTs make computing global aggregations
scalable. Furthermore, their guarantees such as determinism and convergence
enable the design of efficient failure recovery algorithms by decentralized
coordination. Our evaluation shows a 5x lower latency and 2x higher throughput
than an existing stream processing system on global aggregation workloads, with
an 11x latency reduction under failure scenarios. The paper demonstrates the
effectiveness of decentralized coordination with determinism, and the utility
of Windowed CRDTs for global aggregations.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [4] [ML-Based Preamble Collision Detection in the Random Access Procedure of Cellular IoT Networks](https://arxiv.org/abs/2510.25145)
*Giancarlo Maldonado Cardenas,Diana C. Gonzalez,Judy C. Guevara,Carlos A. Astudillo,Nelson L. S. da Fonseca*

Main category: cs.NI

TL;DR: ML-based early collision detection for RA in mMTC: a simulated dataset was used to train nine classifiers; a neural network achieved >98% balanced accuracy in-distribution and ~95% out-of-distribution. Post-training full integer quantization cut inference time from 2500 ms to 0.3 ms with negligible accuracy loss, enabling low-latency deployment on base-station hardware.


<details>
  <summary>Details</summary>
Motivation: Preamble collisions in RACH limit scalability of CIoT/mMTC. Early detection of collisions allows faster resolution, reducing access delays and improving network efficiency in dense IoT deployments.

Method: Generate labeled RA-message dataset under realistic channel models in MATLAB across four scenarios (varying Doppler, multipath, cell radius). Train and evaluate nine classic classifiers (tree ensembles, SVMs, NNs) in both in-distribution and out-of-distribution splits. Apply post-training full integer quantization to the best model for deployment and measure latency and accuracy.

Result: A neural network outperformed other models: >98% balanced accuracy in-distribution, ~95% in out-of-distribution tests. Quantization reduced inference latency from 2500 ms to 0.3 ms while keeping accuracy nearly unchanged.

Conclusion: The approach yields high early-collision detection accuracy and very low inference latency after quantization, making it practically suitable for scalable, real-time CIoT deployments on commodity base-station hardware.

Abstract: Preamble collision in the random access channel (RACH) is a major bottleneck
in massive machine-type communication (mMTC) scenarios, typical of cellular IoT
(CIoT) deployments. This work proposes a machine learning-based mechanism for
early collision detection during the random access (RA) procedure. A labeled
dataset was generated using the RA procedure messages exchanged between the
users and the base station under realistic channel conditions, simulated in
MATLAB. We evaluate nine classic classifiers -- including tree ensembles,
support vector machines, and neural networks -- across four communication
scenarios, varying both channel characteristics (e.g., Doppler spread,
multipath) and the cell coverage radius, to emulate realistic propagation,
mobility, and spatial conditions. The neural network outperformed all other
models, achieving over 98\% balanced accuracy in the in-distribution evaluation
(train and test drawn from the same dataset) and sustaining 95\% under
out-of-distribution evaluation (train/test from different datasets). To enable
deployment on typical base station hardware, we apply post-training
quantization. Full integer quantization reduced inference time from 2500 ms to
as low as 0.3 ms with negligible accuracy loss. The proposed solution combines
high detection accuracy with low-latency inference, making it suitable for
scalable, real-time CIoT applications found in real networks.

</details>
