{"id": "2510.18893", "categories": ["cs.DC", "cs.AI", "cs.SE", "I.2.11; D.2.11"], "pdf": "https://arxiv.org/pdf/2510.18893", "abs": "https://arxiv.org/abs/2510.18893", "authors": ["Sergey Pugachev"], "title": "CodeCRDT: Observation-Driven Coordination for Multi-Agent LLM Code Generation", "comment": "11 pages, 3 figures", "summary": "Multi-agent LLM systems fail to realize parallel speedups due to costly\ncoordination. We present CodeCRDT, an observation-driven coordination pattern\nwhere agents coordinate by monitoring a shared state with observable updates\nand deterministic convergence, rather than explicit message passing. Using\nConflict-Free Replicated Data Types (CRDTs), CodeCRDT enables lock-free,\nconflict-free concurrent code generation with strong eventual consistency.\nEvaluation across 600 trials (6 tasks, 50 runs per mode) shows both benefits\nand trade-offs: up to 21.1% speedup on some tasks, up to 39.4% slowdown on\nothers, and 100% convergence with zero merge failures. The study formalizes\nobservation-driven coordination for stochastic LLM agents, revealing semantic\nconflict rates (5-10%) and quality-performance tradeoffs, and provides\nempirical characterization of when parallel coordination succeeds versus fails\nbased on task structure.", "AI": {"tldr": "CodeCRDT replaces explicit message-passing between stochastic LLM agents with observation-driven coordination using CRDTs to enable lock-free, conflict-free concurrent code generation with eventual consistency. Evaluations (600 trials) show mixed performance: some tasks gain up to 21.1% speedup, others suffer up to 39.4% slowdown, but convergence is always achieved with zero merge failures.", "motivation": "Multi-agent LLM systems aim to parallelize work but are bottlenecked by coordination costs and conflicts; the authors seek a coordination pattern that avoids expensive messaging and merge failures while supporting concurrent code generation.", "method": "Introduce 'observation-driven coordination' where agents monitor a shared CRDT-based state with observable updates and deterministic convergence. Design CodeCRDT to allow lock-free updates with strong eventual consistency. Formalize the model for stochastic LLM agents and evaluate empirically across 6 tasks with 50 runs per mode (600 trials) comparing parallel coordination modes and measuring speed, quality, conflict rates, and convergence.", "result": "Empirical results show 100% convergence with zero merge failures. Performance outcomes vary by task: up to 21.1% speedup on favorable tasks, but up to 39.4% slowdown on others. Semantic conflict rates are reported at 5\u201310%. The study characterizes when parallel coordination helps vs. hurts based on task structure and highlights quality-performance tradeoffs.", "conclusion": "CodeCRDT is a viable coordination pattern that guarantees deterministic convergence and removes merge failures, but its performance benefits are task-dependent. The formalization and empirical characterization clarify when observation-driven parallel coordination is effective and when it introduces overhead or semantic conflicts."}}
{"id": "2510.18897", "categories": ["cs.DC", "cs.AI", "cs.DB", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18897", "abs": "https://arxiv.org/abs/2510.18897", "authors": ["Jacopo Tagliabue"], "title": "AI for Distributed Systems Design: Scalable Cloud Optimization Through Repeated LLMs Sampling And Simulators", "comment": "Pre-print IAAA workshop submission", "summary": "We explore AI-driven distributed-systems policy design by combining\nstochastic code generation from large language models (LLMs) with deterministic\nverification in a domain-specific simulator. Using a Function-as-a-Service\nruntime (Bauplan) and its open-source simulator (Eudoxia) as a case study, we\nframe scheduler design as an iterative generate-and-verify loop: an LLM\nproposes a Python policy, the simulator evaluates it on standardized traces,\nand structured feedback steers subsequent generations. This setup preserves\ninterpretability while enabling targeted search over a large design space. We\ndetail the system architecture and report preliminary results on throughput\nimprovements across multiple models. Beyond early gains, we discuss the limits\nof the current setup and outline next steps; in particular, we conjecture that\nAI will be crucial for scaling this methodology by helping to bootstrap new\nsimulators.", "AI": {"tldr": "LLMs generate candidate Python scheduler policies which are evaluated deterministically in a domain-specific simulator; an iterative generate-and-verify loop with structured feedback improves throughput on a FaaS runtime (Bauplan/Eudoxia) while preserving interpretability.", "motivation": "Searching the large space of distributed-systems scheduling policies is hard; using LLMs to propose human-readable policies plus deterministic simulator verification aims to accelerate discovery while keeping designs interpretable and testable.", "method": "Stochastic code generation from LLMs (Python policies) -> evaluate each candidate in the open-source Eudoxia simulator on standardized traces -> produce structured feedback from simulator to steer further LLM generations. System architecture ties Bauplan runtime, the simulator, and the feedback loop together.", "result": "Preliminary experiments show throughput improvements across multiple models. The approach enables targeted search and preserves interpretability, but gains are early-stage and constrained by current setup.", "conclusion": "Combining LLM-driven policy proposals with deterministic simulation is promising. Future work should address limits: scaling, robustness, simulator bootstrapping (where AI may help), security/sandboxing of generated code, and broader evaluation."}}
{"id": "2510.19225", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19225", "abs": "https://arxiv.org/abs/2510.19225", "authors": ["Yongji Wu", "Xueshen Liu", "Haizhong Zheng", "Juncheng Gu", "Beidi Chen", "Z. Morley Mao", "Arvind Krishnamurthy", "Ion Stoica"], "title": "RLBoost: Harvesting Preemptible Resources for Cost-Efficient Reinforcement Learning on LLMs", "comment": null, "summary": "Reinforcement learning (RL) has become essential for unlocking advanced\nreasoning capabilities in large language models (LLMs). RL workflows involve\ninterleaving rollout and training stages with fundamentally different resource\nrequirements. Rollout typically dominates overall execution time, yet scales\nefficiently through multiple independent instances. In contrast, training\nrequires tightly-coupled GPUs with full-mesh communication. Existing RL\nframeworks fall into two categories: co-located and disaggregated\narchitectures. Co-located ones fail to address this resource tension by forcing\nboth stages to share the same GPUs. Disaggregated architectures, without\nmodifications of well-established RL algorithms, suffer from resource\nunder-utilization. Meanwhile, preemptible GPU resources, i.e., spot instances\non public clouds and spare capacity in production clusters, present significant\ncost-saving opportunities for accelerating RL workflows, if efficiently\nharvested for rollout.\n  In this paper, we present RLBoost, a systematic solution for cost-efficient\nRL training that harvests preemptible GPU resources. Our key insight is that\nrollout's stateless and embarrassingly parallel nature aligns perfectly with\npreemptible and often fragmented resources. To efficiently utilize these\nresources despite frequent and unpredictable availability changes, RLBoost\nadopts a hybrid architecture with three key techniques: (1) adaptive rollout\noffload to dynamically adjust workloads on the reserved (on-demand) cluster,\n(2) pull-based weight transfer that quickly provisions newly available\ninstances, and (3) token-level response collection and migration for efficient\npreemption handling and continuous load balancing. Extensive experiments show\nRLBoost increases training throughput by 1.51x-1.97x while improving cost\nefficiency by 28%-49% compared to using only on-demand GPU resources.", "AI": {"tldr": "RLBoost is a hybrid RL training system that harvests preemptible (spot) GPUs for rollout workloads while keeping training on reserved GPUs. It uses adaptive offload, pull-based weight transfer, and token-level response collection to tolerate frequent preemptions, achieving 1.51\u20131.97\u00d7 higher throughput and 28\u201349% cost savings.", "motivation": "Training RL for LLMs mixes two mismatched stages: stateless, parallel rollouts (cheaply run on many transient GPUs) and tightly-coupled training (needs full-mesh, stable GPUs). Existing co-located and na\u00efvely disaggregated approaches either waste resources or underutilize hardware. Preemptible GPUs are abundant but volatile; harnessing them for rollouts could cut costs and speed up training if handled efficiently.", "method": "RLBoost uses a hybrid architecture: (1) adaptive rollout offload to shift workload between on-demand and preemptible clusters based on availability; (2) pull-based weight transfer so newly provisioned instances quickly fetch current model weights; (3) token-level response collection and migration to allow partial responses to be captured and migrated when preemptions occur, enabling continuous load balancing and minimal wasted work.", "result": "On the reported experiments, RLBoost improved end-to-end training throughput by 1.51\u00d7\u20131.97\u00d7 and reduced cost per unit of training by 28%\u201349% compared to using only on-demand GPUs.", "conclusion": "By aligning rollout characteristics with preemptible resources and adding mechanisms to handle preemption and weight synchronization, RLBoost enables cost-effective, higher-throughput RL training for LLMs without changing core RL algorithms."}}
{"id": "2510.19301", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.19301", "abs": "https://arxiv.org/abs/2510.19301", "authors": ["Ziheng Deng", "Xue Liu", "Jiantong Jiang", "Yankai Li", "Qingxu Deng", "Xiaochun Yang"], "title": "FLASH Viterbi: Fast and Adaptive Viterbi Decoding for Modern Data Systems", "comment": "Accepted for ICDE 2026", "summary": "The Viterbi algorithm is a key operator for structured sequence inference in\nmodern data systems, with applications in trajectory analysis, online\nrecommendation, and speech recognition. As these workloads increasingly migrate\nto resource-constrained edge platforms, standard Viterbi decoding remains\nmemory-intensive and computationally inflexible. Existing methods typically\ntrade decoding time for space efficiency, but often incur significant runtime\noverhead and lack adaptability to various system constraints. This paper\npresents FLASH Viterbi, a Fast, Lightweight, Adaptive, and Hardware-Friendly\nViterbi decoding operator that enhances adaptability and resource efficiency.\nFLASH Viterbi combines a non-recursive divide-and-conquer strategy with pruning\nand parallelization techniques to enhance both time and memory efficiency,\nmaking it well-suited for resource-constrained data systems.To further decouple\nspace complexity from the hidden state space size, we present FLASH-BS Viterbi,\na dynamic beam search variant built on a memory-efficient data structure. Both\nproposed algorithms exhibit strong adaptivity to diverse deployment scenarios\nby dynamically tuning internal parameters.To ensure practical deployment on\nedge devices, we also develop FPGA-based hardware accelerators for both\nalgorithms, demonstrating high throughput and low resource usage. Extensive\nexperiments show that our algorithms consistently outperform existing baselines\nin both decoding time and memory efficiency, while preserving adaptability and\nhardware-friendly characteristics essential for modern data systems. All codes\nare publicly available at https://github.com/Dzh-16/FLASH-Viterbi.", "AI": {"tldr": "This paper introduces FLASH Viterbi, a memory- and compute-efficient Viterbi decoding family tailored for edge systems. It uses a non-recursive divide-and-conquer algorithm with pruning and parallelization, plus a memory-efficient beam-search variant (FLASH-BS), and FPGA accelerators. The methods claim superior decoding time and lower memory use while remaining adaptive to system constraints; code is released.", "motivation": "Standard Viterbi decoding is memory-intensive and computationally inflexible for edge/resource-constrained platforms. Existing space-saving approaches trade runtime for memory and lack adaptability to diverse deployment constraints. The paper aims to provide a fast, lightweight, adaptive, and hardware-friendly decoder.", "method": "FLASH Viterbi: non-recursive divide-and-conquer strategy combined with pruning and parallelization to reduce memory footprint and improve throughput. FLASH-BS: a dynamic beam-search variant using a compact data structure to decouple memory complexity from hidden-state size. Both algorithms expose tunable internal parameters for adaptivity. FPGA-based hardware accelerators are implemented for practical deployment.", "result": "Extensive experiments (details not in abstract) reportedly show consistent improvements over baselines in decoding time and memory efficiency while preserving adaptability and hardware friendliness. FPGA implementations demonstrate high throughput and low resource usage. Code is publicly available.", "conclusion": "FLASH Viterbi and FLASH-BS provide practical, adaptive Viterbi decoders for edge deployment, offering favorable trade-offs among time, space, and hardware resource use; the work supports reproducibility via released code."}}
{"id": "2510.19689", "categories": ["cs.DC", "cs.AI", "cs.LG", "C.2.4; H.3.4; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.19689", "abs": "https://arxiv.org/abs/2510.19689", "authors": ["Guilin Zhang", "Wulan Guo", "Ziqi Tan", "Srinivas Vippagunta", "Suchitra Raman", "Shreeshankar Chatterjee", "Ju Lin", "Shang Liu", "Mary Schladenhauffen", "Jeffrey Luo", "Hailong Jiang"], "title": "Serverless GPU Architecture for Enterprise HR Analytics: A Production-Scale BDaaS Implementation", "comment": "10 pages, 7 figures, 4 tables. Accepted to IEEE BigData 2025", "summary": "Industrial and government organizations increasingly depend on data-driven\nanalytics for workforce, finance, and regulated decision processes, where\ntimeliness, cost efficiency, and compliance are critical. Distributed\nframeworks such as Spark and Flink remain effective for massive-scale batch or\nstreaming analytics but introduce coordination complexity and auditing\noverheads that misalign with moderate-scale, latency-sensitive inference.\nMeanwhile, cloud providers now offer serverless GPUs, and models such as TabNet\nenable interpretable tabular ML, motivating new deployment blueprints for\nregulated environments. In this paper, we present a production-oriented Big\nData as a Service (BDaaS) blueprint that integrates a single-node serverless\nGPU runtime with TabNet. The design leverages GPU acceleration for throughput,\nserverless elasticity for cost reduction, and feature-mask interpretability for\nIL4/FIPS compliance. We conduct benchmarks on the HR, Adult, and BLS datasets,\ncomparing our approach against Spark and CPU baselines. Our results show that\nGPU pipelines achieve up to 4.5x higher throughput, 98x lower latency, and 90%\nlower cost per 1K inferences compared to Spark baselines, while compliance\nmechanisms add only ~5.7 ms latency with p99 < 22 ms. Interpretability remains\nstable under peak load, ensuring reliable auditability. Taken together, these\nfindings provide a compliance-aware benchmark, a reproducible Helm-packaged\nblueprint, and a decision framework that demonstrate the practicality of\nsecure, interpretable, and cost-efficient serverless GPU analytics for\nregulated enterprise and government settings.", "AI": {"tldr": "Paper proposes a production-oriented BDaaS blueprint combining a single-node serverless GPU runtime with TabNet to deliver low-latency, cost-efficient, and interpretable tabular ML for regulated settings. Benchmarks on HR, Adult, and BLS datasets show large improvements over Spark and CPU baselines (up to 4.5x throughput, 98x lower latency, 90% lower cost per 1K inferences), and compliance/interpretability additions add minimal overhead (~5.7 ms, p99 < 22 ms). The work ships a reproducible Helm package and a decision framework for deployment.", "motivation": "Organizations with regulatory requirements need analytics that balance timeliness, cost, and auditability. Traditional distributed frameworks (e.g., Spark/Flink) introduce coordination and auditing overheads that are poorly matched to moderate-scale, latency-sensitive inference; meanwhile serverless GPUs and interpretable tabular models (TabNet) create an opportunity for a new deployment blueprint tailored to regulated environments.", "method": "Design a BDaaS blueprint that integrates a single-node serverless GPU runtime with TabNet. Leverage GPU acceleration for throughput, serverless elasticity for cost savings, and TabNet's feature-mask interpretability to meet IL4/FIPS-style compliance requirements. Implement a Helm-packaged, reproducible runtime and run benchmarks comparing GPU pipeline against Spark and CPU baselines on HR, Adult, and BLS tabular datasets. Measure throughput, latency (including p99), cost per 1K inferences, and interpretability stability under load.", "result": "GPU pipelines outperform Spark/CPU baselines: up to 4.5x higher throughput, 98x lower latency, and ~90% lower cost per 1K inferences. Compliance mechanisms introduce only ~5.7 ms extra latency with p99 < 22 ms. Interpretability (feature masks) remains stable even at peak load, supporting auditability claims.", "conclusion": "The paper demonstrates a practical, compliance-aware serverless GPU approach for secure, interpretable, and cost-efficient tabular analytics in enterprise/government settings, and provides a benchmark, Helm-packaged blueprint, and decision framework to aid adoption."}}
