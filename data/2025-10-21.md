<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 3]
- [cs.DC](#cs.DC) [Total: 5]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Agentic AI for Ultra-Modern Networks: Multi-Agent Framework for RAN Autonomy and Assurance](https://arxiv.org/abs/2510.16144)
*Sukhdeep Singh,Avinash Bhat,Shweta M,Subhash K Singh,Moonki Hong,Madhan Raj K,Kandeepan Sithamparanathan,Sunder A. Khowaja,Kapal Dev*

Main category: cs.NI

TL;DR: Proposes a multi-agent architecture for O-RAN autonomy that distributes data, learning, policy, verification, and deployment across cooperating agents. In a traffic-steering case with surge/drift, the agentic system prevents harmful policies and preserves global KPIs, unlike a naive predictor that boosts local metrics but destabilizes neighbors.


<details>
  <summary>Details</summary>
Motivation: Centralized RIC-centric control loops concentrate intelligence and create risks: policy conflicts, model/data drift, and unsafe actions under unforeseen conditions. There is a need for autonomy with resilience, explainability, and system-wide safety in B5G/6G RANs.

Method: Design a distributed, specialized multi-agent framework covering data collection, model training, prediction, policy generation, verification, deployment, and assurance. Implement a traffic-steering use case subjected to demand surges and data/model drift. Compare naive predictor-driven deployment versus the proposed agentic, safety-verifying system across multiple KPIs (RRC users, IP throughput, PRB utilization, SINR).

Result: The naive predictor improves local KPIs but induces negative externalities, destabilizing neighboring cells. The agent-based system detects and blocks unsafe policies, preserving overall network health across the four KPIs, even under surge and drift.

Conclusion: Multi-agent architectures can provide trustworthy, safe, and resilient AI-driven autonomy for next-generation RANs, outperforming tightly coupled, centralized RIC-based approaches by preventing unsafe actions and maintaining system-wide performance.

Abstract: The increasing complexity of Beyond 5G and 6G networks necessitates new
paradigms for autonomy and assur- ance. Traditional O-RAN control loops rely
heavily on RIC- based orchestration, which centralizes intelligence and exposes
the system to risks such as policy conflicts, data drift, and unsafe actions
under unforeseen conditions. In this work, we argue that the future of
autonomous networks lies in a multi-agentic architecture, where specialized
agents collaborate to perform data collection, model training, prediction,
policy generation, verification, deployment, and assurance. By replacing
tightly- coupled centralized RIC-based workflows with distributed agents, the
framework achieves autonomy, resilience, explainability, and system-wide
safety. To substantiate this vision, we design and evaluate a traffic steering
use case under surge and drift conditions. Results across four KPIs: RRC
connected users, IP throughput, PRB utilization, and SINR, demonstrate that a
naive predictor-driven deployment improves local KPIs but destabilizes
neighbors, whereas the agentic system blocks unsafe policies, preserving global
network health. This study highlights multi- agent architectures as a credible
foundation for trustworthy AI- driven autonomy in next-generation RANs.

</details>


### [2] [Mamba4Net: Distilled Hybrid Mamba Large Language Models For Networking](https://arxiv.org/abs/2510.17147)
*Linhan Xia,Mingzhan Yang,Jingjing Wang,Ziwei Yan,Yakun Ren,Guo Yu,Kai Lei*

Main category: cs.NI

TL;DR: Mamba4Net distills networking-specific knowledge from transformer LLMs into a linear-time Mamba student, delivering superior task performance to non-LLM baselines while achieving 3.96× higher throughput and a 5.48% storage footprint versus prior LLM-based approaches.


<details>
  <summary>Details</summary>
Motivation: Transformer LLMs help with networking problems but suffer from quadratic time and large memory footprints, limiting deployment in resource-constrained environments. The goal is to retain LLM-derived benefits while drastically reducing compute and storage costs.

Method: A cross-architecture knowledge distillation framework: teacher models are transformer-based LLMs; student models use the Mamba architecture (state-space models) with linear time complexity. The framework transfers networking-domain knowledge and is evaluated on viewport prediction, adaptive bitrate streaming, and cluster job scheduling.

Result: Across three tasks, Mamba4Net outperforms methods that do not use LLMs and, compared to directly applying transformer LLMs, yields major efficiency gains—3.96× higher throughput and only 5.48% of the storage requirement—while improving resource utilization.

Conclusion: Mamba4Net enables cost-effective, efficient application of LLM-derived knowledge in networking by leveraging linear-time Mamba students distilled from transformer teachers; open-source code supports reproducibility and further research.

Abstract: Transformer-based large language models (LLMs) are increasingly being adopted
in networking research to address domain-specific challenges. However, their
quadratic time complexity and substantial model sizes often result in
significant computational overhead and memory constraints, particularly in
resource-constrained environments. Drawing inspiration from the efficiency and
performance of the Deepseek-R1 model within the knowledge distillation
paradigm, this paper introduces Mamba4Net, a novel cross-architecture
distillation framework. Mamba4Net transfers networking-specific knowledge from
transformer-based LLMs to student models built on the Mamba architecture, which
features linear time complexity. This design substantially enhances
computational efficiency compared to the quadratic complexity of
transformer-based models, while the reduced model size further minimizes
computational demands, improving overall performance and resource utilization.
To evaluate its effectiveness, Mamba4Net was tested across three diverse
networking tasks: viewport prediction, adaptive bitrate streaming, and cluster
job scheduling. Compared to existing methods that do not leverage LLMs,
Mamba4Net demonstrates superior task performance. Furthermore, relative to
direct applications of transformer-based LLMs, it achieves significant
efficiency gains, including a throughput 3.96 times higher and a storage
footprint of only 5.48% of that required by previous LLM-based approaches.
These results highlight Mamba4Net's potential to enable the cost-effective
application of LLM-derived knowledge in networking contexts. The source code is
openly available to support further research and development.

</details>


### [3] [Is It Worth to Use Feedback Channel in 5G V2X Platoon Scenarios?](https://arxiv.org/abs/2510.17410)
*Dmitry Bankov,Artem Krasilov,Artem Otmakhov,Pavel Savlukovich,Evgeny Khorov*

Main category: cs.NI

TL;DR: In 5G NR V2X, enabling a feedback (ACK/NACK) channel for groupcast/unicast can either double or nearly halve overall capacity in mixed platoon (groupcast) and surrounding broadcast traffic, depending on traffic mix, QoS targets, and platoon size; careful, adaptive feedback configuration is essential.


<details>
  <summary>Details</summary>
Motivation: New V2X use cases (platooning, extended sensors, remote driving) require reliable groupcast/unicast beyond 4G’s broadcast. 5G adds a feedback channel to improve reliability and guide transmission parameter selection, but it consumes radio resources. The paper seeks to quantify this trade-off and when feedback helps or hurts system capacity.

Method: Extensive NS-3 simulations of a mixed scenario: a platoon producing groupcast traffic and nearby vehicles producing legacy broadcast traffic. The study varies platoon size, traffic intensities, and QoS requirements, comparing system capacity with and without the feedback channel and analyzing the impact of feedback resource allocation.

Result: Depending on scenario parameters, feedback can substantially increase capacity (up to 2×) or almost halve it. The outcomes hinge on the balance between reliability gains (fewer retransmissions/better parameter tuning) and resource overhead introduced by feedback, plus interactions between groupcast and broadcast loads and QoS constraints.

Conclusion: A feedback channel is not universally beneficial in NR V2X. Its parameters should be adaptively configured based on platoon size, traffic intensities, and QoS targets to maximize capacity; static allocation risks significant underperformance.

Abstract: 5G Vehicle-to-Everything (V2X) is a new technology developed by 3GPP to
support inter-vehicle communication. In contrast to 4G V2X which allows only
broadcast communication, 5G V2X enables groupcast and unicast communication.
Such types of communication are needed for new V2X scenarios: platooning,
extended sensors, remote driving, etc. To improve the data transmission
reliability and assist in the selection of the transmission parameters in these
scenarios, 5G V2X introduces a feedback channel that allows receivers to send
acknowledgments in response to data packets. However, some part of the overall
resource shall be allocated for the feedback channel, which reduces the amount
of channel resources available for data transmission. In this paper, we
consider a scenario with a platoon, which generates groupcast traffic, and
surrounding vehicles, which generate legacy broadcast traffic. Using extensive
simulations in NS-3, we analyze how the usage of the feedback channel
influences the overall system capacity. Our results show that depending on the
platoon size, groupcast, and broadcast traffic intensities, and their quality
of service requirements, the usage of the feedback channel can in some cases
significantly increase the system capacity (up to 2x), while in other cases it
almost halves the system capacity. We explain the reasons for such effects and
discuss how to adaptively select the feedback channel parameters.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [MeCeFO: Enhancing LLM Training Robustness via Fault-Tolerant Optimization](https://arxiv.org/abs/2510.16415)
*Rizhen Hu,Yutong He,Ran Yan,Mou Sun,Binghang Yuan,Kun Yuan*

Main category: cs.DC

TL;DR: MeCeFO is a fault-tolerant distributed training algorithm for LLMs that hands a failed node’s work to a neighbor and uses three lightweight approximations (skip-connection for MHA in backprop, FFN activation recomputation, and low-rank gradient approximation) to keep the extra compute/memory small, while matching standard convergence and improving resilience with minimal throughput loss.


<details>
  <summary>Details</summary>
Motivation: Large-scale LLM training increasingly encounters hardware failures, but existing fault-tolerant methods impose substantial compute/memory overhead or require extra resources. There is a need for a method that preserves convergence and model quality while minimizing added cost during failure recovery.

Method: Upon node failure, the failed node’s training shard is reassigned to a neighboring node. To control the doubled workload, MeCeFO applies: (i) a skip-connection scheme that drops the MHA module during backpropagation to reduce memory/compute; (ii) activation recomputation in feedforward networks to lower memory; and (iii) low-rank approximation of FFN gradients to cut compute/memory for gradient estimation. The algorithm is analyzed to retain the conventional O(1/sqrt(nT)) convergence rate (n: data-parallel size; T: iterations).

Result: Theory: convergence rate matches standard distributed training. Empirics: robust under high failure rates with only a 4.18% throughput drop and 5.0×–6.7× higher resilience than prior SOTA baselines. Code is publicly available.

Conclusion: MeCeFO offers practical, compute- and memory-efficient fault tolerance for distributed LLM training. It maintains standard convergence guarantees and delivers strong robustness with minimal performance degradation, making it a promising approach for large-scale deployments where failures are common.

Abstract: As distributed optimization scales to meet the demands of Large Language
Model (LLM) training, hardware failures become increasingly non-negligible.
Existing fault-tolerant training methods often introduce significant
computational or memory overhead, demanding additional resources. To address
this challenge, we propose Memory- and Computation-efficient Fault-tolerant
Optimization (MeCeFO), a novel algorithm that ensures robust training with
minimal overhead. When a computing node fails, MeCeFO seamlessly transfers its
training task to a neighboring node while employing memory- and
computation-efficient algorithmic optimizations to minimize the extra workload
imposed on the neighboring node handling both tasks. MeCeFO leverages three key
algorithmic designs: (i) Skip-connection, which drops the multi-head attention
(MHA) module during backpropagation for memory- and computation-efficient
approximation; (ii) Recomputation, which reduces activation memory in
feedforward networks (FFNs); and (iii) Low-rank gradient approximation,
enabling efficient estimation of FFN weight matrix gradients. Theoretically,
MeCeFO matches the convergence rate of conventional distributed training, with
a rate of $\mathcal{O}(1/\sqrt{nT})$, where n is the data parallelism size and
T is the number of iterations. Empirically, MeCeFO maintains robust performance
under high failure rates, incurring only a 4.18% drop in throughput,
demonstrating 5.0$\times$ to 6.7$\times$ greater resilience than previous SOTA
approaches. Codes are available at https://github.com/pkumelon/MeCeFO.

</details>


### [5] [FourierCompress: Layer-Aware Spectral Activation Compression for Efficient and Accurate Collaborative LLM Inference](https://arxiv.org/abs/2510.16418)
*Jian Ma,Xinchen Lyu,Jun Jiang,Longhao Zou,Chenshan Ren,Qimei Cui,Xiaofeng Tao*

Main category: cs.DC

TL;DR: FourierCompress compresses first-layer LLM activations in the frequency domain, keeping only low-frequency FFT coefficients to cut bandwidth ~7.6x with <0.3% accuracy loss and >32x faster compression (with HW accel), enabling efficient edge–server collaborative inference.


<details>
  <summary>Details</summary>
Motivation: Collaborative LLM inference on edge devices is bottlenecked by repeatedly transmitting high-dimensional activations during autoregressive decoding, where bandwidth scales with output length. Existing activation compression methods cannot jointly deliver high compression, low error, and low compute cost.

Method: Leverage frequency-domain sparsity of early Transformer activations: prove/verify first-layer activations are smooth with energy concentrated in low frequencies; apply FFT, retain a compact low-frequency block, reconstruct via conjugate symmetry. Use a layer-aware design and exploit DSP/FPGA acceleration. Compare against Top-k, QR, and SVD baselines.

Result: On Llama 3 and Qwen2.5 across 10 commonsense datasets: average 7.6x activation-size reduction with <0.3% average accuracy loss; outperforms Top-k/QR/SVD; compression time >32x faster than Top-k when using hardware acceleration.

Conclusion: A hardware-friendly, near-lossless activation compression approach that bridges communication efficiency, model accuracy, and compute speed for edge-device LLM inference, making collaborative deployment more practical.

Abstract: Collaborative large language model (LLM) inference enables real-time,
privacy-preserving AI services on resource-constrained edge devices by
partitioning computational workloads between client devices and edge servers.
However, this paradigm is severely hindered by communication bottlenecks caused
by the transmission of high-dimensional intermediate activations, exacerbated
by the autoregressive decoding structure of LLMs, where bandwidth consumption
scales linearly with output length. Existing activation compression methods
struggle to simultaneously achieve high compression ratios, low reconstruction
error, and computational efficiency. This paper proposes FourierCompress, a
novel, layer-aware activation compression framework that exploits the
frequency-domain sparsity of LLM activations. We rigorously demonstrate that
activations from the first Transformer layer exhibit strong smoothness and
energy concentration in the low-frequency domain, making them highly amenable
to near-lossless compression via the Fast Fourier Transform (FFT).
FourierCompress transforms activations into the frequency domain, retains only
a compact block of low-frequency coefficients, and reconstructs the signal at
the server using conjugate symmetry, enabling seamless hardware acceleration on
DSPs and FPGAs. Extensive experiments on Llama 3 and Qwen2.5 models across 10
commonsense reasoning datasets demonstrate that FourierCompress preserves
performance remarkably close to the uncompressed baseline, outperforming Top-k,
QR, and SVD. FourierCompress bridges the gap between communication efficiency
(an average 7.6x reduction in activation size), near-lossless inference (less
than 0.3% average accuracy loss), and significantly faster compression
(achieving over 32x reduction in compression time compared to Top-k via
hardware acceleration) for edge-device LLM inference.

</details>


### [6] [Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages](https://arxiv.org/abs/2510.16497)
*Pacome Simon Mbonimpa,Diane Tuyizere,Azizuddin Ahmed Biyabani,Ozan K. Tonguz*

Main category: cs.DC

TL;DR: Proposes a cascaded edge–cloud framework using Whisper (STT) and SpeechT5 (TTS) to deliver low-latency, low-resource speech transcription and synthesis for Kinyarwanda and Swahili, achieving reduced memory usage on edge devices and sub-minute processing for a short utterance under constrained CPU and bandwidth.


<details>
  <summary>Details</summary>
Motivation: Many East African users lack access to powerful speech technologies due to limited infrastructure and under-resourced language support; enabling practical STT/TTS for Kinyarwanda and Swahili on constrained devices would improve accessibility.

Method: Use pre-trained Whisper and SpeechT5 with a cascading edge–cloud architecture that splits inference between edge and cloud to reduce latency and resource usage. Evaluate memory footprint, latency under a 1.7 GHz CPU and 1 MB/s bandwidth, and validate on real survey data.

Result: Memory usage compressed by 9.5% for SpeechT5 and 14% for Whisper with a maximum of 149 MB on the edge; processing a 270-character text in under one minute for both STT and TTS on the specified hardware/network; demonstrates good accuracy and responsiveness on Kenyan survey data.

Conclusion: The edge–cloud cascade is a practical, accessible platform for STT/TTS in under-resourced languages, offering favorable memory and latency characteristics suitable for deployment.

Abstract: This paper presents a novel framework for speech transcription and synthesis,
leveraging edge-cloud parallelism to enhance processing speed and accessibility
for Kinyarwanda and Swahili speakers. It addresses the scarcity of powerful
language processing tools for these widely spoken languages in East African
countries with limited technological infrastructure. The framework utilizes the
Whisper and SpeechT5 pre-trained models to enable speech-to-text (STT) and
text-to-speech (TTS) translation. The architecture uses a cascading mechanism
that distributes the model inference workload between the edge device and the
cloud, thereby reducing latency and resource usage, benefiting both ends. On
the edge device, our approach achieves a memory usage compression of 9.5% for
the SpeechT5 model and 14% for the Whisper model, with a maximum memory usage
of 149 MB. Experimental results indicate that on a 1.7 GHz CPU edge device with
a 1 MB/s network bandwidth, the system can process a 270-character text in less
than a minute for both speech-to-text and text-to-speech transcription. Using
real-world survey data from Kenya, it is shown that the cascaded edge-cloud
architecture proposed could easily serve as an excellent platform for STT and
TTS transcription with good accuracy and response time.

</details>


### [7] [Layout-Agnostic MPI Abstraction for Distributed Computing in Modern C++](https://arxiv.org/abs/2510.16890)
*Jiří Klepl,Martin Kruliš,Matyáš Brabec*

Main category: cs.DC

TL;DR: They introduce a C++ abstraction layer for MPI, implemented as an extension to the Noarr library, enabling layout-agnostic, type-safe, generic MPI programming with performance comparable to state-of-the-art C++ MPI bindings. A distributed GEMM serves as a case study.


<details>
  <summary>Details</summary>
Motivation: MPI’s legacy C interface lacks modern language features such as strong type-checking and generic programming, making it awkward to build flexible, layout-agnostic distributed applications. The authors aim to bring modern C++ abstractions to MPI while preserving performance.

Method: Extend the C++ Noarr library with MPI capabilities that follow Noarr’s first-class layout and traversal abstractions. Provide an API that decouples data layout from communication and computation. Validate by implementing a layout-agnostic distributed GEMM kernel and comparing against contemporary C++ MPI bindings.

Result: The proposed abstraction shows performance on par with state-of-the-art C++ MPI bindings in the GEMM case study while demonstrating cleaner, more flexible design and syntax.

Conclusion: A Noarr-based, layout-agnostic MPI abstraction in C++ can retain high performance while improving type safety, genericity, and design flexibility for distributed applications.

Abstract: Message Passing Interface (MPI) has been a well-established technology in the
domain of distributed high-performance computing for several decades. However,
one of its greatest drawbacks is a rather ancient pure-C interface. It lacks
many useful features of modern languages (namely C++), like basic type-checking
or support for generic code design. In this paper, we propose a novel
abstraction for MPI, which we implemented as an extension of the C++ Noarr
library. It follows Noarr paradigms (first-class layout and traversal
abstraction) and offers layout-agnostic design of MPI applications. We also
implemented a layout-agnostic distributed GEMM kernel as a case study to
demonstrate the usability and syntax of the proposed abstraction. We show that
the abstraction achieves performance comparable to the state-of-the-art MPI C++
bindings while allowing for a more flexible design of distributed applications.

</details>


### [8] [Tutoring LLM into a Better CUDA Optimizer](https://arxiv.org/abs/2510.16933)
*Matyáš Brabec,Jiří Klepl,Michal Töpfer,Martin Kruliš*

Main category: cs.DC

TL;DR: The paper evaluates whether recent reasoning LLMs can autonomously generate and optimize CUDA kernels for standard tasks, finding they can produce correct, reasonably fast code but typically need targeted tutoring/prompts and interactive iteration to reach expert-level optimization.


<details>
  <summary>Details</summary>
Motivation: To understand the extent to which modern LLMs can perform GPU-oriented performance engineering—specifically, which optimization techniques and parallel patterns they can apply unaided—and whether structured guidance (tutoring) meaningfully improves outcomes.

Method: Generate CUDA implementations for predefined, well-known kernels using state-of-the-art reasoning LLMs. Compare base prompts vs tutoring prompts with detailed hints/guidelines. Evaluate automatically for correctness and speedups; conduct manual code reviews; and test interactive, within-session self-correction to fix prior mistakes.

Result: LLMs reliably produce working CUDA code and some optimizations. However, without tutoring they often miss advanced parallel patterns and performance-critical details. Tutoring and interactive refinement substantially improve quality and speed, in some cases approaching or matching expert-optimized implementations.

Conclusion: LLMs are competent CUDA coders but generally require structured guidance to achieve expert-level optimization. Prompt design and interactive iteration are key levers; fully autonomous expert-grade optimization remains inconsistent without tutoring.

Abstract: Recent leaps in large language models (LLMs) caused a revolution in
programming tools (like GitHub Copilot) that can help with code generation,
debugging, and even performance optimization. In this paper, we focus on the
capabilities of the most recent reasoning models to generate optimized CUDA
code for predefined, well-known tasks. Our objective is to determine which
types of code optimizations and parallel patterns the LLMs can perform by
themselves and whether they can be improved by tutoring (providing more
detailed hints and guidelines in the prompt). The generated solutions were
evaluated both automatically (for correctness and speedup) and manually (code
reviews) to provide a more detailed perspective. We also tried an interactive
approach where the LLM can fix its previous mistakes within a session. The
results indicate that LLMs are quite skilled coders; however, they require
tutoring to reach optimized solutions provided by parallel computing experts.

</details>
