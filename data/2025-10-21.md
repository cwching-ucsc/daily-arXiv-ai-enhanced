<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 3]
- [cs.DC](#cs.DC) [Total: 5]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Agentic AI for Ultra-Modern Networks: Multi-Agent Framework for RAN Autonomy and Assurance](https://arxiv.org/abs/2510.16144)
*Sukhdeep Singh,Avinash Bhat,Shweta M,Subhash K Singh,Moonki Hong,Madhan Raj K,Kandeepan Sithamparanathan,Sunder A. Khowaja,Kapal Dev*

Main category: cs.NI

TL;DR: Proposes replacing centralized RIC workflows with a distributed multi-agent architecture for B5G/6G RAN autonomy; shows in a traffic-steering case that agentic safety checks prevent policies that would destabilize neighbors, preserving global KPIs.


<details>
  <summary>Details</summary>
Motivation: Centralized O-RAN RIC orchestration concentrates intelligence and is vulnerable to policy conflicts, data drift, and unsafe decisions under unforeseen conditions; next-generation networks require autonomous, explainable, resilient assurance mechanisms.

Method: Designs a multi-agentic framework where specialized agents handle data collection, model training, prediction, policy generation, verification, deployment, and assurance. Implements a traffic-steering use case under surge and drift, comparing a naive predictor-driven deployment against the agentic system across four KPIs (RRC connected users, IP throughput, PRB utilization, SINR).

Result: The naive predictor improves local KPIs but produces negative externalities that destabilize neighboring cells. The agentic system detects and blocks unsafe policies, maintaining global network health and better overall KPI balance.

Conclusion: Multi-agent architectures are presented as a credible foundation for trustworthy AI-driven autonomy in next-generation RANs; distributed verification and safety enforcement are key to avoiding local optimizations that harm system-wide performance.

Abstract: The increasing complexity of Beyond 5G and 6G networks necessitates new
paradigms for autonomy and assur- ance. Traditional O-RAN control loops rely
heavily on RIC- based orchestration, which centralizes intelligence and exposes
the system to risks such as policy conflicts, data drift, and unsafe actions
under unforeseen conditions. In this work, we argue that the future of
autonomous networks lies in a multi-agentic architecture, where specialized
agents collaborate to perform data collection, model training, prediction,
policy generation, verification, deployment, and assurance. By replacing
tightly- coupled centralized RIC-based workflows with distributed agents, the
framework achieves autonomy, resilience, explainability, and system-wide
safety. To substantiate this vision, we design and evaluate a traffic steering
use case under surge and drift conditions. Results across four KPIs: RRC
connected users, IP throughput, PRB utilization, and SINR, demonstrate that a
naive predictor-driven deployment improves local KPIs but destabilizes
neighbors, whereas the agentic system blocks unsafe policies, preserving global
network health. This study highlights multi- agent architectures as a credible
foundation for trustworthy AI- driven autonomy in next-generation RANs.

</details>


### [2] [Mamba4Net: Distilled Hybrid Mamba Large Language Models For Networking](https://arxiv.org/abs/2510.17147)
*Linhan Xia,Mingzhan Yang,Jingjing Wang,Ziwei Yan,Yakun Ren,Guo Yu,Kai Lei*

Main category: cs.NI

TL;DR: 提出 Mamba4Net，一個將 transformer-based LLM 的網路領域知識蒸餾到 Mamba 架構（線性時間複雜度）的跨架構蒸餾框架，以在三個網路任務上保有效能同時大幅降低資源消耗（吞吐 3.96×，儲存僅 5.48%）。原始碼公開。


<details>
  <summary>Details</summary>
Motivation: Transformers 在網路研究中表現良好但計算與記憶體開銷高昂；需要一種能在資源受限環境中保留 LLM 知識但更有效率的模型。

Method: 從 transformer-based LLM 以知識蒸餾方式將網路專屬知識轉移至基於 Mamba 的學生模型；Mamba 採線性時間複雜度與較小模型尺寸以減少計算與儲存成本；在視窗預測、適應性碼率串流與叢集排程三個任務上進行評估。

Result: 相較於未利用 LLM 的傳統方法，Mamba4Net 在任務表現上更佳；相較於直接採用 transformer LLM，Mamba4Net 在效率上大幅提升（吞吐約 3.96 倍、儲存僅 5.48%）。

Conclusion: Mamba4Net 展示了跨架構蒸餾可在保留或提升任務效能的同時，顯著降低計算與儲存需求，具備在資源受限網路場景中實用的潛力。

Abstract: Transformer-based large language models (LLMs) are increasingly being adopted
in networking research to address domain-specific challenges. However, their
quadratic time complexity and substantial model sizes often result in
significant computational overhead and memory constraints, particularly in
resource-constrained environments. Drawing inspiration from the efficiency and
performance of the Deepseek-R1 model within the knowledge distillation
paradigm, this paper introduces Mamba4Net, a novel cross-architecture
distillation framework. Mamba4Net transfers networking-specific knowledge from
transformer-based LLMs to student models built on the Mamba architecture, which
features linear time complexity. This design substantially enhances
computational efficiency compared to the quadratic complexity of
transformer-based models, while the reduced model size further minimizes
computational demands, improving overall performance and resource utilization.
To evaluate its effectiveness, Mamba4Net was tested across three diverse
networking tasks: viewport prediction, adaptive bitrate streaming, and cluster
job scheduling. Compared to existing methods that do not leverage LLMs,
Mamba4Net demonstrates superior task performance. Furthermore, relative to
direct applications of transformer-based LLMs, it achieves significant
efficiency gains, including a throughput 3.96 times higher and a storage
footprint of only 5.48% of that required by previous LLM-based approaches.
These results highlight Mamba4Net's potential to enable the cost-effective
application of LLM-derived knowledge in networking contexts. The source code is
openly available to support further research and development.

</details>


### [3] [Is It Worth to Use Feedback Channel in 5G V2X Platoon Scenarios?](https://arxiv.org/abs/2510.17410)
*Dmitry Bankov,Artem Krasilov,Artem Otmakhov,Pavel Savlukovich,Evgeny Khorov*

Main category: cs.NI

TL;DR: This paper evaluates how introducing a feedback channel in 5G V2X (to support groupcast/unicast) affects system capacity in a mixed scenario with a platoon (groupcast) and surrounding vehicles (broadcast). Using NS-3 simulations they find feedback can either nearly double capacity or nearly halve it, depending on platoon size, traffic load and QoS; adaptive feedback parameter selection is proposed.


<details>
  <summary>Details</summary>
Motivation: 5G V2X adds groupcast/unicast and receiver feedback to improve reliability and transmission parameter selection. However feedback consumes radio resources, so the net effect on overall system capacity in mixed traffic scenarios is unclear and must be quantified.

Method: Build a mixed traffic scenario (platoon producing groupcast + surrounding broadcast) in NS-3. Sweep variables: platoon size, groupcast and broadcast traffic intensities, QoS requirements, and feedback-channel parameterization. Measure system capacity and analyze trade-offs.

Result: Simulations show a context-dependent effect: in some parameter regimes feedback increases capacity up to ~2x (by improving reliability and enabling more aggressive transmission settings), while in others it reduces capacity by up to ~50% (because feedback consumes scarce resources and interferes with broadcast flows). The paper analyzes root causes and interactions.

Conclusion: The benefit of a feedback channel in 5G V2X is not universal; it depends on traffic mix, platoon size and QoS. Adaptive selection of feedback resources/parameters is necessary to realize gains and avoid performance degradation.

Abstract: 5G Vehicle-to-Everything (V2X) is a new technology developed by 3GPP to
support inter-vehicle communication. In contrast to 4G V2X which allows only
broadcast communication, 5G V2X enables groupcast and unicast communication.
Such types of communication are needed for new V2X scenarios: platooning,
extended sensors, remote driving, etc. To improve the data transmission
reliability and assist in the selection of the transmission parameters in these
scenarios, 5G V2X introduces a feedback channel that allows receivers to send
acknowledgments in response to data packets. However, some part of the overall
resource shall be allocated for the feedback channel, which reduces the amount
of channel resources available for data transmission. In this paper, we
consider a scenario with a platoon, which generates groupcast traffic, and
surrounding vehicles, which generate legacy broadcast traffic. Using extensive
simulations in NS-3, we analyze how the usage of the feedback channel
influences the overall system capacity. Our results show that depending on the
platoon size, groupcast, and broadcast traffic intensities, and their quality
of service requirements, the usage of the feedback channel can in some cases
significantly increase the system capacity (up to 2x), while in other cases it
almost halves the system capacity. We explain the reasons for such effects and
discuss how to adaptively select the feedback channel parameters.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [MeCeFO: Enhancing LLM Training Robustness via Fault-Tolerant Optimization](https://arxiv.org/abs/2510.16415)
*Rizhen Hu,Yutong He,Ran Yan,Mou Sun,Binghang Yuan,Kun Yuan*

Main category: cs.DC

TL;DR: 提出 MeCeFO，一種在節點故障時將訓練工作無縫移到鄰近節點，並以記憶體與運算效率為設計目標的容錯優化演算法；透過跳接（於反向傳播略過 MHA）、重計算（recomputation）與低秩梯度近似三項技術減少額外負擔，理論上收斂率與常規分散訓練相當（O(1/√{nT})），實驗顯示在高失效率下僅有約 4.18% 吞吐量下降，且比最先進方法有 5.0×–6.7× 的韌性提升。


<details>
  <summary>Details</summary>
Motivation: 隨著 LLM 訓練走向大規模分散化，硬體節點故障頻率不可忽視。既有容錯方法往往引入顯著的計算或記憶體開銷，需要額外資源。設計一套在發生節點故障時，能以最小額外成本繼續穩健訓練的演算法，是主要動機。

Method: MeCeFO 在故障發生時將失效節點的任務移交給鄰近節點，並以三項演算法優化降低鄰居的額外負擔：(i) Skip-connection：於反向傳播時略過多頭注意力（MHA）模組，以節省記憶體與運算量，作為近似策略；(ii) Recomputation：在 FFN 中透過重計算以降低激活值記憶體需求；(iii) Low-rank gradient approximation：對 FFN 權重矩陣的梯度採低秩近似以降低梯度估計成本。整體設計兼顧效率與穩定性。

Result: 理論分析表明 MeCeFO 可保持與常規分散訓練相同等階的收斂速率 O(1/√{nT})。實驗結果顯示在高失效率條件下，MeCeFO 僅造成約 4.18% 的吞吐量下降，且在韌性上比先前最先進方法高出 5.0× 到 6.7×。原始程式碼已公開。

Conclusion: MeCeFO 能在不顯著增加資源需求的情況下，為大規模 LLM 訓練提供高效的容錯能力。其演算法性近似與重計算策略在實務上達到良好權衡，但仍需進一步評估近似對最終模型精度與不同拓樸、失效模型下的泛化性。

Abstract: As distributed optimization scales to meet the demands of Large Language
Model (LLM) training, hardware failures become increasingly non-negligible.
Existing fault-tolerant training methods often introduce significant
computational or memory overhead, demanding additional resources. To address
this challenge, we propose Memory- and Computation-efficient Fault-tolerant
Optimization (MeCeFO), a novel algorithm that ensures robust training with
minimal overhead. When a computing node fails, MeCeFO seamlessly transfers its
training task to a neighboring node while employing memory- and
computation-efficient algorithmic optimizations to minimize the extra workload
imposed on the neighboring node handling both tasks. MeCeFO leverages three key
algorithmic designs: (i) Skip-connection, which drops the multi-head attention
(MHA) module during backpropagation for memory- and computation-efficient
approximation; (ii) Recomputation, which reduces activation memory in
feedforward networks (FFNs); and (iii) Low-rank gradient approximation,
enabling efficient estimation of FFN weight matrix gradients. Theoretically,
MeCeFO matches the convergence rate of conventional distributed training, with
a rate of $\mathcal{O}(1/\sqrt{nT})$, where n is the data parallelism size and
T is the number of iterations. Empirically, MeCeFO maintains robust performance
under high failure rates, incurring only a 4.18% drop in throughput,
demonstrating 5.0$\times$ to 6.7$\times$ greater resilience than previous SOTA
approaches. Codes are available at https://github.com/pkumelon/MeCeFO.

</details>


### [5] [FourierCompress: Layer-Aware Spectral Activation Compression for Efficient and Accurate Collaborative LLM Inference](https://arxiv.org/abs/2510.16418)
*Jian Ma,Xinchen Lyu,Jun Jiang,Longhao Zou,Chenshan Ren,Qimei Cui,Xiaofeng Tao*

Main category: cs.DC

TL;DR: 提出 FourierCompress，一種基於頻域稀疏性的層感知激活壓縮框架，利用 FFT 保留低頻係數並用共軛對稱重建，以在邊緣設備上實現高壓縮比、近無損推理與快速硬體加速。


<details>
  <summary>Details</summary>
Motivation: 協作式 LLM 推理在資源受限的邊緣設備上需要分擔運算與保護隱私，但中間激活向量高維且隨生成長度線性增長，造成通訊瓶頸；現有壓縮方法難以兼顧壓縮率、重建誤差與計算效率。

Method: 觀察 Transformer 首層激活在頻域具有平滑性與能量低頻集中性，對激活做 FFT，僅保留一個低頻係數塊並利用共軛對稱性在伺服器端重建；設計為層感知並可在 DSP/FPU/FPGA 上硬體加速。

Result: 在 Llama 3 與 Qwen2.5 並跨 10 個常識推理數據集的實驗中，平均壓縮比約 7.6×，平均精度損失 <0.3%，且在硬體加速下壓縮時間比 Top-k 快 >32×，在效能上優於 Top-k、QR、SVD。

Conclusion: FourierCompress 在邊緣 LLM 協作推理場景中成功平衡了通訊效率、近無損推理與壓縮速度，為實時、隱私保護的邊緣 AI 提供可行解。

Abstract: Collaborative large language model (LLM) inference enables real-time,
privacy-preserving AI services on resource-constrained edge devices by
partitioning computational workloads between client devices and edge servers.
However, this paradigm is severely hindered by communication bottlenecks caused
by the transmission of high-dimensional intermediate activations, exacerbated
by the autoregressive decoding structure of LLMs, where bandwidth consumption
scales linearly with output length. Existing activation compression methods
struggle to simultaneously achieve high compression ratios, low reconstruction
error, and computational efficiency. This paper proposes FourierCompress, a
novel, layer-aware activation compression framework that exploits the
frequency-domain sparsity of LLM activations. We rigorously demonstrate that
activations from the first Transformer layer exhibit strong smoothness and
energy concentration in the low-frequency domain, making them highly amenable
to near-lossless compression via the Fast Fourier Transform (FFT).
FourierCompress transforms activations into the frequency domain, retains only
a compact block of low-frequency coefficients, and reconstructs the signal at
the server using conjugate symmetry, enabling seamless hardware acceleration on
DSPs and FPGAs. Extensive experiments on Llama 3 and Qwen2.5 models across 10
commonsense reasoning datasets demonstrate that FourierCompress preserves
performance remarkably close to the uncompressed baseline, outperforming Top-k,
QR, and SVD. FourierCompress bridges the gap between communication efficiency
(an average 7.6x reduction in activation size), near-lossless inference (less
than 0.3% average accuracy loss), and significantly faster compression
(achieving over 32x reduction in compression time compared to Top-k via
hardware acceleration) for edge-device LLM inference.

</details>


### [6] [Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages](https://arxiv.org/abs/2510.16497)
*Pacome Simon Mbonimpa,Diane Tuyizere,Azizuddin Ahmed Biyabani,Ozan K. Tonguz*

Main category: cs.DC

TL;DR: 提出一套以邊緣—雲端平行處理為核心的語音轉寫與合成框架，使用 Whisper 與 SpeechT5 作為預訓練模型，透過串流/分散推論降低延遲與記憶體使用，針對肯亞實際問卷資料在低資源裝置（1.7 GHz CPU、1 MB/s）上能於一分鐘內完成 270 字的 STT 與 TTS，並將 SpeechT5 與 Whisper 記憶體分別壓縮約 9.5% 和 14%，最大記憶體使用 149 MB。


<details>
  <summary>Details</summary>
Motivation: 解決東非廣泛使用但資源匱乏語言（如 Kinyarwanda 與 Swahili）缺乏強大語言處理工具，以及當地設備與網路基礎設施有限，需設計能在低規格邊緣裝置上提供可接受延遲與準確度的 STT/TTS 解決方案。

Method: 採用 Whisper（STT）與 SpeechT5（TTS）之預訓練模型，設計一個串接/層級式（cascading）架構將推論工作在邊緣裝置與雲端間分配以平衡延遲與資源消耗；針對邊緣端進行模型壓縮並限制記憶體使用，上傳/下載量受網路頻寬影響，實驗在 1.7 GHz CPU 與 1 MB/s 網路條件下進行效能測試；使用來自肯亞的實務問卷語音/文本作為驗證資料。

Result: 在目標硬體與網路條件下，可於一分鐘內完成 270 字的 STT 與 TTS 任務；對 SpeechT5 與 Whisper 分別達到約 9.5% 與 14% 的記憶體使用壓縮，最大記憶體峰值為 149 MB；實驗數據（基於肯亞問卷）顯示系統在回應時間與準確度上表現良好，適合作為低資源環境的語音處理解決方案。

Conclusion: 所提出的邊緣—雲端串接架構能在受限硬體與頻寬下提供可用的 STT 與 TTS 服務，降低延遲與資源消耗，對 Kinyarwanda 與 Swahili 等低資源語言具有實務應用潛力。然而仍需補充詳盡的評估指標（如 WER、MOS）、與基準比較、可擴展性與網路斷線情境下的處理策略以驗證穩健性。

Abstract: This paper presents a novel framework for speech transcription and synthesis,
leveraging edge-cloud parallelism to enhance processing speed and accessibility
for Kinyarwanda and Swahili speakers. It addresses the scarcity of powerful
language processing tools for these widely spoken languages in East African
countries with limited technological infrastructure. The framework utilizes the
Whisper and SpeechT5 pre-trained models to enable speech-to-text (STT) and
text-to-speech (TTS) translation. The architecture uses a cascading mechanism
that distributes the model inference workload between the edge device and the
cloud, thereby reducing latency and resource usage, benefiting both ends. On
the edge device, our approach achieves a memory usage compression of 9.5% for
the SpeechT5 model and 14% for the Whisper model, with a maximum memory usage
of 149 MB. Experimental results indicate that on a 1.7 GHz CPU edge device with
a 1 MB/s network bandwidth, the system can process a 270-character text in less
than a minute for both speech-to-text and text-to-speech transcription. Using
real-world survey data from Kenya, it is shown that the cascaded edge-cloud
architecture proposed could easily serve as an excellent platform for STT and
TTS transcription with good accuracy and response time.

</details>


### [7] [Layout-Agnostic MPI Abstraction for Distributed Computing in Modern C++](https://arxiv.org/abs/2510.16890)
*Jiří Klepl,Martin Kruliš,Matyáš Brabec*

Main category: cs.DC

TL;DR: 提出一個以 C++ Noarr 擴充為基礎的 MPI 抽象層，提供 layout-agnostic 的分散式應用設計，並以分散式 GEMM 作為案例驗證：語意更現代且性能與現有 MPI C++ 綁定相當。


<details>
  <summary>Details</summary>
Motivation: MPI 原生介面為古老的純 C API，缺乏 C++ 的型別檢查與泛型支援，導致使用者在開發可重用與靈活的分散式高效能程式時受限。需要一種能保留 MPI 性能同時提供現代語言抽象的解決方案。

Method: 在 Noarr（強調資料佈局與遍歷的第一類抽象）基礎上實作一個 MPI 抽象擴充，讓通信 API 與資料佈局分離，支援 layout-agnostic 的程式設計。以一個 layout-agnostic 的分散式 GEMM kernel 作為案例，展示語法與可用性，並與現有 MPI C++ 綁定進行效能比較。

Result: 實驗顯示提出的抽象在效能上可與最先進的 MPI C++ 綁定相當，同時提供更靈活與可泛化的程式設計介面。案例的分散式 GEMM 成功展示了該抽象的適用性與易用性。

Conclusion: 該 Noarr 擴充的 MPI 抽象能在不犧牲效能的情況下，把現代 C++ 的型別與泛型優勢帶入 MPI 應用，促進更具彈性與可重用的分散式高效能程式設計。

Abstract: Message Passing Interface (MPI) has been a well-established technology in the
domain of distributed high-performance computing for several decades. However,
one of its greatest drawbacks is a rather ancient pure-C interface. It lacks
many useful features of modern languages (namely C++), like basic type-checking
or support for generic code design. In this paper, we propose a novel
abstraction for MPI, which we implemented as an extension of the C++ Noarr
library. It follows Noarr paradigms (first-class layout and traversal
abstraction) and offers layout-agnostic design of MPI applications. We also
implemented a layout-agnostic distributed GEMM kernel as a case study to
demonstrate the usability and syntax of the proposed abstraction. We show that
the abstraction achieves performance comparable to the state-of-the-art MPI C++
bindings while allowing for a more flexible design of distributed applications.

</details>


### [8] [Tutoring LLM into a Better CUDA Optimizer](https://arxiv.org/abs/2510.16933)
*Matyáš Brabec,Jiří Klepl,Michal Töpfer,Martin Kruliš*

Main category: cs.DC

TL;DR: LLMs 能生成 CUDA 程式並做部分優化，但要達到並行運算專家的水準仍需提示（tutoring）與互動修正。


<details>
  <summary>Details</summary>
Motivation: 評估最新推理型 LLM 在為已知任務產生優化 CUDA 程式、辨識並行模式與應用最佳化技巧上的能力，以及提示（prompt tutoring）與互動修正對結果的影響。

Method: 讓 LLM 生成針對預定範例任務的 CUDA 程式；使用自動化測試（正確性、加速比）與人工程式碼審查評估；比較原始提示、經過詳細提示（tutoring）與互動修正策略的結果。

Result: LLMs 在產生功能正確的 CUDA 程式方面表現良好，能夠採用部分並行模式與基本優化；透過提示與互動可進一步改善效能，但仍常落後於並行運算專家所提供的最佳化版本。

Conclusion: LLMs 是有用的程式協助工具，但若要達到專家級的 CUDA 優化，需要專家介入、精緻化提示或針對性微調。建議擴大任務集、研究自動提示生成與人機互動流程來縮小差距。

Abstract: Recent leaps in large language models (LLMs) caused a revolution in
programming tools (like GitHub Copilot) that can help with code generation,
debugging, and even performance optimization. In this paper, we focus on the
capabilities of the most recent reasoning models to generate optimized CUDA
code for predefined, well-known tasks. Our objective is to determine which
types of code optimizations and parallel patterns the LLMs can perform by
themselves and whether they can be improved by tutoring (providing more
detailed hints and guidelines in the prompt). The generated solutions were
evaluated both automatically (for correctness and speedup) and manually (code
reviews) to provide a more detailed perspective. We also tried an interactive
approach where the LLM can fix its previous mistakes within a session. The
results indicate that LLMs are quite skilled coders; however, they require
tutoring to reach optimized solutions provided by parallel computing experts.

</details>
