<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 3]
- [cs.DC](#cs.DC) [Total: 6]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [A Layered Protocol Architecture for the Internet of Agents](https://arxiv.org/abs/2511.19699)
*Charles Fleming,Vijoy Pandey,Ramana Kompella,Luca Muscariello*

Main category: cs.NI

TL;DR: Proposes extending network stacks with two new layers — Agent Communication Layer (L8) for message structure and interaction patterns, and Agent Semantic Negotiation Layer (L9) for discovering/locking a shared formal context — to enable scalable, distributed collaboration among LLM-based agents.


<details>
  <summary>Details</summary>
Motivation: LLMs can act as agents and call tools but are limited by finite context windows and computation; complex problems require distributed agent collaboration and shared semantic understanding, which existing network stacks (OSI/TCP-IP) don't support.

Method: Introduce L8 to standardize envelopes, performatives and interaction patterns (building on protocols like MCP), and L9 to formalize semantics via a negotiable Shared Context (schema of concepts, tasks, parameters) enabling discovery, negotiation and locking among agents.

Result: A conceptual two-layer extension that provides structure (L8) and meaning (L9) for agent communication, outlining a path to scalable agent collaboration (Internet of Agents) and enabling multi-agentic systems to coordinate complex, distributed computations.

Conclusion: Formalizing both communication structure and shared semantics is crucial to scale agent collaboration beyond individual LLM limits; L8 and L9 together create a foundation for interoperable, semantically-grounded agent ecosystems, though practical challenges in semantics, trust, synchronization and standardization remain.

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance improvements and the ability to learn domain-specific languages (DSLs), including APIs and tool interfaces. This capability has enabled the creation of AI agents that can perform preliminary computations and act through tool calling, now being standardized via protocols like MCP. However, LLMs face fundamental limitations: their context windows cannot grow indefinitely, constraining their memory and computational capacity. Agent collaboration emerges as essential for solving increasingly complex problems, mirroring how computational systems rely on different types of memory to scale. The "Internet of Agents" (IoA) represents the communication stack that enables agents to scale by distributing computation across collaborating entities.
  Current network architectural stacks (OSI and TCP/IP) were designed for data delivery between hosts and processes, not for agent collaboration with semantic understanding. To address this gap, we propose two new layers: an \textbf{Agent Communication Layer (L8)} and an \textbf{Agent Semantic Negotiation Layer (L9)}. L8 formalizes the \textit{structure} of communication, standardizing message envelopes, speech-act performatives (e.g., REQUEST, INFORM), and interaction patterns (e.g., request-reply, publish-subscribe), building on protocols like MCP. L9, which does not exist today, formalizes the \textit{meaning} of communication, enabling agents to discover, negotiate, and lock a "Shared Context" -- a formal schema defining the concepts, tasks, and parameters relevant to their interaction. Together, these layers provide the foundation for scalable, distributed agent collaboration, enabling the next generation of multi-agentic systems.

</details>


### [2] [Field Test of 5G New Radio (NR) UL-MIMO and UL-256QAM for HD Live-Streaming](https://arxiv.org/abs/2511.19868)
*Kasidis Arunruangsirilert*

Main category: cs.NI

TL;DR: This paper evaluates how 5G uplink features — UL‑MIMO and UL‑256QAM — affect HD live video streaming over a commercial 5G network. Using firmware-modified UEs to toggle features on the same device, the study measures dropped frames, connection stability, and CSI-derived RF metrics (CSI‑RSRP, CSI‑RSRQ, CSI‑SINR) while streaming via RTMP.


<details>
  <summary>Details</summary>
Motivation: UGC and HD live streaming stress mobile uplinks; 5G NR adds UL‑MIMO and UL‑256QAM to raise uplink throughput and spectral efficiency, but the real-world impact on latency‑sensitive, real‑time applications (e.g., live streaming) is poorly understood. The paper aims to connect PHY/MAC enhancements to application‑level performance.

Method: Modify commercial UE modem firmware to selectively enable/disable UL‑MIMO and UL‑256QAM on the same device. Stream HD video using RTMP over a commercial 5G network. Collect application metrics (dropped frames, connection stability) and RF/CSI metrics (CSI‑RSRP, CSI‑RSRQ, CSI‑SINR). Perform comparative analysis across feature states.

Result: The abstract does not give numeric results; it reports a comparative evaluation and analysis of spectral efficiency via CSI metrics. Presumably the study quantifies improvements in throughput/stability and changes in CSI metrics when enabling UL‑MIMO and UL‑256QAM, but exact magnitudes and statistical significance are not reported in the abstract.

Conclusion: The work attempts to bridge PHY enhancements to real-time application QoE by using controlled firmware toggling on commercial UEs and measuring both application and RF metrics. The contribution is a practical, empirically driven assessment, though the abstract lacks details on experimental conditions, statistical rigor, and concrete outcomes.

Abstract: The exponential growth of User-Generated Content (UGC), especially High-Definition (HD) live video streaming, places a significant demand on the uplink capabilities of mobile networks. To address this, the 5G New Radio (NR) standard introduced key uplink enhancements, including Uplink Multi-Input Multi-Output (UL-MIMO) and Uplink 256QAM, to improve throughput and spectral efficiency. However, while the benefits of these features for raw data rates are well-documented, their practical impact on real-time applications like live-streaming is not yet well understood. This paper investigates the performance of UL-MIMO and UL-256QAM for HD live-streaming over a commercial 5G network using the Real-Time Messaging Protocol (RTMP). To ensure a fair assessment, we conduct a comparative analysis by modifying the modem firmware of commercial User Equipment (UE), allowing these features to be selectively enabled and disabled on the same device. Performance is evaluated based on key metrics, including dropped video frames and connection stability. Furthermore, this study analyzes 5G Radio Frequency (RF) parameters to quantify the spectral efficiency impact, specifically examining metrics derived from the Channel State Information (CSI) framework, including Reference Signal Received Power (CSI-RSRP), Reference Signal Received Quality (CSI-RSRQ), and Signal-to-Interference-plus-Noise Ratio (CSI-SINR).

</details>


### [3] [RIS-Assisted Downlink Pinching-Antenna Systems: GNN-Enabled Optimization Approaches](https://arxiv.org/abs/2511.20305)
*Changpeng He,Yang Lu,Yanqing Xu,Chong-Yung Chi,Bo Ai,Arumugam Nallanathan*

Main category: cs.NI

TL;DR: Proposes a RIS-assisted multi-waveguide pinching-antenna system (PASS) for multi-user downlink and formulates unified sum-rate and energy-efficiency maximization problems. Introduces a three-stage graph neural network (GNN) that sequentially predicts PA positions, RIS phase shifts, and beamforming vectors; trained unsupervised and combined with convex optimization strategies for trade-offs between speed and optimality. Numerical results show good generalization, reliability, and real-time potential.


<details>
  <summary>Details</summary>
Motivation: The integration of movable pinching-antenna systems (PASS) with reconfigurable intelligent surfaces (RIS) is new, and its impact on wireless downlink performance (SR/EE) is not well understood; need methods that jointly optimize PA placement, RIS configuration, and beamforming under practical constraints.

Method: Formulate SR and EE maximization under PA movable-region, power budget, and RIS phase constraints. Model the system as a graph and design a three-stage GNN: Stage 1 maps user locations to PA positions, Stage 2 maps composite channel conditions to RIS phase shifts, Stage 3 outputs beamforming vectors. Use unsupervised training directly on objective(s), and propose three integration strategies combining the GNN with convex optimization to balance inference latency and solution quality.

Result: Extensive simulations demonstrate the proposed GNN achieves strong performance, generalizes well to new scenarios, provides reliable results, and can operate in real time. The paper also analyzes how key system parameters affect RIS-assisted PASS performance.

Conclusion: A GNN-based control framework effectively coordinates movable PAs, RIS phase shifts, and beamforming to improve SR/EE in RIS-assisted PASS. The approach shows practical promise but would benefit from further theoretical analysis, robustness tests, and real-world validation.

Abstract: This paper investigates a reconfigurable intelligent surface (RIS)-assisted multi-waveguide pinching-antenna (PA) system (PASS) for multi-user downlink information transmission, motivated by the unknown impact of the integration of emerging PASS and RIS on wireless communications. First, we formulate sum rate (SR) and energy efficiency (EE) maximization problems in a unified framework, subject to constraints on the movable region of PAs, total power budget, and tunable phase of RIS elements. Then, by leveraging a graph-structured topology of the RIS-assisted PASS, a novel three-stage graph neural network (GNN) is proposed, which learns PA positions based on user locations, and RIS phase shifts according to composite channel conditions at the first two stages, respectively, and finally determines beamforming vectors. Specifically, the proposed GNN is achieved through unsupervised training, together with three implementation strategies for its integration with convex optimization, thus offering trade-offs between inference time and solution optimality. Extensive numerical results are provided to validate the effectiveness of the proposed GNN, and to support its unique attributes of viable generalization capability, good performance reliability, and real-time applicability. Moreover, the impact of key parameters on RIS-assisted PASS is illustrated and analyzed.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [AI-driven Predictive Shard Allocation for Scalable Next Generation Blockchains](https://arxiv.org/abs/2511.19450)
*M. Zeeshan Haider,Tayyaba Noreen,M. D. Assuncao,Kaiwen Zhang*

Main category: cs.DC

TL;DR: PSAP is a dynamic shard allocation framework that uses temporal workload forecasting and a safety-constrained RL controller to proactively migrate accounts/transactions, enforcing deterministic inference and safety limits to reduce hotspots and cross-shard overhead.


<details>
  <summary>Details</summary>
Motivation: Static/heuristic shard allocation leads to workload skew, congestion, and excessive cross-shard communication; a predictive, coordinated approach is needed to maintain parallelism and throughput as workload hotspots evolve.

Method: Combine a Temporal Workload Forecasting (TWF) model for multi-step prediction with a Safe-PPO reinforcement learning controller to decide shard reconfigurations. Deterministic inference is enforced with synchronized quantized runtimes; a safety gate constrains stake concentration, migration gas, and utilization thresholds. Migrations are bounded and atomic to preserve safety and liveness.

Result: On datasets derived from Ethereum, NEAR, and Fabric (via address-clustering heuristics), PSAP reports up to 2x throughput improvement, 35% lower latency, and 20% reduced cross-shard overhead versus dynamic sharding baselines.

Conclusion: Predictive, deterministic, and security-aware shard allocation can significantly improve performance and reduce cross-shard costs; integrating forecasting with safety-aware control is a promising direction for scalable blockchains.

Abstract: Sharding has emerged as a key technique to address blockchain scalability by partitioning the ledger into multiple shards that process transactions in parallel. Although this approach improves throughput, static or heuristic shard allocation often leads to workload skew, congestion, and excessive cross-shard communication diminishing the scalability benefits of sharding. To overcome these challenges, we propose the Predictive Shard Allocation Protocol (PSAP), a dynamic and intelligent allocation framework that proactively assigns accounts and transactions to shards based on workload forecasts. PSAP integrates a Temporal Workload Forecasting (TWF) model with a safety-constrained reinforcement learning (Safe-PPO) controller, jointly enabling multi-block-ahead prediction and adaptive shard reconfiguration. The protocol enforces deterministic inference across validators through a synchronized quantized runtime and a safety gate that limits stake concentration, migration gas, and utilization thresholds. By anticipating hotspot formation and executing bounded, atomic migrations, PSAP achieves stable load balance while preserving Byzantine safety. Experimental evaluation on heterogeneous datasets, including Ethereum, NEAR, and Hyperledger Fabric mapped via address-clustering heuristics, demonstrates up to 2x throughput improvement, 35\% lower latency, and 20\% reduced cross-shard overhead compared to existing dynamic sharding baselines. These results confirm that predictive, deterministic, and security-aware shard allocation is a promising direction for next-generation scalable blockchain systems.

</details>


### [5] [SparOA: Sparse and Operator-aware Hybrid Scheduling for Edge DNN Inference](https://arxiv.org/abs/2511.19457)
*Ziyang Zhang,Jie Liu,Luca Mottola*

Main category: cs.DC

TL;DR: SparOA is a CPU–GPU hybrid inference framework that schedules DNN operators based on sparsity and computational intensity. It uses a threshold predictor, an RL-based scheduler, and an asynchronous hybrid engine to improve latency and energy efficiency, achieving average speedups of 1.22–1.31× over baselines and up to 50.7× over CPU-only, while saving 7–16% energy compared to the state-of-the-art co-execution baseline.


<details>
  <summary>Details</summary>
Motivation: DNNs are computationally heavy and hard to run efficiently on resource-constrained edge devices. Existing approaches (compression, specialized hardware, simple co-execution) either harm accuracy, cost too much, or fail to account for operator-level characteristics that influence where and how operators should execute.

Method: SparOA leverages operator sparsity and computational intensity for scheduling. It comprises: (1) a threshold predictor to pick optimal sparsity/intensity cutoffs for offloading; (2) a reinforcement-learning scheduler that dynamically assigns operators to CPU/GPU using real-time hardware state as input; (3) a hybrid inference engine using asynchronous execution and batch-size optimization to improve utilization and throughput.

Result: Experimental evaluation reports 1.22–1.31× average speedup against multiple baselines, up to 50.7× speedup relative to CPU-only execution, and 7–16% lower energy-per-inference than the leading co-execution baseline.

Conclusion: SparOA demonstrates that combining sparsity- and intensity-aware operator scheduling with RL-driven dynamic allocation and an asynchronous hybrid engine yields meaningful runtime and energy gains for edge DNN inference. The paper would benefit from more details on model sets, hardware used, overheads, and generalization.

Abstract: The resource demands of deep neural network (DNN) models introduce significant performance challenges, especially when deployed on resource-constrained edge devices. Existing solutions like model compression often sacrifice accuracy, while specialized hardware remains costly and inflexible. Hybrid inference methods, however, typically overlook how operator characteristics impact performance. In this work, we present SparOA, a CPU-GPU hybrid inference framework, which leverages both sparsity and computational intensity to optimize operator scheduling. SparOA embraces aforementioned challenges through three key components: (1) a threshold predictor that accurately determines optimal sparsity and computational intensity thresholds; (2) a reinforcement learning-based scheduler that dynamically optimizes resource allocation based on real-time hardware states; and (3) a hybrid inference engine that enhances efficiency through asynchronous execution and batch size optimization.Extensive results show that SparOA achieves an average speedup of 1.22-1.31x compared to all baselines, and outperforms the CPU-Only by up to 50.7x. Also, SparOA achieves optimal energy-per-inference, consuming 7\%-16\% less energy than the SOTA co-execution baseline.

</details>


### [6] [Temperature in SLMs: Impact on Incident Categorization in On-Premises Environments](https://arxiv.org/abs/2511.19464)
*Marcio Pohlmann,Alex Severo,Gefté Almeida,Diego Kreutz,Tiago Heinrich,Lourenço Pereira*

Main category: cs.DC

TL;DR: Locally run small-to-medium SLMs can classify incidents, but performance depends mainly on model size and GPU capacity; temperature has negligible effect.


<details>
  <summary>Details</summary>
Motivation: SOC/CSIRT teams want to automate incident categorization while avoiding the cost, latency and confidentiality risks of cloud LLMs, so the study asks whether locally executed SLMs can meet operational needs.

Method: Empirical evaluation of 21 SLMs (1B–20B parameters). The study varied temperature and measured execution time and precision across two different model architectures and different GPU capacities.

Result: Temperature had little effect on classification precision. Model parameter count and available GPU capacity were the dominant factors affecting both precision and execution time.

Conclusion: Running SLMs locally is feasible for incident categorization, but practical deployment requires selecting sufficiently large models and adequate GPU resources; tuning temperature alone is unlikely to improve performance significantly.

Abstract: SOCs and CSIRTs face increasing pressure to automate incident categorization, yet the use of cloud-based LLMs introduces costs, latency, and confidentiality risks. We investigate whether locally executed SLMs can meet this challenge. We evaluated 21 models ranging from 1B to 20B parameters, varying the temperature hyperparameter and measuring execution time and precision across two distinct architectures. The results indicate that temperature has little influence on performance, whereas the number of parameters and GPU capacity are decisive factors.

</details>


### [7] [Federated Learning Framework for Scalable AI in Heterogeneous HPC and Cloud Environments](https://arxiv.org/abs/2511.19479)
*Sangam Ghimire,Paribartan Timalsina,Nirjal Bhurtel,Bishal Neupane,Bigyan Byanju Shrestha,Subarna Bhattarai,Prajwal Gaire,Jessica Thapa,Sudan Jha*

Main category: cs.DC

TL;DR: Presents a federated learning framework tailored for hybrid HPC+cloud environments that tackles system heterogeneity, communication bottlenecks, and resource scheduling; shows scalable, fault-tolerant convergence on a hybrid testbed with non-IID data.


<details>
  <summary>Details</summary>
Motivation: Growing need for scalable, privacy-preserving AI across distributed, mixed computing infrastructures (HPC and cloud) that present heterogenous hardware, limited communication, and non-uniform data distributions.

Method: Design and implementation of a federated learning system optimized for mixed HPC/cloud deployments. The framework handles system heterogeneity, reduces communication overhead, and includes resource scheduling and fault-tolerance mechanisms while preserving model accuracy and data privacy. Evaluated on a hybrid testbed.

Result: Experimental results indicate strong scalability, fault tolerance, and convergence even under non-IID data and varied hardware conditions on the hybrid testbed. The system maintains model accuracy and privacy while operating efficiently across mixed environments.

Conclusion: Federated learning can be made practical for modern distributed AI by adapting systems-level strategies for heterogenous HPC and cloud resources; the proposed framework demonstrates this potential through robust empirical performance.

Abstract: As the demand grows for scalable and privacy-aware AI systems, Federated Learning (FL) has emerged as a promising solution, allowing decentralized model training without moving raw data. At the same time, the combination of high- performance computing (HPC) and cloud infrastructure offers vast computing power but introduces new complexities, especially when dealing with heteroge- neous hardware, communication limits, and non-uniform data. In this work, we present a federated learning framework built to run efficiently across mixed HPC and cloud environments. Our system addresses key challenges such as system het- erogeneity, communication overhead, and resource scheduling, while maintaining model accuracy and data privacy. Through experiments on a hybrid testbed, we demonstrate strong performance in terms of scalability, fault tolerance, and convergence, even under non-Independent and Identically Distributed (non-IID) data distributions and varied hardware. These results highlight the potential of federated learning as a practical approach to building scalable Artificial Intelligence (AI) systems in modern, distributed computing settings.

</details>


### [8] [QiMeng-Kernel: Macro-Thinking Micro-Coding Paradigm for LLM-Based High-Performance GPU Kernel Generation](https://arxiv.org/abs/2511.20100)
*Xinguo Zhu,Shaohui Peng,Jiaming Guo,Yunji Chen,Qi Guo,Yuanbo Wen,Hang Qin,Ruizhi Chen,Qirui Zhou,Ke Gao,Yanjun Wu,Chen Zhao,Ling Li*

Main category: cs.DC

TL;DR: MTMC is a two-level framework for generating high-performance GPU kernels with LLMs. A Macro Thinking module (RL-guided) searches semantic optimization strategies, while a Micro Coding module (general LLMs) incrementally implements each step. Decoupling strategy from low-level code reduces search complexity, improves correctness, and accelerates generation. On KernelBench and TritonBench MTMC outperforms SOTA LLMs in accuracy and runtime, achieving large speedups over LLM baselines and expert PyTorch kernels.


<details>
  <summary>Details</summary>
Motivation: Hand-optimized GPU kernels are essential but hard to produce and poorly portable. Existing LLM methods that directly synthesize full optimized kernels struggle because they must explore an immense joint space of optimization policies and low-level code, leading to trade-offs between correctness and efficiency.

Method: MTMC uses a hierarchical staged approach: Macro Thinking applies reinforcement learning to explore and learn high-level semantic optimization strategies that aim to maximize hardware utilization. Micro Coding uses general-purpose LLMs to implement each proposed optimization incrementally (stepwise), avoiding the brittleness of generating whole kernels at once. This decouples policy search from code generation and reduces error accumulation.

Result: Empirical results on KernelBench show MTMC achieves near-100% accuracy at Levels 1–2 and ~70% at Level 3, outperforming general and domain-finetuned LLMs by over 50% and yielding up to 7.3× speedup versus LLMs and 2.2× over expert PyTorch Eager kernels. On TritonBench, MTMC reaches up to 59.64% accuracy and up to 34× speedup.

Conclusion: Decoupling high-level strategy search (via RL) from low-level incremental code generation (via LLMs) effectively navigates the huge optimization and implementation space, producing more correct and efficient GPU kernels than prior LLM-based methods and even some expert kernels.

Abstract: Developing high-performance GPU kernels is critical for AI and scientific computing, but remains challenging due to its reliance on expert crafting and poor portability. While LLMs offer promise for automation, both general-purpose and finetuned LLMs suffer from two fundamental and conflicting limitations: correctness and efficiency. The key reason is that existing LLM-based approaches directly generate the entire optimized low-level programs, requiring exploration of an extremely vast space encompassing both optimization policies and implementation codes. To address the challenge of exploring an intractable space, we propose Macro Thinking Micro Coding (MTMC), a hierarchical framework inspired by the staged optimization strategy of human experts. It decouples optimization strategy from implementation details, ensuring efficiency through high-level strategy and correctness through low-level implementation. Specifically, Macro Thinking employs reinforcement learning to guide lightweight LLMs in efficiently exploring and learning semantic optimization strategies that maximize hardware utilization. Micro Coding leverages general-purpose LLMs to incrementally implement the stepwise optimization proposals from Macro Thinking, avoiding full-kernel generation errors. Together, they effectively navigate the vast optimization space and intricate implementation details, enabling LLMs for high-performance GPU kernel generation. Comprehensive results on widely adopted benchmarks demonstrate the superior performance of MTMC on GPU kernel generation in both accuracy and running time. On KernelBench, MTMC achieves near 100% and 70% accuracy at Levels 1-2 and 3, over 50% than SOTA general-purpose and domain-finetuned LLMs, with up to 7.3x speedup over LLMs, and 2.2x over expert-optimized PyTorch Eager kernels. On the more challenging TritonBench, MTMC attains up to 59.64% accuracy and 34x speedup.

</details>


### [9] [Beluga: A CXL-Based Memory Architecture for Scalable and Efficient LLM KVCache Management](https://arxiv.org/abs/2511.20172)
*Xinjun Yang,Qingda Hu,Junru Li,Feifei Li,Yuqi Zhou,Yicong Zhu,Qiuru Lin,Jian Dai,Yang Kong,Jiayu Zhang,Guoqiang Xu,Qiang Liu*

Main category: cs.DC

TL;DR: Beluga leverages CXL switch-based memory pools to let GPUs and CPUs share a large KVCache with native load/store semantics, achieving near-local latency and simpler programming versus RDMA. Implemented as Beluga-KVCache for LLM inference, it reduces Time-To-First-Token by 89.6% and improves throughput 7.35x in vLLM compared to RDMA-based solutions.


<details>
  <summary>Details</summary>
Motivation: LLM inference demands large KVCache sizes beyond GPU HBM and single-socket DRAM limits; current disaggregated memory via RDMA suffers high latency, complex protocols, and synchronization overhead. Emerging CXL switches offer a chance to provide large, shared memory with lower latency and simpler semantics.

Method: Systematically characterize a commercial CXL switch-based memory pool to derive design guidelines; design Beluga architecture that provides native load/store access over CXL fabric for GPU/CPU shared memory; implement Beluga-KVCache to manage large-scale KVCache and integrate with vLLM for evaluation.

Result: Beluga delivers near-local memory access latency, reduces programming complexity and synchronization overhead, and in evaluations achieves an 89.6% reduction in TTFT and 7.35x throughput improvement in vLLM inference over RDMA-based baselines.

Conclusion: Beluga is the first system enabling GPUs direct access to large-scale memory pools through CXL switches, demonstrating that CXL can enable low-latency, shared, large memory for LLM inference. The work suggests CXL as a promising path beyond RDMA for memory disaggregation.

Abstract: The rapid increase in LLM model sizes and the growing demand for long-context inference have made memory a critical bottleneck in GPU-accelerated serving systems. Although high-bandwidth memory (HBM) on GPUs offers fast access, its limited capacity necessitates reliance on host memory (CPU DRAM) to support larger working sets such as the KVCache. However, the maximum DRAM capacity is constrained by the limited number of memory channels per CPU socket. To overcome this limitation, current systems often adopt RDMA-based disaggregated memory pools, which introduce significant challenges including high access latency, complex communication protocols, and synchronization overhead. Fortunately, the emerging CXL technology introduces new opportunities in KVCache design. In this paper, we propose Beluga, a novel memory architecture that enables GPUs and CPUs to access a shared, large-scale memory pool through CXL switches. By supporting native load/store access semantics over the CXL fabric, our design delivers near-local memory latency, while reducing programming complexity and minimizing synchronization overhead. We conduct a systematic characterization of a commercial CXL switch-based memory pool and propose a set of design guidelines. Based on Beluga, we design and implement Beluga-KVCache, a system tailored for managing the large-scale KVCache in LLM inference. Beluga-KVCache achieves an 89.6% reduction in Time-To-First-Token (TTFT) and 7.35x throughput improvement in the vLLM inference engine compared to RDMA-based solutions. To the best of our knowledge, Beluga is the first system that enables GPUs to directly access large-scale memory pools through CXL switches, marking a significant step toward low-latency, shared access to vast memory resources by GPUs.

</details>
