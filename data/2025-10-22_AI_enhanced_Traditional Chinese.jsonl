{"id": "2510.17852", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17852", "abs": "https://arxiv.org/abs/2510.17852", "authors": ["Yuze Sun", "Wentao Luo", "Yanfei Xiang", "Jiancheng Pan", "Jiahao Li", "Quan Zhang", "Xiaomeng Huang"], "title": "Deploying Atmospheric and Oceanic AI Models on Chinese Hardware and Framework: Migration Strategies, Performance Optimization and Analysis", "comment": null, "summary": "With the growing role of artificial intelligence in climate and weather\nresearch, efficient model training and inference are in high demand. Current\nmodels like FourCastNet and AI-GOMS depend heavily on GPUs, limiting hardware\nindependence, especially for Chinese domestic hardware and frameworks. To\naddress this issue, we present a framework for migrating large-scale\natmospheric and oceanic models from PyTorch to MindSpore and optimizing for\nChinese chips, and evaluating their performance against GPUs. The framework\nfocuses on software-hardware adaptation, memory optimization, and parallelism.\nFurthermore, the model's performance is evaluated across multiple metrics,\nincluding training speed, inference speed, model accuracy, and energy\nefficiency, with comparisons against GPU-based implementations. Experimental\nresults demonstrate that the migration and optimization process preserves the\nmodels' original accuracy while significantly reducing system dependencies and\nimproving operational efficiency by leveraging Chinese chips as a viable\nalternative for scientific computing. This work provides valuable insights and\npractical guidance for leveraging Chinese domestic chips and frameworks in\natmospheric and oceanic AI model development, offering a pathway toward greater\ntechnological independence.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u500b\u5c07\u5927\u578b\u5927\u6c23\u8207\u6d77\u6d0b AI \u6a21\u578b\u5f9e PyTorch \u9077\u79fb\u5230 MindSpore\uff0c\u4e26\u91dd\u5c0d\u570b\u7522\u8655\u7406\u5668\u9032\u884c\u8edf\u786c\u9ad4\u9069\u914d\u3001\u8a18\u61b6\u9ad4\u512a\u5316\u8207\u4e26\u884c\u5316\u7684\u6846\u67b6\uff0c\u4e26\u8207 GPU \u5be6\u73fe\u9032\u884c\u591a\u6307\u6a19\u6bd4\u8f03\u3002", "motivation": "\u73fe\u6709\u6c23\u5019\u8207\u5929\u6c23 AI \u6a21\u578b\u9ad8\u5ea6\u4f9d\u8cf4 GPU \u8207\u5916\u570b\u751f\u614b\u7cfb\uff0c\u9650\u5236\u4e86\u5728\u570b\u5167\u786c\u9ad4\u8207\u6846\u67b6\u4e0a\u7684\u53ef\u79fb\u690d\u6027\u8207\u81ea\u4e3b\u6027\uff1b\u56e0\u6b64\u9700\u8981\u4e00\u5957\u80fd\u5728\u570b\u7522\u82af\u7247\u4e0a\u9ad8\u6548\u8a13\u7df4\u8207\u63a8\u7406\u7684\u9077\u79fb\u8207\u512a\u5316\u6d41\u7a0b\u3002", "method": "\u8a2d\u8a08\u8edf\u786c\u9ad4\u9069\u914d\u5c64\uff08\u5305\u62ec\u904b\u7b97\u6838\u66ff\u63db\u8207\u7b97\u5b50\u5be6\u73fe\uff09\u3001\u8a18\u61b6\u9ad4\u8207\u901a\u8a0a\u512a\u5316\uff08\u5167\u5b58\u4f48\u5c40\u3001checkpoint\u3001\u6df7\u5408\u7cbe\u5ea6\uff09\u3001\u4ee5\u53ca\u4e26\u884c\u5316\u7b56\u7565\uff08\u6578\u64da/\u6a21\u578b\u4e26\u884c\u8207\u5206\u4f48\u5f0f\u901a\u4fe1\u512a\u5316\uff09\uff1b\u5c07\u6a21\u578b\u5f9e PyTorch \u8f49\u8b6f\u5230 MindSpore\uff0c\u4e26\u5728\u570b\u7522\u82af\u7247\u4e0a\u9032\u884c\u5fae\u8abf\u8207\u57fa\u6e96\u6e2c\u8a66\u3002", "result": "\u5be6\u9a57\u986f\u793a\u5728\u4e0d\u964d\u4f4e\u539f\u6a21\u578b\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\uff0c\u9077\u79fb\u8207\u512a\u5316\u5f8c\u80fd\u5728\u570b\u7522\u82af\u7247\u4e0a\u9054\u5230\u53ef\u6bd4\u8f03\u7684\u8a13\u7df4/\u63a8\u7406\u6548\u7387\u4e26\u63d0\u5347\u7cfb\u7d71\u4f9d\u8cf4\u6027\u8207\u80fd\u6548\u8868\u73fe\uff0c\u8b49\u660e\u4e86\u570b\u7522\u786c\u9ad4\u4f5c\u70ba\u79d1\u5b78\u8a08\u7b97\u66ff\u4ee3\u65b9\u6848\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8a72\u6846\u67b6\u63d0\u4f9b\u4e86\u5be6\u52d9\u6307\u5f15\u8207\u7d93\u9a57\uff0c\u5e6b\u52a9\u5c07\u6c23\u8c61\u6d77\u6d0b\u9818\u57df\u7684\u5927\u578b AI \u6a21\u578b\u90e8\u7f72\u5230\u570b\u5167\u751f\u614b\u7cfb\u7d71\uff0c\u4fc3\u9032\u6280\u8853\u81ea\u4e3b\u4e26\u63d0\u5347\u904b\u884c\u6548\u7387\u3002"}}
{"id": "2510.18152", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.18152", "abs": "https://arxiv.org/abs/2510.18152", "authors": ["Zhuoyu Yao", "Yue Wang", "Songyang Zhang", "Yingshu Li", "Zhipeng Cai", "Zhi Tian"], "title": "Efficient Multi-Worker Selection based Distributed Swarm Learning via Analog Aggregation", "comment": "5 pages, 4 figures, conference", "summary": "Recent advances in distributed learning systems have introduced effective\nsolutions for implementing collaborative artificial intelligence techniques in\nwireless communication networks. Federated learning approaches provide a\nmodel-aggregation mechanism among edge devices to achieve collaborative\ntraining, while ensuring data security, communication efficiency, and sharing\ncomputational overheads. On the other hand, limited transmission resources and\ncomplex communication environments remain significant bottlenecks to the\nefficient collaborations among edge devices, particularly within large-scale\nnetworks. To address such issues, this paper proposes an over-the-air (OTA)\nanalog aggregation method designed for the distributed swarm learning (DSL),\ntermed DSL-OTA, aiming to enhance communication efficiency, enable effective\ncooperation, and ensure privacy preserving. Incorporating multi-worker\nselection strategy with over-the-air aggregation not only makes the standard\nDSL based on single best worker contributing to global model update to become\nmore federated, but also secures the aggregation from potential risks of data\nleakage. Our theoretical analyses verify the advantages of the proposed DSL-OTA\nalgorithm in terms of fast convergence rate and low communication costs.\nSimulation results reveal that our DSL-OTA outperforms the other existing\nmethods by achieving better learning performance under both homogeneous and\nheterogeneous dataset settings.", "AI": {"tldr": "\u63d0\u51fa DSL-OTA\uff1a\u5c07\u985e\u6bd4\u7a7a\u4e2d\uff08over-the-air\uff09\u805a\u5408\u6574\u5408\u5230\u5206\u6563\u5f0f\u7fa4\u9ad4\u5b78\u7fd2\uff08DSL\uff09\uff0c\u900f\u904e\u591a\u5de5\u4f5c\u8005\u9078\u64c7\u63d0\u5347\u901a\u4fe1\u6548\u7387\u3001\u5354\u4f5c\u6027\u8207\u96b1\u79c1\u4fdd\u8b77\uff0c\u4e26\u5728\u7406\u8ad6\u8207\u6a21\u64ec\u4e0a\u5c55\u793a\u66f4\u5feb\u6536\u6582\u8207\u66f4\u4f4e\u901a\u4fe1\u6210\u672c\u3002", "motivation": "Federated/Swarm learning \u5728\u908a\u7de3\u88dd\u7f6e\u5354\u540c\u8a13\u7df4\u6642\u53d7\u9650\u65bc\u50b3\u8f38\u8cc7\u6e90\u8207\u8907\u96dc\u901a\u9053\u74b0\u5883\uff1b\u73fe\u6709 DSL \u5e38\u4ef0\u8cf4\u55ae\u4e00\u6700\u4f73\u5de5\u4f5c\u8005\uff0c\u9020\u6210\u6548\u80fd\u8207\u96b1\u79c1\u98a8\u96aa\u3002\u9700\u8981\u4e00\u7a2e\u80fd\u6e1b\u5c11\u901a\u4fe1\u8ca0\u64d4\u3001\u52a0\u5f37\u5354\u4f5c\u4e26\u4fdd\u8b77\u8cc7\u6599\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa DSL-OTA\uff1a\u5229\u7528 OTA \u985e\u6bd4\u8a0a\u865f\u758a\u52a0\u505a\u6a21\u578b\u53c3\u6578\u805a\u5408\uff0c\u7d50\u5408\u591a\u5de5\u4f5c\u8005\u9078\u64c7\u7b56\u7565\u53d6\u4ee3\u55ae\u4e00\u8ca2\u737b\u8005\uff0c\u4e26\u8a2d\u8a08\u76f8\u61c9\u7684\u540c\u6b65\u3001\u529f\u7387\u63a7\u5236\u8207\u96b1\u79c1\u4fdd\u8b77\u6a5f\u5236\u3002\u7406\u8ad6\u4e0a\u63a8\u5c0e\u6536\u6582\u6027\u8207\u901a\u4fe1\u6210\u672c\u5206\u6790\u3002", "result": "\u7406\u8ad6\u8b49\u660e DSL-OTA \u5728\u6536\u6582\u901f\u5ea6\u8207\u901a\u4fe1\u6548\u7387\u4e0a\u5177\u512a\u52e2\uff1b\u6a21\u64ec\u986f\u793a\u5728\u540c\u8cea\u8207\u7570\u8cea\u8cc7\u6599\u5206\u5e03\u4e0b\uff0cDSL-OTA \u76f8\u8f03\u73fe\u6709\u65b9\u6cd5\u53ef\u9054\u5230\u66f4\u597d\u7684\u5b78\u7fd2\u6548\u80fd\u8207\u901a\u4fe1\u7bc0\u7701\u3002", "conclusion": "DSL-OTA \u80fd\u6709\u6548\u964d\u4f4e\u901a\u4fe1\u6210\u672c\u3001\u52a0\u5feb\u6a21\u578b\u6536\u6582\u4e26\u63d0\u9ad8\u5354\u4f5c\u53ca\u96b1\u79c1\u6027\uff0c\u662f\u5728\u8cc7\u6e90\u53d7\u9650\u908a\u7de3\u7db2\u8def\u4e2d\u5be6\u73fe\u7fa4\u9ad4\u5b78\u7fd2\u7684\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.18300", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18300", "abs": "https://arxiv.org/abs/2510.18300", "authors": ["Ankur Lahiry", "Ayush Pokharel", "Banooqa Banday", "Seth Ockerman", "Amal Gueroudji", "Mohammad Zaeed", "Tanzima Z. Islam", "Line Pouchard"], "title": "A Distributed Framework for Causal Modeling of Performance Variability in GPU Traces", "comment": null, "summary": "Large-scale GPU traces play a critical role in identifying performance\nbottlenecks within heterogeneous High-Performance Computing (HPC)\narchitectures. However, the sheer volume and complexity of a single trace of\ndata make performance analysis both computationally expensive and\ntime-consuming. To address this challenge, we present an end-to-end parallel\nperformance analysis framework designed to handle multiple large-scale GPU\ntraces efficiently. Our proposed framework partitions and processes trace data\nconcurrently and employs causal graph methods and parallel coordinating chart\nto expose performance variability and dependencies across execution flows.\nExperimental results demonstrate a 67% improvement in terms of scalability,\nhighlighting the effectiveness of our pipeline for analyzing multiple traces\nindependently.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u500b\u7aef\u5230\u7aef\u4e26\u884c\u6548\u80fd\u5206\u6790\u6846\u67b6\uff0c\u80fd\u540c\u6642\u5206\u5272\u8207\u8655\u7406\u591a\u500b\u5927\u578b GPU trace\uff0c\u7d50\u5408\u56e0\u679c\u5716 (causal graph) \u8207\u4e26\u884c\u5354\u8abf\u5716\u8868\u4f86\u63ed\u793a\u57f7\u884c\u6d41\u7a0b\u9593\u7684\u6548\u80fd\u8b8a\u7570\u8207\u76f8\u4f9d\u6027\uff0c\u5be6\u9a57\u986f\u793a\u5728\u53ef\u64f4\u5145\u6027\u4e0a\u63d0\u5347\u7d04 67%\u3002", "motivation": "\u55ae\u4e00\u5927\u578b GPU trace \u9ad4\u7a4d\u9f90\u5927\u4e14\u7d50\u69cb\u8907\u96dc\uff0c\u9010\u4e00\u5206\u6790\u8017\u6642\u4e14\u8a08\u7b97\u6210\u672c\u9ad8\u3002\u96a8\u8457\u9700\u8981\u5206\u6790\u591a\u7b46 trace \u7684\u60c5\u6cc1\u65e5\u76ca\u5e38\u898b\uff08\u4f8b\u5982\u5927\u898f\u6a21\u53e2\u96c6\u6216\u9577\u671f\u76e3\u63a7\uff09\uff0c\u9700\u8981\u4e00\u5957\u80fd\u5920\u4e26\u884c\u8655\u7406\u591a\u7b46 trace\u3001\u964d\u4f4e\u5ef6\u9072\u4e26\u4fdd\u7559\u4f9d\u8cf4\u8cc7\u8a0a\u7684\u6846\u67b6\u3002", "method": "\u5c0d trace \u8cc7\u6599\u9032\u884c\u5206\u5272\uff08partition\uff09\u5f8c\u4e26\u884c\u8655\u7406\uff1b\u5229\u7528\u56e0\u679c\u5716\u65b9\u6cd5\u62bd\u53d6\u4e8b\u4ef6\u9593\u7684\u76f8\u4f9d\u95dc\u4fc2\uff0c\u4e26\u4ee5\u6240\u8b02\u7684\u300c\u4e26\u884c\u5354\u8abf\u5716\u8868\u300d\uff08parallel coordinating chart\uff09\u4f86\u5448\u73fe\u8207\u6bd4\u8f03\u4e0d\u540c\u884c\u7a0b\u6216\u4e0d\u540c trace \u7684\u6027\u80fd\u5dee\u7570\u8207\u8b8a\u7570\u4f86\u6e90\u3002\u6574\u9ad4\u6d41\u7a0b\u70ba\u7aef\u5230\u7aef\u7ba1\u7dda\u5316\u4ee5\u5229\u64f4\u5145\u8207\u7368\u7acb\u5206\u6790\u591a\u7b46 trace\u3002", "result": "\u5be6\u9a57\u7d50\u679c\u6307\u51fa\u6b64\u7ba1\u7dda\u5728\u53ef\u64f4\u5145\u6027\u4e0a\u76f8\u8f03\u57fa\u6e96\u65b9\u6cd5\u63d0\u5347\u7d04 67%\uff0c\u4e26\u80fd\u7368\u7acb\u5206\u6790\u591a\u500b\u5927\u578b trace\uff0c\u63ed\u793a\u4e86\u57f7\u884c\u6d41\u7a0b\u9593\u7684\u6027\u80fd\u8b8a\u7570\u8207\u76f8\u4f9d\u6027\u3002", "conclusion": "\u6240\u63d0\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u591a\u7b46\u5927\u578b GPU trace \u7684\u5206\u6790\u53ef\u64f4\u5145\u6027\u4e26\u63d0\u4f9b\u56e0\u679c/\u5354\u8abf\u8996\u89d2\u4f86\u7406\u89e3\u6027\u80fd\u74f6\u9838\uff0c\u70ba\u5927\u898f\u6a21\u7570\u8cea HPC \u5e73\u53f0\u7684\u6548\u80fd\u5075\u932f\u8207\u512a\u5316\u63d0\u4f9b\u5be6\u7528\u5de5\u5177\u3002"}}
{"id": "2510.18544", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.18544", "abs": "https://arxiv.org/abs/2510.18544", "authors": ["Pan Zhou", "Yiming Lei", "Ling Liu", "Xiaoqiong Xu", "Ying Cai", "Daji Ergu", "Hongfang Yu", "Yueyue Dai"], "title": "SLICE: SLO-Driven Scheduling for LLM Inference on Edge Computing Devices", "comment": null, "summary": "Large Language Models (LLMs), as the foundational architecture for\nnext-generation interactive AI applications, not only power intelligent\ndialogue systems but also drive the evolution of embodied intelligence on edge\ndevices, including humanoid robots, smart vehicles, and other scenarios. The\napplications running on these edge devices impose differentiated Service Level\nObjectives (SLO) requirements on LLM services, specifically manifested as\ndistinct constraints on Time to First Token (TTFT) and Time Per Output Token\n(TPOT) as well as end-to-end latency. Notably, edge devices typically handle\nreal-time tasks that are extremely sensitive to latency, such as machine\ncontrol and navigation planning. However, existing scheduling service systems\nstill prioritize maximizing output token throughput as the sole optimization\nobjective, failing to adequately address the diversity of SLO requirements.\nThis ultimately results in persistently high violation rates for end-to-end\nlatency or TPOT related SLOs.\n  This paper proposes SLICE, an innovative scheduling solution designed for\nedge computing scenarios with differentiated SLO requirements. By combining a\nutility-maximizing request scheduling algorithm with a dynamic iterative\ncontrol mechanism for generation rates, SLICE significantly improves LLM\ninference service SLO attainment. Experimental results demonstrate that\ncompared to state-of-the-art solutions Orca and FastServe, SLICE achieves up to\n35x higher SLO attainment and 3.4x advantage in task completion time than the\nother two solutions.", "AI": {"tldr": "\u63d0\u51fa SLICE\uff0c\u4e00\u7a2e\u91dd\u5c0d\u908a\u7de3\u8a2d\u5099\u5dee\u7570\u5316 SLO\uff08TTFT\u3001TPOT\u3001\u7aef\u5230\u7aef\u5ef6\u9072\uff09\u8a2d\u8a08\u7684\u6392\u7a0b\u8207\u751f\u6210\u901f\u7387\u52d5\u614b\u63a7\u5236\u65b9\u6848\uff0c\u53ef\u5927\u5e45\u63d0\u5347 LLM \u63a8\u7406\u7684 SLO \u9054\u6210\u7387\u3002", "motivation": "\u908a\u7de3\u8a2d\u5099\u4e0a\u7684\u61c9\u7528\uff08\u5982\u6a5f\u5668\u4eba\u3001\u8eca\u8f1b\uff09\u6709\u56b4\u82db\u4e14\u591a\u6a23\u7684\u5ef6\u9072\u9700\u6c42\uff08TTFT\u3001TPOT\u3001\u7aef\u5230\u7aef\u5ef6\u9072\uff09\uff0c\u800c\u73fe\u6709\u6392\u7a0b\u7cfb\u7d71\u50c5\u4ee5\u6700\u5927\u5316\u8f38\u51fa token \u541e\u5410\u91cf\u70ba\u76ee\u6a19\uff0c\u5c0e\u81f4\u5c0d\u5ef6\u9072\u654f\u611f\u4efb\u52d9\u7684 SLO \u9055\u898f\u7387\u9ad8\u3002", "method": "\u63d0\u51fa SLICE\uff1a\u7d50\u5408\u4e00\u500b\u4ee5\u6548\u7528\u6700\u5927\u5316\u70ba\u76ee\u6a19\u7684\u8acb\u6c42\u6392\u7a0b\u6f14\u7b97\u6cd5\u8207\u4e00\u500b\u52d5\u614b\u8fed\u4ee3\u7684\u751f\u6210\u901f\u7387\u63a7\u5236\u6a5f\u5236\uff0c\u5c07\u4e0d\u540c SLO \u9700\u6c42\u7d0d\u5165\u6392\u7a0b\u8207\u901f\u7387\u8abf\u7bc0\uff0c\u5be6\u73fe\u5c0d\u751f\u6210\u904e\u7a0b\u7684\u7d30\u7c92\u5ea6\u63a7\u5236\u8207\u512a\u5148\u6b0a\u5206\u914d\u3002", "result": "\u5be6\u9a57\u986f\u793a\uff0c\u8207 Orca \u548c FastServe \u76f8\u6bd4\uff0cSLICE \u5728 SLO \u9054\u6210\u7387\u4e0a\u6700\u9ad8\u53ef\u9054 35\u00d7 \u512a\u52e2\uff0c\u4efb\u52d9\u5b8c\u7562\u6642\u9593\u4e0a\u7d04 3.4\u00d7 \u6539\u5584\uff0c\u5c0d\u5ef6\u9072\u654f\u611f\u5de5\u4f5c\u8ca0\u8f09\u6709\u660e\u986f\u63d0\u5347\u3002", "conclusion": "SLICE \u6709\u6548\u89e3\u6c7a\u908a\u7de3 LLM \u670d\u52d9\u7684\u5dee\u7570\u5316 SLO \u554f\u984c\uff0c\u900f\u904e\u6392\u7a0b\u8207\u751f\u6210\u901f\u7387\u5354\u540c\u964d\u4f4e\u5ef6\u9072\u9055\u898f\uff1b\u5f8c\u7e8c\u53ef\u63a2\u8a0e\u591a\u6a21\u578b\u3001\u591a\u88dd\u7f6e\u5354\u8abf\u3001\u80fd\u8017\u8207\u7cfb\u7d71\u958b\u92b7\u7b49\u5ef6\u4f38\u8b70\u984c\u3002"}}
{"id": "2510.18058", "categories": ["cs.NI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.18058", "abs": "https://arxiv.org/abs/2510.18058", "authors": ["Hongbo Lu", "Junsung Hwang", "Bernard Tenreiro", "Nabila Jaman Tripti", "Darren Hamilton", "Yuefan Deng"], "title": "A New Broadcast Model for Several Network Topologies", "comment": "19 pages, 11 figures", "summary": "We present Broadcast by Balanced Saturation (BBS), a general broadcast\nalgorithm designed to optimize communication efficiency across diverse network\ntopologies. BBS maximizes node utilization, addressing challenges in broadcast\noperations such as topology constraints, bandwidth limitations, and\nsynchronization overhead, particularly in large-scale systems like\nsupercomputers. The algorithm ensures sustained activity with nodes throughout\nthe broadcast, thereby enhancing data propagation and significantly reducing\nlatency. Through a precise communication cycle, BBS provides a repeatable,\nstreamlined, stepwise broadcasting framework. Simulation results across various\ntopologies demonstrate that the BBS algorithm consistently outperforms common\ngeneral broadcast algorithms, often by a substantial margin. These findings\nsuggest that BBS is a versatile and robust framework with the potential to\nredefine broadcast strategies across network topologies.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u7a2e\u540d\u70ba Broadcast by Balanced Saturation (BBS) \u7684\u5ee3\u64ad\u6f14\u7b97\u6cd5\uff0c\u900f\u904e\u5e73\u8861\u98fd\u548c\uff08balanced saturation\uff09\u8207\u7cbe\u78ba\u901a\u8a0a\u9031\u671f\u7dad\u6301\u7bc0\u9ede\u6d3b\u8e8d\u5ea6\uff0c\u65e8\u5728\u63d0\u9ad8\u5404\u7a2e\u7db2\u8def\u62d3\u64b2\u4e0b\u7684\u901a\u8a0a\u6548\u7387\u8207\u964d\u4f4e\u5ef6\u9072\uff0c\u6a21\u64ec\u7d50\u679c\u986f\u793a\u5728\u591a\u6578\u62d3\u64b2\u4e0a\u512a\u65bc\u4e00\u822c\u5ee3\u64ad\u6f14\u7b97\u6cd5\u3002", "motivation": "\u5728\u5927\u898f\u6a21\u7cfb\u7d71\uff08\u5982\u8d85\u7d1a\u96fb\u8166\uff09\u4e2d\uff0c\u5ee3\u64ad\u9762\u81e8\u62d3\u64b2\u9650\u5236\u3001\u983b\u5bec\u74f6\u9838\u8207\u540c\u6b65\u958b\u92b7\u3002\u76ee\u6a19\u662f\u6700\u5927\u5316\u7bc0\u9ede\u5229\u7528\u7387\u4e26\u6301\u7e8c\u4fdd\u6301\u7bc0\u9ede\u6d3b\u52d5\u4ee5\u52a0\u901f\u8cc7\u6599\u50b3\u64ad\uff0c\u5f9e\u800c\u964d\u4f4e\u6574\u9ad4\u5ef6\u9072\u3002", "method": "\u8a2d\u8a08\u4e00\u500b\u53ef\u91cd\u8907\u3001\u6b65\u9a5f\u5316\u7684\u901a\u8a0a\u9031\u671f\uff08precise communication cycle\uff09\uff0c\u4ee5\u5e73\u8861\u98fd\u548c\u7b56\u7565\u5b89\u6392\u7bc0\u9ede\u767c\u9001\u8207\u63a5\u6536\uff0c\u78ba\u4fdd\u6bcf\u500b\u901a\u8a0a\u5faa\u74b0\u4e2d\u7bc0\u9ede\u4fdd\u6301\u9ad8\u5229\u7528\u7387\u4e26\u6709\u6548\u5206\u914d\u983b\u5bec\u8207\u50b3\u64ad\u8def\u5f91\u3002\u900f\u904e\u6a21\u64ec\u5728\u591a\u7a2e\u62d3\u64b2\u4e0a\u9a57\u8b49\u6548\u80fd\u3002", "result": "\u6a21\u64ec\u7d50\u679c\u6307\u51fa BBS \u5728\u4e0d\u540c\u62d3\u64b2\u4e0b\u901a\u5e38\u986f\u8457\u512a\u65bc\u5e38\u898b\u7684\u4e00\u822c\u5ee3\u64ad\u6f14\u7b97\u6cd5\uff08\u5ef6\u9072\u964d\u4f4e\u3001\u7bc0\u9ede\u5229\u7528\u5ea6\u63d0\u5347\uff09\uff0c\u5728\u591a\u500b\u6848\u4f8b\u4e2d\u6709\u5927\u5e45\u6539\u9032\u3002", "conclusion": "BBS \u63d0\u4f9b\u4e00\u500b\u901a\u7528\u4e14\u5177\u97cc\u6027\u7684\u5ee3\u64ad\u6846\u67b6\uff0c\u80fd\u5728\u591a\u6a23\u62d3\u64b2\u4e2d\u63d0\u5347\u5ee3\u64ad\u6548\u80fd\uff0c\u5177\u6709\u53d6\u4ee3\u6216\u91cd\u5851\u65e2\u6709\u5ee3\u64ad\u7b56\u7565\u7684\u6f5b\u529b\u3002"}}
{"id": "2510.18417", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18417", "abs": "https://arxiv.org/abs/2510.18417", "authors": ["Rahul Soundrarajan", "Claudio Fiandrino", "Michele Polese", "Salvatore D'Oro", "Leonardo Bonati", "Tommaso Melodia"], "title": "On AI Verification in Open RAN", "comment": null, "summary": "Open RAN introduces a flexible, cloud-based architecture for the Radio Access\nNetwork (RAN), enabling Artificial Intelligence (AI)/Machine Learning\n(ML)-driven automation across heterogeneous, multi-vendor deployments. While\nEXplainable Artificial Intelligence (XAI) helps mitigate the opacity of AI\nmodels, explainability alone does not guarantee reliable network operations. In\nthis article, we propose a lightweight verification approach based on\ninterpretable models to validate the behavior of Deep Reinforcement Learning\n(DRL) agents for RAN slicing and scheduling in Open RAN. Specifically, we use\nDecision Tree (DT)-based verifiers to perform near-real-time consistency checks\nat runtime, which would be otherwise unfeasible with computationally expensive\nstate-of-the-art verifiers. We analyze the landscape of XAI and AI\nverification, propose a scalable architectural integration, and demonstrate\nfeasibility with a DT-based slice-verifier. We also outline future challenges\nto ensure trustworthy AI adoption in Open RAN.", "AI": {"tldr": "Lightweight DT-based verifiers provide near-real-time consistency checks for DRL agents in Open RAN slicing/scheduling, trading off some verification completeness for runtime feasibility.", "motivation": "Open RAN enables AI-driven control (DRL) across heterogeneous, multi-vendor RAN; however, opaque AI decisions threaten reliability of network operations and require verification beyond XAI explanations.", "method": "Use interpretable Decision Tree models as runtime verifiers to validate DRL agent actions for slicing and scheduling. Integrate DT-verifier into an Open RAN scalable architecture to run near-real-time consistency checks; compare with computationally expensive state-of-the-art verifiers and demonstrate feasibility with a DT-based slice-verifier.", "result": "Feasibility demonstrated: DT-based verifiers can perform near-real-time checks that are otherwise infeasible with heavy verifiers. The paper provides architectural integration and a working slice-verifier prototype/evaluation.", "conclusion": "DT-based, interpretable verifiers are a practical step toward trustworthy AI in Open RAN, but future challenges remain (scalability, robustness, completeness of verification, maintenance under non-stationarity)."}}
