{"id": "2510.20111", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20111", "abs": "https://arxiv.org/abs/2510.20111", "authors": ["Huawei Bai", "Yifan Huang", "Wenqi Shi", "Ansheng You", "Feifan Shao", "Tengfei Han", "Minghui Yu"], "title": "AsyncHZP: Hierarchical ZeRO Parallelism with Asynchronous Scheduling for Scalable LLM Training", "comment": "14 pages, 5 figures, tech report", "summary": "The training efficiency and scalability of language models on massive\nclusters currently remain a critical bottleneck. Mainstream approaches like ND\nparallelism are often cumbersome and complex, while flexible alternatives such\nas the Zero Redundancy Optimizer (ZeRO) are frequently hampered by\ncommunication overhead. In this paper, we propose Asynchronous Hierarchical\nZero Parallelism (AsyncHZP), a novel asynchronous variant of ZeRO designed to\nachieve superior performance while maintaining simplicity and memory\nefficiency. Unlike traditional ZeRO, which employs over-fine-grained sharding\nthat can lead to inefficient communication, AsyncHZP adaptively reshards\nparameters, gradients, and optimizer states across different replica groups.\nThis strategy optimizes device memory utilization and significantly reduces\ncommunication overhead. In addition, we also design a multi-stream asynchronous\nscheduling method that executes parameter all-gather and gradient\nreduce-scatter operations in dedicated background threads, effectively\noverlapping communication with computation while incurring negligible memory\nfragmentation. Empirical evaluations on both Dense and Mixture-of-Experts (MoE)\nmodels confirm that AsyncHZP maintains robust stability at scale. It\nconsistently outperforms classic ND parallelism, achieving state-of-the-art\nperformance without complex strategic tuning, thereby simplifying the path to\nefficient large-scale training.", "AI": {"tldr": "AsyncHZP is an asynchronous, hierarchical variant of ZeRO that adaptively reshards states across replica groups and uses multi-stream background communication to reduce overhead and overlap communication with computation, improving large-scale training performance and scalability for Dense and MoE models.", "motivation": "Large-scale LM training suffers from communication and memory bottlenecks: fine-grained ZeRO sharding causes inefficient communication; ND parallelism is complex to manage. There is a need for a simple, memory-efficient method that reduces communication and scales well.", "method": "Introduce Asynchronous Hierarchical Zero Parallelism (AsyncHZP): 1) adaptive resharing of parameters, gradients, and optimizer states across different replica groups (hierarchical sharding) to optimize device memory utilization and lower communication; 2) multi-stream asynchronous scheduling, running parameter all-gather and gradient reduce-scatter in dedicated background threads to overlap communication with compute with minimal memory fragmentation.", "result": "Empirical results on Dense and Mixture-of-Experts models show robust stability at scale and consistent performance gains over classic ND parallelism. AsyncHZP achieves state-of-the-art wall-clock performance without complex tuning.", "conclusion": "AsyncHZP simplifies large-scale model training by reducing communication overhead and improving memory utilization through adaptive hierarchical sharding and async background communication, making it a practical, high-performance alternative to existing parallelism schemes."}}
{"id": "2510.20171", "categories": ["cs.DC", "cs.AI", "cs.NI", "C.2.4; I.2"], "pdf": "https://arxiv.org/pdf/2510.20171", "abs": "https://arxiv.org/abs/2510.20171", "authors": ["Min Si", "Pavan Balaji", "Yongzhou Chen", "Ching-Hsiang Chu", "Adi Gangidi", "Saif Hasan", "Subodh Iyengar", "Dan Johnson", "Bingzhe Liu", "Jingliang Ren", "Ashmitha Jeevaraj Shetty", "Greg Steinbrecher", "Xinfeng Xie", "Yulun Wang", "Bruce Wu", "Jingyi Yang", "Mingran Yang", "Minlan Yu", "Cen Zhao", "Wes Bland", "Denis Boyda", "Suman Gumudavelli", "Cristian Lumezanu", "Rui Miao", "Zhe Qu", "Venkat Ramesh", "Maxim Samoylov", "Jan Seidel", "Feng Tian", "Qiye Tan", "Shuqiang Zhang", "Yimeng Zhao", "Shengbao Zheng", "Art Zhu", "Hongyi Zeng"], "title": "Collective Communication for 100k+ GPUs", "comment": null, "summary": "The increasing scale of large language models (LLMs) necessitates highly\nefficient collective communication frameworks, particularly as training\nworkloads extend to hundreds of thousands of GPUs. Traditional communication\nmethods face significant throughput and latency limitations at this scale,\nhindering both the development and deployment of state-of-the-art models. This\npaper presents the NCCLX collective communication framework, developed at Meta,\nengineered to optimize performance across the full LLM lifecycle, from the\nsynchronous demands of large-scale training to the low-latency requirements of\ninference. The framework is designed to support complex workloads on clusters\nexceeding 100,000 GPUs, ensuring reliable, high-throughput, and low-latency\ndata exchange. Empirical evaluation on the Llama4 model demonstrates\nsubstantial improvements in communication efficiency. This research contributes\na robust solution for enabling the next generation of LLMs to operate at\nunprecedented scales.", "AI": {"tldr": "Presents NCCLX, a collective communication framework from Meta that targets high-throughput and low-latency data exchange across clusters >100k GPUs, evaluated on Llama4 with reported large gains in communication efficiency.", "motivation": "Training and serving very large LLMs demand communication frameworks that scale beyond existing solutions; current methods struggle with throughput and latency when workloads span tens to hundreds of thousands of GPUs.", "method": "Design and engineering of the NCCLX framework to optimize collective communication for both synchronous large-scale training and low-latency inference across massive clusters, emphasizing reliability and efficiency.", "result": "Empirical results on Llama4 show substantial improvements in communication efficiency, suggesting higher throughput and lower latency compared to previous approaches (details not provided in abstract).", "conclusion": "NCCLX is positioned as a robust, scalable communication solution enabling next-generation LLMs to operate at unprecedented scale."}}
{"id": "2510.19973", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19973", "abs": "https://arxiv.org/abs/2510.19973", "authors": ["Hatim Chergui", "Farhad Rezazadeh", "Merouane Debbah", "Christos Verikoukis"], "title": "A Tutorial on Cognitive Biases in Agentic AI-Driven 6G Autonomous Networks", "comment": "19 pages, 15 figures, 1 table", "summary": "The path to higher network autonomy in 6G lies beyond the mere optimization\nof key performance indicators (KPIs). While KPIs have enabled automation gains\nunder TM Forum Levels 1--3, they remain numerical abstractions that act only as\nproxies for the real essence of communication networks: seamless connectivity,\nfairness, adaptability, and resilience. True autonomy requires perceiving and\nreasoning over the network environment as it is. Such progress can be achieved\nthrough \\emph{agentic AI}, where large language model (LLM)-powered agents\nperceive multimodal telemetry, reason with memory, negotiate across domains,\nand act via APIs to achieve multi-objective goals. However, deploying such\nagents introduces the challenge of cognitive biases inherited from human\ndesign, which can distort reasoning, negotiation, tool use, and actuation.\nBetween neuroscience and AI, this paper provides a tutorial on a selection of\nwell-known biases, including their taxonomy, definition, mathematical\nformulation, emergence in telecom systems and the commonly impacted agentic\ncomponents. The tutorial also presents various mitigation strategies tailored\nto each type of bias. The article finally provides two practical use-cases,\nwhich tackle the emergence, impact and mitigation gain of some famous biases in\n6G inter-slice and cross-domain management. In particular, anchor\nrandomization, temporal decay and inflection bonus techniques are introduced to\nspecifically address anchoring, temporal and confirmation biases. This avoids\nthat agents stick to the initial high resource allocation proposal or decisions\nthat are recent and/or confirming a prior hypothesis. By grounding decisions in\na richer and fairer set of past experiences, the quality and bravery of the\nagentic agreements in the second use-case, for instance, are leading to $\\times\n5$ lower latency and around $40\\%$ higher energy saving.", "AI": {"tldr": "This paper argues that 6G network autonomy should move beyond KPI optimization toward LLM-powered agentic AI that perceives multimodal telemetry, reasons with memory, negotiates across domains, and acts via APIs. It catalogs cognitive biases that can impair such agents, gives mathematical formulations and telecom-specific emergence points, and proposes tailored mitigations. Two applied use-cases (inter-slice and cross-domain management) demonstrate techniques\u2014anchor randomization, temporal decay, inflection bonus\u2014yielding up to 5\u00d7 lower latency and \u224840% higher energy savings.", "motivation": "KPIs are insufficient proxies for the true goals of next-gen networks (seamless connectivity, fairness, adaptability, resilience). Agentic LLMs can deliver true autonomy but inherit cognitive biases that distort reasoning, negotiation and actuation in telecom contexts.", "method": "Provide a tutorial-style taxonomy and formal definitions of common cognitive biases; map how each bias emerges in telecom agentic components (perception, memory, negotiation, tool use, actuation); propose mitigation strategies per bias. Validate with two practical use-cases applying specific mitigation techniques (anchor randomization, temporal decay, inflection bonus) and measure impact on latency and energy.", "result": "Mitigations reduce bias effects in test scenarios: anchored/recency/confirmation biases were mitigated, leading to significantly better negotiation outcomes and resource allocations. Reported gains include 5\u00d7 lower latency and ~40% energy savings in the second use-case.", "conclusion": "Agentic AI is a promising path to 6G autonomy but must confront cognitive biases. The paper offers a taxonomy, mathematical treatments, telecom mappings, mitigation techniques and empirical evidence that targeted debiasing substantially improves performance and robustness."}}
{"id": "2510.20495", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.20495", "abs": "https://arxiv.org/abs/2510.20495", "authors": ["Panagiotis Giannakopoulos", "Bart van Knippenberg", "Kishor Chandra Joshi", "Nicola Calabretta", "George Exarchakos"], "title": "Accurate Performance Predictors for Edge Computing Applications", "comment": null, "summary": "Accurate prediction of application performance is critical for enabling\neffective scheduling and resource management in resource-constrained dynamic\nedge environments. However, achieving predictable performance in such\nenvironments remains challenging due to the co-location of multiple\napplications and the node heterogeneity. To address this, we propose a\nmethodology that automatically builds and assesses various performance\npredictors. This approach prioritizes both accuracy and inference time to\nidentify the most efficient model. Our predictors achieve up to 90% accuracy\nwhile maintaining an inference time of less than 1% of the Round Trip Time.\nThese predictors are trained on the historical state of the most correlated\nmonitoring metrics to application performance and evaluated across multiple\nservers in dynamic co-location scenarios. As usecase we consider electron\nmicroscopy (EM) workflows, which have stringent real-time demands and diverse\nresource requirements. Our findings emphasize the need for a systematic\nmethodology that selects server-specific predictors by jointly optimizing\naccuracy and inference latency in dynamic co-location scenarios. Integrating\nsuch predictors into edge environments can improve resource utilization and\nresult in predictable performance.", "AI": {"tldr": "Automated methodology builds and evaluates server-specific performance predictors for edge environments (trained on correlated monitoring metrics) optimizing both accuracy and inference latency; achieves up to 90% accuracy and inference time <1% of RTT, validated on electron microscopy workflows under dynamic co-location.", "motivation": "Achieve predictable application performance in resource-constrained, heterogeneous, co-located edge environments to enable effective scheduling and resource management\u2014particularly for real-time, resource-diverse workloads like electron microscopy workflows.", "method": "Automatically construct and assess multiple predictor models using historical monitoring metrics that are most correlated with application performance. Jointly optimize for prediction accuracy and inference latency to select server-specific models. Evaluate predictors across multiple servers and dynamic co-location scenarios, measuring accuracy and inference time relative to Round Trip Time (RTT).", "result": "Predictors reach up to 90% accuracy while keeping inference latency below 1% of RTT. Models trained on top-correlated metrics generalize across dynamic co-location scenarios and improve predictability for EM workflows, enabling better resource utilization.", "conclusion": "Selecting server-specific predictors by jointly optimizing accuracy and inference latency is necessary in dynamic co-location edge settings. Integrating such predictors into scheduling/resource management can yield predictable performance. Future directions include online adaptation, broader workload evaluation, and addressing model-selection overhead."}}
