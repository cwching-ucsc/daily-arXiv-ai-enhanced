<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 2]
- [cs.DC](#cs.DC) [Total: 10]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Pinching Antennas Meet AI in Next-Generation Wireless Networks](https://arxiv.org/abs/2511.07442)
*Fang Fang,Zhiguo Ding,Victor C. M. Leung,Lajos Hanzo*

Main category: cs.NI

TL;DR: Paper proposes integrating AI with pinching antennas (PAs) to create adaptive, low-cost line-of-sight links and to enable edge-AI services (federated learning, over‑the‑air aggregation). AI optimizes PA activation and resource allocation; PAs in turn support edge-AI tasks. It highlights future directions like LLM-driven PA control, semantic communications, and integrated sensing and communication.


<details>
  <summary>Details</summary>
Motivation: Next‑generation wireless networks require low-latency, ultra‑reliable, and adaptive links to support applications such as extended reality and autonomous systems. Pinching antennas offer flexible, low‑cost, on‑demand line‑of‑sight creation but need intelligent control. AI can manage the complex, dynamic control and resource allocation problems.

Method: Conceptual framework: use AI to adaptively optimize PA activation positions along a waveguide and perform resource allocation; leverage PAs to support edge AI workflows such as federated learning and over‑the‑air aggregation. Discusses possible architectures (e.g., LLM-driven control) and synergies with semantic communications and ISAC.

Result: No experimental results presented—this is a position/survey-style article. The claimed results are conceptual: AI+PA cooperation can enable adaptive, resilient, and self‑optimizing NG networks and support edge AI tasks efficiently.

Conclusion: Synergistic integration of AI and pinching antennas promises a ‘‘win‑win’’: AI improves PA operation and network optimization, while PAs provide physical-layer support for edge AI and next‑gen services. The article outlines research avenues for realizing this vision.

Abstract: Next-generation (NG) wireless networks must embrace innate intelligence in support of demanding emerging applications, such as extended reality and autonomous systems, under ultra-reliable and low-latency requirements. Pinching antennas (PAs), a new flexible low-cost technology, can create line-of-sight links by dynamically activating small dielectric pinches along a waveguide on demand. As a compelling complement, artificial intelligence (AI) offers the intelligence needed to manage the complex control of PA activation positions and resource allocation in these dynamic environments. This article explores the "win-win" cooperation between AI and PAs: AI facilitates the adaptive optimization of PA activation positions along the waveguide, while PAs support edge AI tasks such as federated learning and over-the-air aggregation. We also discuss promising research directions including large language model-driven PA control frameworks, and how PA-AI integration can advance semantic communications, and integrated sensing and communication. This synergy paves the way for adaptive, resilient, and self-optimizing NG networks.

</details>


### [2] [SRE-Llama -- Fine-Tuned Meta's Llama LLM, Federated Learning, Blockchain and NFT Enabled Site Reliability Engineering(SRE) Platform for Communication and Networking Software Services](https://arxiv.org/abs/2511.08282)
*Eranga Bandara,Safdar H. Bouk,Sachin Shetty,Ravi Mukkamala,Abdul Rahman,Peter Foytik,Ross Gore,Xueping Liang,Ng Wee Keong,Kasun De Zoysa*

Main category: cs.NI

TL;DR: The paper proposes SRE-Llama, an automated SRE platform that captures cloud-native metrics, uses Federated Learning to select SLIs, fine-tuned Llama-3 to generate SLIs/SLOs/error budgets/alerts, and records SLI/SLO artifacts as NFTs on a blockchain with smart-contract-driven automation; a prototype with Open5GS 5G Core is implemented.


<details>
  <summary>Details</summary>
Motivation: Developers often lack deep expertise with monitoring tools (Prometheus/Grafana) and in defining appropriate SLIs/SLOs; the platform aims to automate generation and management of SRE artifacts while preserving data privacy and providing immutable audit trails.

Method: Collect time-series metrics from services (Prometheus/Mimir), apply Federated Learning to identify relevant SLI metrics while keeping data private, feed results to a fine-tuned Meta Llama-3 LLM to generate SLIs/SLOs/error budgets and alerts, encode these artifacts as NFTs and store them on a blockchain, and orchestrate automation with smart contracts; prototype implemented on an Open5GS 5G Core.

Result: A working prototype (SRE-Llama) demonstrates the feasibility of end-to-end automation from metric acquisition to SLO generation and on-chain storage; the abstract does not provide quantitative evaluation metrics in the text.

Conclusion: SRE-Llama is a novel, privacy-aware, and audit-friendly approach to automating SRE tasks using Federated Learning, LLMs, and blockchain/NFTs; further work should validate effectiveness, cost, scalability, and governance in production environments.

Abstract: Software services are crucial for reliable communication and networking; therefore, Site Reliability Engineering (SRE) is important to ensure these systems stay reliable and perform well in cloud-native environments. SRE leverages tools like Prometheus and Grafana to monitor system metrics, defining critical Service Level Indicators (SLIs) and Service Level Objectives (SLOs) for maintaining high service standards. However, a significant challenge arises as many developers often lack in-depth understanding of these tools and the intricacies involved in defining appropriate SLIs and SLOs. To bridge this gap, we propose a novel SRE platform, called SRE-Llama, enhanced by Generative-AI, Federated Learning, Blockchain, and Non-Fungible Tokens (NFTs). This platform aims to automate and simplify the process of monitoring, SLI/SLO generation, and alert management, offering ease in accessibility and efficy for developers. The system operates by capturing metrics from cloud-native services and storing them in a time-series database, like Prometheus and Mimir. Utilizing this stored data, our platform employs Federated Learning models to identify the most relevant and impactful SLI metrics for different services and SLOs, addressing concerns around data privacy. Subsequently, fine-tuned Meta's Llama-3 LLM is adopted to intelligently generate SLIs, SLOs, error budgets, and associated alerting mechanisms based on these identified SLI metrics. A unique aspect of our platform is the encoding of generated SLIs and SLOs as NFT objects, which are then stored on a Blockchain. This feature provides immutable record-keeping and facilitates easy verification and auditing of the SRE metrics and objectives. The automation of the proposed platform is governed by the blockchain smart contracts. The proposed SRE-Llama platform prototype has been implemented with a use case featuring a customized Open5GS 5G Core.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [From Attention to Disaggregation: Tracing the Evolution of LLM Inference](https://arxiv.org/abs/2511.07422)
*Madabattula Rajesh Kumar,Srinivasa Rao Aravilli,Mustafa Saify,Shashank Srivastava*

Main category: cs.DC

TL;DR: Proposes shifting LLM serving from monolithic GPU clusters to a disaggregated inference architecture that separates compute-heavy prefill from memory-heavy decode, enabling independent scaling and optimization to reduce Time to First Token and Inter Token Latency while improving throughput and cost-efficiency.


<details>
  <summary>Details</summary>
Motivation: As LLMs scale to trillions of parameters, training is no longer the bottleneck; real-time inference introduces new constraints (memory bandwidth, compute throughput, latency). Current monolithic GPU deployments suffer resource contention that prevents optimal tuning of latency, throughput, and cost simultaneously.

Method: Apply distributed systems principles—service decomposition, resource disaggregation, workload partitioning—to split inference into independently scalable components. Specifically, decouple the prefill phase (compute-bound) from the decode phase (memory-bound) so each can be optimized on appropriate hardware and scaled independently.

Result: The disaggregated approach mitigates resource contention seen in unified GPU clusters and allows separate optimization of Time to First Token and Inter Token Latency. It enables higher throughput, better cost-performance, and more predictable SLAs by tailoring resources to phase-specific bottlenecks.

Conclusion: A paradigm shift to disaggregated inference is necessary for practical deployment of very large LLMs. Decoupling phases and applying disaggregation principles yields measurable benefits in latency and throughput, though it introduces new challenges (network overhead, partitioning complexity) that require careful system design.

Abstract: The evolution of Large Language Models from the Transformer architecture to models with trillions of parameters has shifted the primary bottleneck from model training to real time inference. Deploying these massive models is a complex distributed systems challenge constrained by memory bandwidth, computational throughput, and latency requirements. LLM inference fundamentally requires solving a multi objective optimization problem to minimize latency, maximize throughput, and reduce cost. This paper explores the necessary architectural shift towards disaggregated inference, which applies distributed systems principles such as service decomposition, resource disaggregation, and workload partitioning to overcome the limitations of traditional monolithic GPU clusters. By decoupling the compute intensive prefill phase from the memory intensive decode phase into independently scalable components, this paradigm mitigates resource contention and enables independent optimization of key metrics like Time to First Token and Inter Token Latency.

</details>


### [4] [Synera: Synergistic LLM Serving across Device and Cloud at Scale](https://arxiv.org/abs/2511.07423)
*Genglin Wang,Liekang Zeng,Bufang Yang,Kaiwei Liu,Guoliang Xing,Chumin Sun,Li Zhou,Jie Sun,Zhenyu Yan*

Main category: cs.DC

TL;DR: Synera is a device-cloud synergistic LLM serving system that combines a small on-device model (SLM) with cloud LLMs via selective offloading, stall-free parallel inference, and scalable batching to improve generation quality and latency. Evaluations show 1.20–5.47x better quality versus baselines with similar latency and 8.2–16.5% lower cloud cost.


<details>
  <summary>Details</summary>
Motivation: Mobile OSes increasingly embed LLMs for smart apps, but cloud offloading suffers communication bottlenecks and on-device SLMs sacrifice quality. A hybrid device-cloud approach can potentially achieve better quality-latency-cost trade-offs.

Method: Synera analyzes LLM computing characteristics to identify optimization targets (offloading decisions, pipeline stalls, batching). It implements communication-efficient selective offloading, stall-free parallel inference between device and cloud, and scalable cloud batching to avoid pipeline and batching bottlenecks.

Result: On real-world testbeds, Synera improves generation quality by 1.20–5.47x over competitive baselines at comparable latency, and reduces cloud serving cost by 8.2–16.5% compared to pure cloud serving across benchmarks.

Conclusion: A carefully designed SLM-LLM synergy across device and cloud can significantly improve generation quality and cost-efficiency without sacrificing latency, making hybrid serving viable for mobile LLM applications.

Abstract: Large Language Models (LLMs) are becoming key components in various mobile operating systems, driving smart applications like interactive chatbots and personal assistants. While bringing enhanced intelligence to mobile ends, their deployment suffers from a set of performance challenges, especially the generation quality degradation and prolonged latency. Prior works have mainly relied on solutions of cloud offloading or on-device Small Language Models (SLMs). However, the former is usually limited by the communication bottleneck, and the latter sacrifices generation quality due to resource constraints. To mitigate these limitations, this paper proposes Synera, a device-cloud synergistic LLM serving system that applies an efficient SLM-LLM synergistic mechanism. Through empirical studies on LLM's unique computing characteristics, Synera identifies a set of underexplored optimization opportunities in device-cloud synergistic LLM inference, including offloading decisions, pipeline stalls, and batching bottlenecks. To translate them into enhanced performance, Synera introduces tailored designs of communication-efficient selective offloading, stall-free parallel inference, and scalable cloud batching. Extensive evaluations with real-world testbeds show that Synera enables 1.20-5.47x better generation quality against competitive baselines with on-par latency performance. Compared with existing cloud serving, Synera achieves 8.2-16.5% lower cloud serving cost on various benchmarks.

</details>


### [5] [Enhancing reliability in AI inference services: An empirical study on real production incidents](https://arxiv.org/abs/2511.07424)
*Bhala Ranganathan,Mickey Zhang,Kai Wu*

Main category: cs.DC

TL;DR: Provider-internal, practice-based analysis of LLM inference incidents using a validated taxonomy on 156 high-severity incidents and a focused Apr–Jun 2025 study; finds inference engine failures (esp. timeouts) dominant, many incidents auto-detected or mitigated by routing/rebalancing, and proposes mitigations plus an adoption checklist.


<details>
  <summary>Details</summary>
Motivation: Hyperscale LLM inference has high operational risk where brief failures have large user/business impact; the paper seeks to understand root causes and mitigation opportunities through real-world incident analysis to improve reliability and cost-efficiency.

Method: Developed a taxonomy and labeling methodology from one year of operational experience; validated labeling (Cohen's K ≈ 0.89) on 156 high-severity incidents; performed a quantitative study focused on Apr–Jun 2025 for recency; categorized failures and tracked mitigation actions (auto-detection, hotfixes, routing, rebalancing, capacity changes).

Result: Dominant failure modes: ~60% inference-engine failures, of which ~40% were timeouts. Mitigation distribution: ~74% auto-detected, ~28% required hotfix (some overlap likely). Many incidents handled via traffic routing, node rebalancing, or capacity policies. Targeted strategies (connection liveness, GPU capacity-aware routing, per-endpoint isolation) reduced impact and sped recovery. Produced practitioner adoption checklist.

Conclusion: A systematic, empirically grounded taxonomy and incident analysis can reveal dominant failure modes and practical mitigation levers, enabling targeted automation and operational practices that improve reliability and cost-efficiency of LLM serving at scale.

Abstract: Hyperscale large language model (LLM) inference places extraordinary demands on cloud systems, where even brief failures can translate into significant user and business impact. To better understand and mitigate these risks, we present one of the first provider-internal, practice-based analysis of LLM inference incidents. We developed a taxonomy and methodology grounded in a year of operational experience, validating it on 156 high-severity incidents, and conducted a focused quantitative study of Apr-Jun 2025 to ensure recency and relevance. Our approach achieves high labeling consistency (Cohen's K ~0.89), identifies dominant failure modes (in our dataset ~60% inference engine failures, within that category ~40% timeouts), and surfaces mitigation levers (~74% auto-detected; ~28% required hotfix). Beyond hotfixes, many incidents were mitigated via traffic routing, node rebalancing, or capacity increase policies, indicating further automation opportunities. We also show how the taxonomy guided targeted strategies such as connection liveness, GPU capacity-aware routing, and per-endpoint isolation and reduced incident impact and accelerated recovery. Finally, we contribute a practitioner-oriented adoption checklist that enables others to replicate our taxonomy, analysis, and automation opportunities in their own systems. This study demonstrates how systematic, empirically grounded analysis of inference operations can drive more reliable and cost-efficient LLM serving at scale.

</details>


### [6] [An Evaluation of LLMs Inference on Popular Single-board Computers](https://arxiv.org/abs/2511.07425)
*Tung,Nguyen,Tuyen Nguyen*

Main category: cs.DC

TL;DR: Benchmarks 25 quantized open-source LLMs on three single-board computers (Raspberry Pi 4, Raspberry Pi 5, Orange Pi 5 Pro) using two runtimes (Ollama, Llamafile). Finds SBCs can run models up to 1.5B parameters; Llamafile gives up to 4× throughput and 30–40% lower power than Ollama. Identifies hardware and runtime bottlenecks and gives deployment recommendations.


<details>
  <summary>Details</summary>
Motivation: Enable low-cost, privacy-preserving on-device LLM inference by exploring the underexamined capability of common SBCs (Raspberry Pi and Orange Pi) for running quantized open-source models.

Method: Measured generation throughput, memory usage, and power consumption for 25 quantized LLMs across three SBCs and two runtimes, varying CPU configurations and prompt types to simulate realistic workloads.

Result: SBCs reliably supported models up to 1.5B parameters. Llamafile outperformed Ollama (up to 4× throughput, 30–40% less power). The study uncovered architecture-specific bottlenecks and trade-offs between runtimes.

Conclusion: Provides the first broad evaluation of LLM inference on SBCs and practical deployment guidance, narrowing the gap between large language models and affordable edge hardware.

Abstract: The growing demand for on-device large language model (LLM) inference is driving interest in deploying lightweight, cost-effective AI solutions on edge hardware. Single-board computers (SBCs) such as the Raspberry Pi and Orange Pi offer a promising platform for localized, privacy-preserving inference-but remain underexplored in the context of LLM workloads. In this work, we benchmark the performance of 25 quantized open-source LLMs across three SBCs-Raspberry Pi 4, Raspberry Pi 5, and Orange Pi 5 Pro-using two inference runtimes: Ollama and Llamafile. We evaluate generation throughput, memory usage, and power consumption under varying CPU configurations, using multiple prompt types to simulate realistic workloads. Our results show that SBCs can reliably support models up to 1.5B parameters, with Llamafile achieving up to 4x higher throughput and 30-40% lower power usage than Ollama. We identify architecture-specific bottlenecks, highlight runtime-level trade-offs, and provide practical deployment recommendations. This study offers the first broad evaluation of LLM inference on SBCs, bridging the gap between high-performance language models and affordable edge computing.

</details>


### [7] [Network and Systems Performance Characterization of MCP-Enabled LLM Agents](https://arxiv.org/abs/2511.07426)
*Zihao Ding,Mufeng Zhu,Yao Liu*

Main category: cs.DC

TL;DR: Paper measures how including Model Context Protocol (MCP) information (system prompts, tool definitions, histories) inflates token usage and costs for LLM interactions, quantifies trade-offs between capability, performance and cost, and suggests optimizations such as parallel tool calls and abort mechanisms.


<details>
  <summary>Details</summary>
Motivation: MCP standardizes LLM interactions with external tools but expands the conversation context significantly. Since providers bill per token, practitioners need empirical data and guidance on how MCP choices affect monetary cost, latency, and success rates.

Method: A measurement-driven study comparing different LLMs and MCP configurations across token efficiency, monetary cost, task completion time and success rate. Analyses explore contributions of system prompts, tool definitions and context histories to token counts and evaluate optimization ideas (parallel calls, aborts).

Result: MCP-enabled interactions substantially increase token consumption and costs. Different models and MCP setups show varying cost-latency-success trade-offs. Some optimizations (parallel tool calls, task aborts) reduce latency/cost but need careful engineering to preserve correctness.

Conclusion: The paper provides actionable insights for building more efficient and robust MCP workflows: tune MCP verbosity, choose appropriate models, adopt parallelism and abort mechanisms, and investigate context-reduction techniques to balance capability and cost.

Abstract: Model Context Protocol (MCP) has recently gained increased attention within the AI community for providing a standardized way for large language models (LLMs) to interact with external tools and services, significantly enhancing their capabilities. However, the inclusion of extensive contextual information, including system prompts, MCP tool definitions, and context histories, in MCP-enabled LLM interactions, dramatically inflates token usage. Given that LLM providers charge based on tokens, these expanded contexts can quickly escalate monetary costs and increase the computational load on LLM services. This paper presents a comprehensive measurement-based analysis of MCP-enabled interactions with LLMs, revealing trade-offs between capability, performance, and cost. We explore how different LLM models and MCP configurations impact key performance metrics such as token efficiency, monetary cost, task completion times, and task success rates, and suggest potential optimizations, including enabling parallel tool calls and implementing robust task abort mechanisms. These findings provide useful insights for developing more efficient, robust, and cost-effective MCP-enabled workflows.

</details>


### [8] [DynaKV: Enabling Accurate and Efficient Long-Sequence LLM Decoding on Smartphones](https://arxiv.org/abs/2511.07427)
*Tuowei Wang,Minxing Huang,Fengzu Li,Ligeng Chen,Jinrui Zhang,Ju Ren*

Main category: cs.DC

TL;DR: DynaKV is an adaptive key-value cache management system for long-sequence LLM decoding on smartphones. It dynamically adapts clusters during retrieval, organizes flash layout for correlated entries, and virtualizes cache across DRAM and flash to improve retrieval accuracy and latency under mobile resource constraints.


<details>
  <summary>Details</summary>
Motivation: Long-sequence LLM decoding on smartphones is limited by KVCache memory footprint that grows with sequence length and by smartphone-specific constraints (bandwidth, IOPS, DRAM). Retrieval-based offloading to flash helps but static clustering becomes misaligned as the KVCache distribution shifts during decoding, causing missed relevant entries or redundant fetches.

Method: DynaKV combines: (1) Migration-Free Cluster Adaptation—adaptive cluster splitting during retrieval without extra transfers; (2) Continuity-Centric Flash Management—co-locating correlated entries and clusters with a dual-head layout for efficient updates; (3) Memory-Efficient Cache Design—virtualizing cache across DRAM and flash and using cluster-aware replacement policies.

Result: Evaluations report that DynaKV improves retrieval accuracy by 1.38× and end-to-end decoding speed by 1.47× compared to state-of-the-art baselines. The design is claimed to generalize to other long-context workloads and multi-tier memory hierarchies.

Conclusion: DynaKV addresses dynamic distribution shifts and resource limits for smartphone LLM decoding via adaptive clustering, flash-aware layout, and memory virtualization, yielding measurable accuracy and latency improvements and broader applicability.

Abstract: As the demand for human-like reasoning, multi-turn dialogues, and long-form responses grows, large language models (LLMs) are increasingly expected to support efficient and effective long-sequence decoding. However, due to limited DRAM capacity, long-seuqence LLM decoding on smartphones is constrained by the key-value cache (KVCache), whose memory footprint increases linearly with sequence length. Retrieval-based methods mitigate DRAM pressure by offloading KVCache to flash and retrieving query-relevant entries through cluster-based indexing. Unfortunately, as decoding progresses, KVCache distribution shifts render static or local cluster updates progressively misaligned, excluding essential entries or fetching redundant ones. These issues are further exacerbated by smartphone-specific limitations in bandwidth, IOPS, and memory capacity.
  We propose DynaKV, the first adaptive KVCache management approach that jointly addresses accuracy and efficiency for long-sequence decoding on smartphones. DynaKV integrates three key techniques: (1) Migration-Free Cluster Adaptation, which adaptively splits clusters during retrieval without incurring additional transfers; (2) Continuity-Centric Flash Management, which co-locates correlated entries and clusters and employs a dual-head layout for efficient updates; and (3) Memory-Efficient Cache Design, which virtualizes cache space across DRAM and flash and extends replacement policies to align with cluster-level access patterns. Evaluations demonstrate that DynaKV improves retrieval accuracy and reduces end-to-end latency compared to state-of-the-art solutions, achieving average gains of $1.38\times$ in accuracy and $1.47\times$ speedups. Furthermore, the insights of DynaKV naturally extend to other long-context workloads and multi-tier memory hierarchies, underscoring its broader applicability.

</details>


### [9] [HyProv: Hybrid Provenance Management for Scientific Workflows](https://arxiv.org/abs/2511.07574)
*Vasilis Bountris,Lauritz Thamsen,Ulf Leser*

Main category: cs.DC

TL;DR: HyProv is a hybrid provenance management system combining a centralized store for small, stable workflow-specification provenance and federated querying across scalable execution log stores to achieve workflow-aware, low-latency provenance queries for distributed workflows. Implemented for Airflow + Kubernetes, it achieves sub-second query latencies, scales to large workflows, and imposes modest CPU/memory overhead.


<details>
  <summary>Details</summary>
Motivation: Scientific workflows are growing in complexity and run on distributed clusters; collecting, integrating, and analyzing provenance in real time is challenging. Existing systems trade off scalability, real-time processing, and workflow awareness. A workflow-aware, scalable, low-latency provenance system can enable better monitoring, failure analysis, and optimization.

Method: Design a hybrid architecture (HyProv) with a centralized component that stores workflow-specification-specific provenance and federated querying over multiple monitoring/provenance databases for large-scale execution logs. The central component accelerates access to stable logical provenance while federated queries retrieve current execution data from scalable stores; example integration with Airflow and Kubernetes demonstrates complex provenance queries.

Result: HyProv scales to large workflows, answers provenance queries with sub-second latencies, and incurs only modest CPU and memory overhead in the cluster.

Conclusion: A hybrid centralized+federated, workflow-aware provenance approach can deliver scalable, real-time provenance analytics with low latency and minimal resource impact, making it practical for real distributed workflow systems.

Abstract: Provenance plays a crucial role in scientific workflow execution, for instance by providing data for failure analysis, real-time monitoring, or statistics on resource utilization for right-sizing allocations. The workflows themselves, however, become increasingly complex in terms of involved components. Furthermore, they are executed on distributed cluster infrastructures, which makes the real-time collection, integration, and analysis of provenance data challenging. Existing provenance systems struggle to balance scalability, real-time processing, online provenance analytics, and integration across different components and compute resources. Moreover, most provenance solutions are not workflow-aware; by focusing on arbitrary workloads, they miss opportunities for workflow systems where optimization and analysis can exploit the availability of a workflow specification that dictates, to some degree, task execution orders and provides abstractions for physical tasks at a logical level.
  In this paper, we present HyProv, a hybrid provenance management system that combines centralized and federated paradigms to offer scalable, online, and workflow-aware queries over workflow provenance traces. HyProv uses a centralized component for efficient management of the small and stable workflow-specification-specific provenance, and complements this with federated querying over different scalable monitoring and provenance databases for the large-scale execution logs. This enables low-latency access to current execution data. Furthermore, the design supports complex provenance queries, which we exemplify for the workflow system Airflow in combination with the resource manager Kubernetes. Our experiments indicate that HyProv scales to large workflows, answers provenance queries with sub-second latencies, and adds only modest CPU and memory overhead to the cluster.

</details>


### [10] [Intelligence per Watt: Measuring Intelligence Efficiency of Local AI](https://arxiv.org/abs/2511.07885)
*Jon Saad-Falcon,Avanika Narayan,Hakki Orhun Akengin,J. Wes Griffin,Herumb Shandilya,Adrian Gamarra Lafuente,Medhya Goel,Rebecca Joseph,Shlok Natarajan,Etash Kumar Guha,Shang Zhu,Ben Athiwaratkun,John Hennessy,Azalia Mirhoseini,Christopher Ré*

Main category: cs.DC

TL;DR: Local inference on small LMs (<=20B) can accurately handle a large fraction of real-world single-turn queries while using far less energy per correct answer than cloud setups; the paper introduces intelligence per watt (IPW) and finds improving IPW and widening local coverage from 2023–2025.


<details>
  <summary>Details</summary>
Motivation: Centralized cloud has scaling limits as LLM demand grows. New small LMs and efficient local accelerators enable potential redistribution of inference to user devices. The work asks whether local inference is accurate and energy-efficient enough to be practical on power-constrained devices.

Method: Define IPW = task accuracy / energy consumed. Large-scale empirical evaluation across 20+ local LMs, 8 accelerators (including mobile/desktop chips), and 1M real-world single-turn chat/reasoning queries. For each query they measured accuracy, energy, latency, and power to compare model-accelerator pairs and cloud baselines.

Result: Key findings: (1) Local LMs can correctly answer 88.7% of single-turn chat and reasoning queries, with domain-dependent variation. (2) From 2023–2025, IPW improved 5.3× and local query coverage grew from 23.2% to 71.3%. (3) Local accelerators show at least 1.4× lower IPW (i.e., better efficiency) than cloud accelerators on identical models, indicating additional optimization headroom. They release an IPW profiling harness.

Conclusion: Local inference is a viable way to redistribute significant LLM demand away from centralized cloud infrastructure; IPW is a practical metric to track capability and efficiency of this transition.

Abstract: Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm: small LMs (<=20B active parameters) now achieve competitive performance to frontier models on many tasks, and local accelerators (e.g., Apple M4 Max) run these models at interactive latencies. This raises the question: can local inference viably redistribute demand from centralized infrastructure? Answering this requires measuring whether local LMs can accurately answer real-world queries and whether they can do so efficiently enough to be practical on power-constrained devices (i.e., laptops). We propose intelligence per watt (IPW), task accuracy divided by unit of power, as a metric for assessing capability and efficiency of local inference across model-accelerator pairs. We conduct a large-scale empirical study across 20+ state-of-the-art local LMs, 8 accelerators, and a representative subset of LLM traffic: 1M real-world single-turn chat and reasoning queries. For each query, we measure accuracy, energy, latency, and power. Our analysis reveals $3$ findings. First, local LMs can accurately answer 88.7% of single-turn chat and reasoning queries with accuracy varying by domain. Second, from 2023-2025, IPW improved 5.3x and local query coverage rose from 23.2% to 71.3%. Third, local accelerators achieve at least 1.4x lower IPW than cloud accelerators running identical models, revealing significant headroom for optimization. These findings demonstrate that local inference can meaningfully redistribute demand from centralized infrastructure, with IPW serving as the critical metric for tracking this transition. We release our IPW profiling harness for systematic intelligence-per-watt benchmarking.

</details>


### [11] [Generic Algorithm for Universal TDM Communication Over Inter Satellite Links](https://arxiv.org/abs/2511.08034)
*Miroslav Popovic,Marko Popovic,Pavle Vasiljevic,Ilija Basicevic*

Main category: cs.DC

TL;DR: Paper extends a Python federated learning testbed by adding a universal TDM communication algorithm that allows a node to communicate with multiple peers simultaneously (if peers agree). It provides theoretical foundations, system design, and validation, enabling realistic inter-satellite link scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing TDM implementation supports only pairwise exchanges, which limits fidelity for real-world networks (e.g., satellites or multi-peer IoT) where nodes may contact multiple peers during a time slot. The extension aims to model and support those realistic multi-peer contact patterns within federated learning experiments.

Method: Develop a generic algorithm for universal TDM that permits many-to-many communications within a time slot, grounded in theoretical formulation (matching/negotiation rules and scheduling constraints). Integrate the algorithm into the Python Testbed, describe system architecture, and validate via simulations or experiments demonstrating functionality over inter-satellite-like links.

Result: New algorithm implemented in the testbed, validated to allow multi-peer TDM exchanges. The testbed now supports more realistic communication patterns and can model inter-satellite link behavior, presumably showing correct operation and improved applicability to satellite scenarios.

Conclusion: The contribution removes the pairwise-communication limitation of the original testbed, enabling universal TDM useful for federated learning over inter-satellite links. Further evaluation of performance, scalability, convergence impact on learning, and robustness would strengthen the work.

Abstract: The original Python Testbed for Federated Learning Algorithms is a light FL framework, which provides the three generic algorithms: the centralized federated learning, the decentralized federated learning, and the TDM communication (i.e., peer data exchange) in the current time slot. The limitation of the latter is that it allows communication only between pairs of network nodes. This paper presents the new generic algorithm for the universal TDM communication that overcomes this limitation, such that a node can communicate with an arbitrary number of peers (assuming the peers also want to communicate with it). The paper covers: (i) the algorithm's theoretical foundation, (ii) the system design, and (iii) the system validation. The main advantage of the new algorithm is that it supports real-world TDM communications over inter satellite links.

</details>


### [12] [ProbSelect: Stochastic Client Selection for GPU-Accelerated Compute Devices in the 3D Continuum](https://arxiv.org/abs/2511.08147)
*Andrija Stanisic,Stefan Nastic*

Main category: cs.DC

TL;DR: ProbSelect uses analytical modeling plus probabilistic forecasting to select GPU-accelerated clients for federated learning across edge/cloud/space without historical monitoring, improving SLO compliance by 13.77% and reducing computational waste by 72.5%.


<details>
  <summary>Details</summary>
Motivation: Integrating edge, cloud and space devices into a single 3D continuum creates a highly dynamic environment where continuous monitoring and historical data collection are impractical. Existing client-selection methods focus on CPU-centric assumptions and miss GPU-specific characteristics important for training performance.

Method: Constructs analytical models of GPU-accelerated training cost and performance, applies probabilistic forecasting of device availability/performance within user-defined SLO windows, and uses these to perform SLO-aware client selection without relying on historical logs or continuous monitoring.

Result: Across multiple GPU architectures and workloads, ProbSelect increased SLO compliance by 13.77% on average and cut computational waste by 72.5% relative to baseline approaches.

Conclusion: ProbSelect is an effective SLO-aware client selection method for dynamic, GPU-accelerated federated learning in a 3D continuum, enabling better SLO compliance and much lower waste while avoiding the need for historical monitoring.

Abstract: Integration of edge, cloud and space devices into a unified 3D continuum imposes significant challenges for client selection in federated learning systems. Traditional approaches rely on continuous monitoring and historical data collection, which becomes impractical in dynamic environments where satellites and mobile devices frequently change operational conditions. Furthermore, existing solutions primarily consider CPU-based computation, failing to capture complex characteristics of GPU-accelerated training that is prevalent across the 3D continuum. This paper introduces ProbSelect, a novel approach utilizing analytical modeling and probabilistic forecasting for client selection on GPU-accelerated devices, without requiring historical data or continuous monitoring. We model client selection within user-defined SLOs. Extensive evaluation across diverse GPU architectures and workloads demonstrates that ProbSelect improves SLO compliance by 13.77% on average while achieving 72.5% computational waste reduction compared to baseline approaches.

</details>
