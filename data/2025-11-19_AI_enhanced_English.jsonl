{"id": "2511.14462", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.14462", "abs": "https://arxiv.org/abs/2511.14462", "authors": ["Michael Gundall", "Hans D. Schotten"], "title": "Cracking the Microsecond: An Efficient and Precise Time Synchronization Scheme for Hybrid 5G-TSN Networks", "comment": null, "summary": "Achieving precise time synchronization in wireless systems is essential for both industrial applications and 5G, where sub-microsecond accuracy is required. However, since the Industrial Internet of Things (IIoT) market is negligible compared to the consumer electronics market, the so-called IIoT enhancements have not yet been implemented in silicon. Moreover, there is no guarantee that this situation will change soon. Thus, alternative solutions must be explored. This paper addresses this challenge by introducing a scheme that uses a protocol capable of leveraging existing infrastructure to synchronize User Equipments (UEs), with one of the UEs serving as the master. If this master is connected via a wired link to the factory network, it can also function as a boundary clock for the factory network, including any Time-Sensitive Networking (TSN) network. Furthermore, the 5G Core Network (5GC) and 5G Base Station (gNB) can also be synchronized if they are connected either to the factory network or to the master UE. The proposed solution is implemented and evaluated on a hardware testbed using OpenAirInterface (OAI) and Software Defined Radios (SDRs). Time offset and clock skew are analyzed using a moving average filter with various window sizes. Results show that a filter size of 1024 provides the best accuracy for offset prediction between UEs. In a controlled lab environment, the approach consistently achieves synchronization within +/-50 ns, leaving sufficient margin for synchronization errors in real deployments while still maintaining sub-microsecond accuracy. These findings demonstrate the feasibility and high performance of the proposed protocol for stringent industrial use cases.", "AI": {"tldr": "Paper proposes a protocol that synchronizes UEs by using one UE as a master (which can act as a boundary clock to a factory TSN network and to 5GC/gNB if wired). Implemented on OAI + SDR testbed. Time offset and skew are estimated and smoothed with a moving-average filter; window=1024 gave best results. In lab it achieved \u00b150 ns synchronization, meeting sub\u2011microsecond industrial requirements.", "motivation": "Sub\u2011microsecond synchronization is required for industrial applications and 5G, but IIoT-specific silicon enhancements are not widely available. The paper seeks an alternative that leverages existing infrastructure to provide high\u2011precision sync without new silicon.", "method": "Designs a protocol that uses an existing UE as a master clock and distributes time to other UEs. If the master is wired to the factory network it acts as a boundary clock and can also synchronize 5GC/gNB when they are connected. Implemented on a hardware testbed using OpenAirInterface and SDRs. Time offset and clock skew are analyzed; a moving average filter with varying window sizes is used for offset prediction.", "result": "A moving average filter with window size 1024 yields the best offset prediction. In a controlled laboratory environment, the scheme consistently achieved synchronization within \u00b150 ns, well within sub\u2011microsecond requirements.", "conclusion": "The proposed protocol is feasible and achieves high performance for stringent industrial use cases; it can use existing infrastructure to provide boundary clock functionality and synchronize core and radio elements when connected."}}
{"id": "2511.14467", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.14467", "abs": "https://arxiv.org/abs/2511.14467", "authors": ["Heng Zhao", "Ruoyu Wang", "Tianhang Zheng", "Qi Li", "Bo Lv", "Yuyi Wang", "Wenliang Du"], "title": "From Topology to Behavioral Semantics: Enhancing BGP Security by Understanding BGP's Language with LLMs", "comment": "18 pages, 10 figures", "summary": "The trust-based nature of Border Gateway Protocol (BGP) makes it vulnerable to disruptions like prefix hijacking and misconfigurations, threatening routing stability. Traditional detection relies on manual inspection with limited scalability. Machine/Deep Learning (M/DL) approaches automate detection but suffer from suboptimal precision, limited generalizability, and high retraining costs. This is because existing methods focus on topological structures rather than comprehensive semantic characteristics of Autonomous Systems (ASes), often misinterpreting functionally similar but topologically distant ASes.\n  To address this, we propose BGPShield, an anomaly detection framework built on LLM embeddings that captures the Behavior Portrait and Routing Policy Rationale of each AS beyond topology, such as operational scale and global role. We propose a segment-wise aggregation scheme to transform AS descriptions into LLM representations without information loss, and a lightweight contrastive reduction network to compress them into a semantic-consistent version. Using these representations, our AR-DTW algorithm aligns and accumulates semantic distances to reveal behavioral inconsistencies. Evaluated on 16 real-world datasets, BGPShield detects 100% of verified anomalies with a false discovery rate below 5%. Notably, the employed LLMs were released prior to evaluation events, verifying generalizability. Furthermore, BGPShield constructs representations for unseen ASes within one second, significantly outperforming BEAM which demands costly retraining (averaging 65 hours).", "AI": {"tldr": "BGPShield is an LLM-embedding-based framework for BGP anomaly detection that creates semantic AS representations (Behavior Portrait and Routing Policy Rationale), compresses them via a contrastive reduction network, and uses AR-DTW to detect behavioral inconsistencies. On 16 datasets it detected all verified anomalies with <5% false discovery rate and can represent unseen ASes in ~1s, avoiding costly retraining.", "motivation": "BGP is vulnerable to hijacks and misconfigurations. Existing M/DL methods rely on topology and thus misclassify functionally similar but topologically distant ASes, have low precision, poor generalizability, and high retraining costs.", "method": "Use LLMs to produce semantic-rich AS descriptions via segment-wise aggregation to avoid information loss; compress embeddings using a lightweight contrastive reduction network to produce semantic-consistent vectors; apply AR-DTW to align and accumulate semantic distances over time to detect anomalies.", "result": "On 16 real-world datasets, BGPShield detected 100% of verified anomalies with a false discovery rate below 5%. LLM models used were released before events (supporting generalizability). Representation construction for unseen ASes takes ~1 second, compared with BEAM which requires ~65 hours of retraining.", "conclusion": "LLM-based semantic representations of AS behavior enable highly accurate, generalizable, and fast BGP anomaly detection, substantially reducing retraining overhead compared with prior methods."}}
{"id": "2511.14116", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.14116", "abs": "https://arxiv.org/abs/2511.14116", "authors": ["Ziyi Xu", "Zhiqiang Xie", "Swapnil Gandhi", "Christos Kozyrakis"], "title": "FailSafe: High-performance Resilient Serving", "comment": null, "summary": "Tensor parallelism (TP) enables large language models (LLMs) to scale inference efficiently across multiple GPUs, but its tight coupling makes systems fragile: a single GPU failure can halt execution, trigger costly KVCache recomputation, and introduce long-term compute and memory imbalance. We present FailSafe, a fault-tolerant TP serving system that sustains high performance under irregular GPU availability. FailSafe introduces three techniques to balance computation and memory across GPUs: (1) Cyclic KVCache Placement for uniform memory utilization, (2) Hybrid Attention combining tensor- and data-parallel attention to eliminate stragglers, and (3) Fine-Grained Load-Aware Routing to dynamically balance requests. It further employs proactive KVCache backup and on-demand weight recovery to avoid expensive recomputation and redundant data transfers. We implement these techniques in a lightweight serving engine compatible with existing LLM infrastructures. Evaluated on an 8xH100 DGX system with real-world fault traces and representative workloads, FailSafe achieves up to 2x higher throughput and two orders of magnitude lower recovery latency compared to standard fault handling approaches. Even with up to three GPU failures, FailSafe sustains high throughput and balanced utilization, demonstrating robust and efficient LLM serving under dynamic and unreliable hardware conditions.", "AI": {"tldr": "FailSafe is a fault-tolerant tensor-parallel (TP) serving system for LLM inference that maintains performance under GPU failures by rebalancing computation and memory and avoiding expensive KVCache recomputation. It introduces Cyclic KVCache Placement, Hybrid Attention (tensor+data-parallel), Fine-Grained Load-Aware Routing, proactive KVCache backup, and on-demand weight recovery. Evaluated on an 8x H100 DGX with real fault traces, it achieves up to 2\u00d7 throughput and ~100\u00d7 lower recovery latency, sustaining throughput even with up to three GPU failures.", "motivation": "Tensor parallelism enables efficient large-model inference but creates tight coupling across GPUs: a single GPU failure can stop execution, force costly KVCache recomputation, and induce lasting compute/memory imbalance. The goal is to make TP serving robust to irregular GPU availability while preserving high throughput and efficient resource use.", "method": "FailSafe introduces (1) Cyclic KVCache Placement to spread KVCache shards uniformly across GPUs for balanced memory utilization; (2) Hybrid Attention that mixes tensor- and data-parallel attention to avoid straggler effects during attention; (3) Fine-Grained Load-Aware Routing to dynamically distribute requests to balance compute load; plus proactive KVCache backups and on-demand weight recovery to reduce recomputation and redundant transfers. These are implemented in a lightweight serving engine compatible with existing LLM stacks.", "result": "On an 8x H100 DGX system with realistic fault traces and representative workloads, FailSafe achieves up to 2\u00d7 higher throughput and two orders of magnitude lower recovery latency versus standard fault-handling approaches, and maintains high throughput and balanced utilization even with up to three GPU failures.", "conclusion": "FailSafe demonstrates that a combination of KVCache placement, hybrid attention parallelism, dynamic routing, and lightweight backup/recovery mechanisms can make TP-based LLM serving robust and efficient under dynamic, unreliable hardware conditions, offering practical improvements in throughput and recovery times."}}
{"id": "2511.14124", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14124", "abs": "https://arxiv.org/abs/2511.14124", "authors": ["Sabiha Afroz", "Redwan Ibne Seraj Khan", "Hadeel Albahar", "Jingoo Han", "Ali R. Butt"], "title": "10Cache: Heterogeneous Resource-Aware Tensor Caching and Migration for LLM Training", "comment": "This paper accepted for presentation to the 16th ACM Symposium on Cloud Computing (SOCC'25)", "summary": "Training large language models (LLMs) in the cloud faces growing memory bottlenecks due to the limited capacity and high cost of GPUs. While GPU memory offloading to CPU and NVMe has made large-scale training more feasible, existing approaches suffer from high tensor migration latency and suboptimal device memory utilization, ultimately increasing training time and cloud costs. To address these challenges, we present 10Cache, a resource-aware tensor caching and migration system that accelerates LLM training by intelligently coordinating memory usage across GPU, CPU, and NVMe tiers. 10Cache profiles tensor execution order to construct prefetch policies, allocates memory buffers in pinned memory based on tensor size distributions, and reuses memory buffers to minimize allocation overhead.\n  Designed for cloud-scale deployments, 10Cache improves memory efficiency and reduces reliance on high-end GPUs. Across diverse LLM workloads, it achieves up to 2x speedup in training time, improves GPU cache hit rate by up to 86.6x, and increases CPU/GPU memory utilization by up to 2.15x and 1.33x, respectively, compared to state-of-the-art offloading methods. These results demonstrate that 10Cache is a practical and scalable solution for optimizing LLM training throughput and resource efficiency in cloud environments.", "AI": {"tldr": "10Cache is a multi-tier tensor caching and migration system for cloud LLM training that profiles tensor execution to prefetch tensors, uses pinned CPU memory buffer allocation and reuse, and coordinates GPU/CPU/NVMe usage to reduce migration latency and improve memory utilization. The authors report up to 2x training speedup, huge improvements in GPU cache hit rate, and higher CPU/GPU memory utilization versus prior offloading methods.", "motivation": "GPU memory limits and high GPU costs constrain large LLM training. Existing CPU/NVMe offloading solutions suffer high tensor migration latency and poor device memory utilization, increasing training time and cloud costs.", "method": "Profile tensor execution order to derive prefetch policies; allocate and reuse pinned CPU memory buffers sized by tensor distributions; coordinate tensor caching and migration across GPU, CPU, NVMe tiers to maximize cache hits and minimize allocation/migration overhead.", "result": "On varied LLM workloads, 10Cache achieves up to 2x training speedup, up to 86.6x GPU cache hit rate increase, and up to 2.15x and 1.33x improvements in CPU and GPU memory utilization respectively compared to state-of-the-art offloading methods.", "conclusion": "10Cache is an effective, scalable approach to reduce reliance on high-end GPUs and improve throughput and resource efficiency for cloud-scale LLM training."}}
{"id": "2511.14450", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.14450", "abs": "https://arxiv.org/abs/2511.14450", "authors": ["Mulei Ma", "Minrui Xu", "Zihan Chen", "Yang Yang", "Tony Q. S. Quek"], "title": "Hyperion: Hierarchical Scheduling for Parallel LLM Acceleration in Multi-tier Networks", "comment": null, "summary": "Large Language Models (LLMs) are increasingly executed across edge, fog, and cloud tiers where limited GPU memory, heterogeneous compute, and variable inter-tier bandwidth jointly constrain deployment and motivate model partitioning and request scheduling. In this setting, achieving low end-to-end latency is governed not only by where a model is deployed (inter-tier model partitioning) but also by how incoming requests are scheduled (intra-tier task scheduling) across heterogeneous nodes. These two problems are tightly coupled, as a suboptimal scheduler can negate the benefits of a good partition, and vice versa. In this paper, we propose Hyperion, a hierarchical two-stage framework that jointly optimizes partitioning and scheduling to minimize end-to-end latency for pipelined LLM inference in multi-tier networks, balancing compute and memory across tiers while introducing negligible runtime overhead and requiring no model retraining. Motivated by the observation that partition choices evolve on slower timescales than request arrivals, Stage 1 performs offline, inter-tier partitioning via a Binary Search with Dynamic Programming (BSDP) procedure to produce balanced stage times under tier capacity and memory constraints; to adapt to time-varying load, Stage 2 performs online, intra-tier scheduling with a lightweight Adaptive Real-time Task Scheduling (ARTS) algorithm that maps each request to the best available node using real-time estimates of queue length and effective capacity. Experimental results on multi-tier inference tasks demonstrate that Hyperion significantly reduces end-to-end latency by up to 52.1\\% and 31.2\\%, with the Phi-3-medium model, compared to the GPipe and HEFT baselines, respectively. Furthermore, Hyperion shows superior scalability in long-sequence generation, maintaining a 44.5\\% lower latency than GPipe and achieving higher GPU utilization.", "AI": {"tldr": "Hyperion is a hierarchical two-stage system that jointly optimizes inter-tier model partitioning and intra-tier request scheduling for pipelined LLM inference across edge/fog/cloud. Stage 1 uses a Binary Search with Dynamic Programming (BSDP) offline to produce capacity- and memory-aware partitions; Stage 2 uses an online Adaptive Real-time Task Scheduling (ARTS) to assign requests to nodes using real-time queue and capacity estimates. Experiments (Phi-3-medium) show up to 52.1% and 31.2% latency reductions versus GPipe and HEFT, and 44.5% lower latency than GPipe on long-sequence generation while improving GPU utilization.", "motivation": "LLM deployment across heterogeneous edge/fog/cloud tiers faces constrained GPU memory, varying compute power, and fluctuating inter-tier bandwidth. Optimal end-to-end pipelined inference latency depends jointly on where model stages are placed (partitioning) and how requests are scheduled across heterogeneous nodes (scheduling), and these two problems interact tightly.", "method": "Two-stage hierarchical framework: (1) Offline inter-tier partitioning via Binary Search with Dynamic Programming (BSDP) to produce balanced stage times under tier capacity and memory constraints, leveraging slower timescale of partition changes; (2) Online intra-tier scheduling with Adaptive Real-time Task Scheduling (ARTS), a lightweight scheduler that maps incoming requests to best-available nodes using real-time queue length and effective capacity estimates to adapt to time-varying load.", "result": "On multi-tier inference tasks with the Phi-3-medium model, Hyperion reduces end-to-end latency by up to 52.1% relative to GPipe and 31.2% relative to HEFT. For long-sequence generation it maintains a 44.5% lower latency than GPipe and achieves higher GPU utilization. The framework reportedly introduces negligible runtime overhead and requires no model retraining.", "conclusion": "Hyperion effectively and efficiently reduces pipelined LLM inference latency in multi-tier networks by jointly optimizing partitioning and scheduling with a practical offline/online split, demonstrating significant empirical gains and scalability without retraining or heavy runtime cost."}}
{"id": "2511.14456", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14456", "abs": "https://arxiv.org/abs/2511.14456", "authors": ["Fabian Stricker", "David Bermbach", "Christian Zirpins"], "title": "Analyzing the Impact of Participant Failures in Cross-Silo Federated Learning", "comment": "Accepted for publication in 3rd IEEE International Conference on Federated Learning Applications and Technologies (FLTA2025)", "summary": "Federated learning (FL) is a new paradigm for training machine learning (ML) models without sharing data. While applying FL in cross-silo scenarios, where organizations collaborate, it is necessary that the FL system is reliable; however, participants can fail due to various reasons (e.g., communication issues or misconfigurations). In order to provide a reliable system, it is necessary to analyze the impact of participant failures. While this problem received attention in cross-device FL where mobile devices with limited resources participate, there is comparatively little research in cross-silo FL.\n  Therefore, we conduct an extensive study for analyzing the impact of participant failures on the model quality in the context of inter-organizational cross-silo FL with few participants. In our study, we focus on analyzing generally influential factors such as the impact of the timing and the data as well as the impact on the evaluation, which is important for deciding, if the model should be deployed. We show that under high skews the evaluation is optimistic and hides the real impact. Furthermore, we demonstrate that the timing impacts the quality of the trained model. Our results offer insights for researchers and software architects aiming to build robust FL systems.", "AI": {"tldr": "Empirical study of participant failures in cross-silo federated learning with few parties. Analyzes how timing of failures, data skew, and evaluation choices affect model quality. Finds that high data skew makes evaluations optimistic and that failure timing significantly influences trained-model quality.", "motivation": "Cross-silo FL (inter-organizational) requires reliability because participant failures (communication, misconfiguration, etc.) occur. Prior work focuses on cross-device FL; less is known about the effects of failures when few, heterogeneous organizations collaborate. The paper aims to fill this gap and inform robust FL system design.", "method": "Conducts an extensive experimental study in cross-silo settings with a small number of participants. Systematically injects participant failures at different times, varies data distributions (including high skew), and examines evaluation protocols. Measures resulting model quality and compares reported evaluation to ground-truth impact.", "result": "Key findings: (1) Under high data skew, commonly used evaluation procedures are optimistic and mask the true negative impact of participant failures; (2) The timing of failures during training materially affects final model quality \u2014 some failure points hurt more than others; (3) Evaluation bias and timing effects are important considerations for deployment decisions.", "conclusion": "Practitioners and researchers should adopt evaluation protocols that account for data heterogeneity and failure scenarios, pay attention to failure timing during training, and design aggregation and fault-tolerance mechanisms for cross-silo FL. The paper provides empirical guidance for building more reliable FL systems and suggests avenues for future robustness research."}}
{"id": "2511.14617", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14617", "abs": "https://arxiv.org/abs/2511.14617", "authors": ["Ruoyu Qin", "Weiran He", "Weixiao Huang", "Yangkun Zhang", "Yikai Zhao", "Bo Pang", "Xinran Xu", "Yingdi Shan", "Yongwei Wu", "Mingxing Zhang"], "title": "Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning", "comment": "16 pages, 12 figures, 6 tables", "summary": "Reinforcement Learning (RL) has become critical for advancing modern Large Language Models (LLMs), yet existing synchronous RL systems face severe performance bottlenecks. The rollout phase, which dominates end-to-end iteration time, suffers from substantial long-tail latency and poor resource utilization due to inherent workload imbalance. We present Seer, a novel online context learning system that addresses these challenges by exploiting previously overlooked similarities in output lengths and generation patterns among requests sharing the same prompt. Seer introduces three key techniques: divided rollout for dynamic load balancing, context-aware scheduling, and adaptive grouped speculative decoding. Together, these mechanisms substantially reduce long-tail latency and improve resource efficiency during rollout. Evaluations on production-grade RL workloads demonstrate that Seer improves end-to-end rollout throughput by 74% to 97% and reduces long-tail latency by 75% to 93% compared to state-of-the-art synchronous RL systems, significantly accelerating RL training iterations.", "AI": {"tldr": "Seer is an online context learning system that reduces long-tail latency and improves resource utilization in synchronous RL rollouts by grouping similar requests and applying divided rollout, context-aware scheduling, and adaptive grouped speculative decoding, yielding 74\u201397% higher throughput and 75\u201393% reduced tail latency.", "motivation": "Synchronous RL for LLMs is dominated by the rollout phase, which incurs large end-to-end iteration time due to long-tail latency and workload imbalance from heterogeneous generation lengths and patterns across requests sharing the same prompt.", "method": "Seer exploits similarities among requests with the same prompt. It (1) performs divided rollout to split work dynamically for better load balance, (2) uses context-aware scheduling to order and batch workloads based on expected generation behavior, and (3) applies adaptive grouped speculative decoding to speculatively generate tokens for groups to hide latency while controlling wasted computation.", "result": "On production-grade RL workloads, Seer improves end-to-end rollout throughput by 74%\u201397% and reduces long-tail latency by 75%\u201393% versus state-of-the-art synchronous RL systems, substantially accelerating RL training iterations.", "conclusion": "Seer effectively addresses rollout bottlenecks through grouping- and context-aware techniques, delivering large efficiency and latency gains for RL training of LLMs. Future work should detail robustness across diverse prompt distributions, effects on sample quality and RL convergence, and implementation/overhead trade-offs."}}
