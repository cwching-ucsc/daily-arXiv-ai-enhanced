<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [OMPILOT: Harnessing Transformer Models for Auto Parallelization to Shared Memory Computing Paradigms](https://arxiv.org/abs/2511.03866)
*Arijit Bhattacharjee,Ali TehraniJamsaz,Le Chen,Niranjan Hasabnis,Mihai Capota,Nesreen Ahmed,Ali Jannesari*

Main category: cs.DC

TL;DR: Proposes OMPILOT, a domain-specific encoder–decoder transformer for translating C++ to OpenMP parallel code using custom pretraining that encodes parallel semantics and combined unsupervised/supervised training; introduces OMPBLEU, a metric for evaluating OpenMP constructs. Claims function-level translation and improved robustness over prior loop-level approaches.


<details>
  <summary>Details</summary>
Motivation: Automate and improve C++→OpenMP translation to accelerate shared-memory parallelization and legacy code modernization, leveraging LLM strengths in code understanding while addressing limitations of rule-based and loop-focused translators.

Method: Design a domain-specific encoder–decoder transformer (OMPILOT) with custom pretraining objectives that encode semantics of parallel constructs; use a mixture of unsupervised and supervised learning; operate at function granularity to capture broader context beyond loop-level transforms.

Result: Presents OMPILOT and the OMPBLEU composite metric tailored to assess correctness and quality of OpenMP parallel constructs. The abstract claims improved robustness and broader semantic capture, but provides no quantitative results or dataset/benchmark details.

Conclusion: The approach is promising—specialized modeling plus a targeted metric could improve automated parallelization—but the abstract lacks experimental validation, ablation studies, dataset description, and comparison baselines; these are needed to substantiate claims.

Abstract: Recent advances in large language models (LLMs) have significantly
accelerated progress in code translation, enabling more accurate and efficient
transformation across programming languages. While originally developed for
natural language processing, LLMs have shown strong capabilities in modeling
programming language syntax and semantics, outperforming traditional rule-based
systems in both accuracy and flexibility. These models have streamlined
cross-language conversion, reduced development overhead, and accelerated legacy
code migration. In this paper, we introduce OMPILOT, a novel domain-specific
encoder-decoder transformer tailored for translating C++ code into OpenMP,
enabling effective shared-memory parallelization. OMPILOT leverages custom
pre-training objectives that incorporate the semantics of parallel constructs
and combines both unsupervised and supervised learning strategies to improve
code translation robustness. Unlike previous work that focused primarily on
loop-level transformations, OMPILOT operates at the function level to capture a
wider semantic context. To evaluate our approach, we propose OMPBLEU, a novel
composite metric specifically crafted to assess the correctness and quality of
OpenMP parallel constructs, addressing limitations in conventional translation
metrics.

</details>


### [2] [Enabling Dynamic Sparsity in Quantized LLM Inference](https://arxiv.org/abs/2511.04477)
*Rongxiang Wang,Kangyuan Shu,Felix Xiaozhu Lin*

Main category: cs.DC

TL;DR: Introduces methods to combine dynamic activation sparsity with low-bit, group-wise quantization for LLM inference on commodity GPUs by reordering weight layout (zigzag), a custom GEMV kernel, and a lightweight sparse-index gatherer, yielding up to 1.55x decoding throughput with similar accuracy to dense quantized baselines.


<details>
  <summary>Details</summary>
Motivation: LLM inference on end-user devices is constrained by memory and compute; dynamic activation sparsity offers a route to reduce work but conflicts with dominant group-wise quantization layouts used to fit models into GPU memory. The goal is to reconcile these to get faster, memory-efficient inference.

Method: Proposes a zigzag-patterned weight layout aligned with activation sparsity to improve memory locality under quantization, a specialized GPU GEMV kernel to exploit the layout, and a compact runtime to collect sparse activation indices with minimal overhead.

Result: Across multiple models and hardware setups, they report up to 1.55x faster decoding throughput while maintaining accuracy comparable to dense quantized inference.

Conclusion: Structured/dynamic sparsity can be made compatible with group-wise low-bit quantization on commodity GPUs via layout, kernel, and runtime co-design, enabling practical speedups without accuracy loss.

Abstract: Deploying large language models (LLMs) on end-user devices is gaining
importance due to benefits in responsiveness, privacy, and operational cost.
Yet the limited memory and compute capability of mobile and desktop GPUs make
efficient execution difficult. Recent observations suggest that the internal
activations of LLMs are often dynamically sparse, meaning that for each input,
only part of the network contributes significantly to the output. Such sparsity
could reduce computation, but it interacts poorly with group-wise quantization,
which remains the dominant approach for fitting LLMs onto resource-constrained
hardware. To reconcile these two properties, this study proposes a set of
techniques that realize dynamic sparse inference under low-bit quantization.
The method features: (1) a zigzag-patterned quantization layout that organizes
weights in a way consistent with activation sparsity and improves GPU memory
locality; (2) a specialized GEMV kernel designed for this layout to fully
utilize parallel compute units; and (3) a compact runtime mechanism that
gathers sparse indices with minimal overhead. Across several model scales and
hardware configurations, the approach achieves up to 1.55x faster decoding
throughput while maintaining accuracy comparable to dense quantized inference,
showing that structured sparsity and quantization can effectively coexist on
commodity GPUs.

</details>
