{"id": "2512.02278", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.02278", "abs": "https://arxiv.org/abs/2512.02278", "authors": ["Yi Liu", "Chen Qian"], "title": "Fantasy: Efficient Large-scale Vector Search on GPU Clusters with GPUDirect Async", "comment": null, "summary": "Vector similarity search has become a critical component in AI-driven applications such as large language models (LLMs). To achieve high recall and low latency, GPUs are utilized to exploit massive parallelism for faster query processing. However, as the number of vectors continues to grow, the graph size quickly exceeds the memory capacity of a single GPU, making it infeasible to store and process the entire index on a single GPU. Recent work uses CPU-GPU architectures to keep vectors in CPU memory or SSDs, but the loading step stalls GPU computation. We present Fantasy, an efficient system that pipelines vector search and data transfer in a GPU cluster with GPUDirect Async. Fantasy overlaps computation and network communication to significantly improve search throughput for large graphs and deliver large query batch sizes.", "AI": {"tldr": "Fantasy is a system that pipelines vector search and data transfer across a GPU cluster using GPUDirect Async to overlap computation and communication, improving throughput for very large graph-based vector indexes that exceed single-GPU memory.", "motivation": "Vector similarity search is critical for AI applications (e.g., LLMs). GPUs accelerate search but single-GPU memory limits are hit as index sizes grow. CPU/SSD-backed approaches incur stalls when loading data to GPUs, reducing throughput and increasing latency.", "method": "Fantasy implements a pipelined execution model in a GPU cluster leveraging GPUDirect Async to overlap network/PCIe transfers with GPU computation. It keeps parts of the graph outside a single GPU and streams needed data while continuing useful work, thereby minimizing idle GPU time and supporting large query batches.", "result": "By overlapping data transfer and computation, Fantasy significantly improves search throughput on large graph indexes and can handle large query batch sizes with lower stall time compared to CPU-GPU loading approaches. The abstract reports qualitative throughput improvements (no numerical values provided).", "conclusion": "Fantasy demonstrates that combining pipelined execution and GPUDirect Async in a multi-GPU cluster is an effective way to scale graph-based vector search beyond single-GPU memory limits, improving utilization and throughput for large-scale similarity search."}}
{"id": "2512.02272", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.02272", "abs": "https://arxiv.org/abs/2512.02272", "authors": ["Ali Diab", "Adel Chehade", "Edoardo Ragusa", "Paolo Gastaldo", "Rodolfo Zunino", "Amer Baghdadi", "Mostafa Rizk"], "title": "Intrusion Detection on Resource-Constrained IoT Devices with Hardware-Aware ML and DL", "comment": "Accepted at the 2025 IEEE International Conference on Emerging Trends in Engineering and Computing (ETECOM). Recipient of the ETECOM 2025 Best Paper Award", "summary": "This paper proposes a hardware-aware intrusion detection system (IDS) for Internet of Things (IoT) and Industrial IoT (IIoT) networks; it targets scenarios where classification is essential for fast, privacy-preserving, and resource-efficient threat detection. The goal is to optimize both tree-based machine learning (ML) models and compact deep neural networks (DNNs) within strict edge-device constraints. This allows for a fair comparison and reveals trade-offs between model families. We apply constrained grid search for tree-based classifiers and hardware-aware neural architecture search (HW-NAS) for 1D convolutional neural networks (1D-CNNs). Evaluation on the Edge-IIoTset benchmark shows that selected models meet tight flash, RAM, and compute limits: LightGBM achieves 95.3% accuracy using 75 KB flash and 1.2 K operations, while the HW-NAS-optimized CNN reaches 97.2% with 190 KB flash and 840 K floating-point operations (FLOPs). We deploy the full pipeline on a Raspberry Pi 3 B Plus, confirming that tree-based models operate within 30 ms and that CNNs remain suitable when accuracy outweighs latency. These results highlight the practicality of hardware-constrained model design for real-time IDS at the edge.", "AI": {"tldr": "Hardware-aware IDS for IoT/IIoT compares optimized tree-based models and HW-NAS 1D-CNNs under strict edge constraints. LightGBM: 95.3% accuracy (75 KB flash, 1.2K ops). HW-NAS CNN: 97.2% (190 KB flash, 840K FLOPs). Deployed on Raspberry Pi 3 B+, trees run <30 ms; CNNs trade latency for accuracy.", "motivation": "Provide fast, privacy-preserving, resource-efficient intrusion detection on constrained edge devices by designing models that meet flash/RAM/compute limits and comparing tree-based vs compact DNN approaches.", "method": "Constrained grid search for tree-based classifiers; hardware-aware neural architecture search (HW-NAS) for 1D-CNNs; optimization targets flash, RAM, and compute budgets; evaluation on Edge-IIoTset; deployment and latency measurement on Raspberry Pi 3 B+.", "result": "Selected LightGBM achieves 95.3% accuracy within 75 KB flash and 1.2K operations; HW-NAS optimized CNN achieves 97.2% with 190 KB flash and 840K FLOPs. Trees are extremely low-resource and low-latency; CNN provides higher accuracy at larger compute cost. Both are feasible for edge real-time IDS.", "conclusion": "Hardware-constrained model design enables practical real-time IDS at the edge; choice between trees and CNNs depends on the accuracy vs latency/resource trade-offs."}}
{"id": "2512.02861", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.02861", "abs": "https://arxiv.org/abs/2512.02861", "authors": ["Oscar G. Lira", "Oscar M. Caicedo", "Nelson L. S. Da Fonseca"], "title": "Network Self-Configuration based on Fine-Tuned Small Language Models", "comment": "16 pages, 11 figures, 3 tables", "summary": "As modern networks grow in scale and complexity, manual configuration becomes increasingly inefficient and prone to human error. While intent-driven self-configuration using large language models has shown significant promise, such models remain computationally expensive, resource-intensive, and often raise privacy concerns because they typically rely on external cloud infrastructure. This work introduces SLM_netconfig, a fine-tuned small language model framework that uses an agent-based architecture and parameter-efficient adaptation techniques to translate configuration intents expressed as natural language requirements or questions into syntactically and semantically valid network configurations. The system is trained on a domain-specific dataset generated through a pipeline derived from vendor documentation, ensuring strong alignment with real-world configuration practices. Extensive evaluation shows that SLM_netconfig, when using its question-to-configuration model, achieves higher syntactic accuracy and goal accuracy than LLM-NetCFG while substantially reducing translation latency and producing concise, interpretable configurations. These results demonstrate that fine-tuned small language models, as implemented in SLM_netconfig, can deliver efficient, accurate, and privacy-preserving automated configuration generation entirely on-premise, making them a practical and scalable solution for modern autonomous network configuration.", "AI": {"tldr": "SLM_netconfig is a fine-tuned small-language-model framework (agent-based + parameter-efficient adaptation) that translates natural-language configuration intents into valid network configurations. Trained on vendor-document-derived data, it outperforms LLM-NetCFG in syntactic and goal accuracy, reduces latency, and enables on-prem, privacy-preserving automated configuration generation.", "motivation": "Manual network configuration is error-prone and slow; large LLMs enable intent-driven automation but are expensive, resource-intensive, and raise privacy concerns when run in the cloud. The paper aims to deliver an efficient, accurate, and private on-premise alternative.", "method": "Fine-tune a small language model using parameter-efficient techniques (e.g., PEFT) inside an agent-based architecture. Generate a domain-specific training dataset by extracting and synthesizing examples from vendor documentation. Train a question-to-configuration model to map natural-language requirements/questions to syntactically and semantically valid configs.", "result": "On evaluations, SLM_netconfig\u2019s question-to-configuration model achieves higher syntactic accuracy and goal accuracy than LLM-NetCFG, with substantially lower translation latency and more concise, interpretable outputs.", "conclusion": "Fine-tuned small language models (as implemented in SLM_netconfig) can be a practical, scalable, and privacy-preserving on-prem solution for automated network configuration, offering competitive accuracy and much lower resource/latency costs compared to large cloud LLMs."}}
