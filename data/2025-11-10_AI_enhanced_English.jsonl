{"id": "2511.05053", "categories": ["cs.DC", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.05053", "abs": "https://arxiv.org/abs/2511.05053", "authors": ["Wakuto Matsumi", "Riaz-Ul-Haque Mian"], "title": "Accelerating HDC-CNN Hybrid Models Using Custom Instructions on RISC-V GPUs", "comment": null, "summary": "Machine learning based on neural networks has advanced rapidly, but the high\nenergy consumption required for training and inference remains a major\nchallenge. Hyperdimensional Computing (HDC) offers a lightweight,\nbrain-inspired alternative that enables high parallelism but often suffers from\nlower accuracy on complex visual tasks. To overcome this, hybrid accelerators\ncombining HDC and Convolutional Neural Networks (CNNs) have been proposed,\nthough their adoption is limited by poor generalizability and programmability.\nThe rise of open-source RISC-V architectures has created new opportunities for\ndomain-specific GPU design. Unlike traditional proprietary GPUs, emerging\nRISC-V-based GPUs provide flexible, programmable platforms suitable for custom\ncomputation models such as HDC. In this study, we design and implement custom\nGPU instructions optimized for HDC operations, enabling efficient processing\nfor hybrid HDC-CNN workloads. Experimental results using four types of custom\nHDC instructions show a performance improvement of up to 56.2 times in\nmicrobenchmark tests, demonstrating the potential of RISC-V GPUs for\nenergy-efficient, high-performance computing.", "AI": {"tldr": "Paper proposes custom RISC-V GPU instructions for Hyperdimensional Computing (HDC) to accelerate hybrid HDC\u2013CNN workloads, reporting up to 56.2\u00d7 microbenchmark speedups.", "motivation": "Neural networks are energy-hungry; HDC is a low-energy, highly parallel alternative but has lower accuracy on complex visual tasks. Hybrid HDC\u2013CNN accelerators exist but face programmability and generalizability issues. RISC-V-based GPUs offer a flexible platform to implement domain-specific instructions for HDC.", "method": "Design and implement four custom GPU instructions tailored to HDC primitives and integrate them into a RISC-V GPU pipeline. Use these instructions to accelerate hybrid HDC\u2013CNN workloads and evaluate via microbenchmarks and end-to-end experiments.", "result": "Microbenchmark experiments show performance improvements up to 56.2\u00d7 with the custom HDC instructions. Results demonstrate feasibility and efficiency gains of implementing HDC primitives as GPU instructions on RISC-V-based platforms.", "conclusion": "Custom HDC instructions on RISC-V GPUs can markedly accelerate HDC and hybrid HDC\u2013CNN workloads, suggesting RISC-V GPUs are a promising, programmable, energy-efficient platform for domain-specific ML acceleration."}}
{"id": "2511.05238", "categories": ["cs.NI", "68T05, 90C26, 68M10", "I.2.11; C.2.1; C.4; G.3"], "pdf": "https://arxiv.org/pdf/2511.05238", "abs": "https://arxiv.org/abs/2511.05238", "authors": ["Peide Li", "Liu Cao", "Lyutianyang Zhang", "Dongyu Wei", "Ye Hu", "Qipeng Xie"], "title": "EPFL-REMNet: Efficient Personalized Federated Digital Twin Towards 6G Heterogeneous Radio Environme", "comment": "Approx. 12 pages, 3 figures, 3 tables; focuses on 6G heterogeneous\n  radio environment digital twin construction via personalized federated\n  learning", "summary": "Radio Environment Map (REM) is transitioning from 5G homogeneous environments\nto B5G/6G heterogeneous landscapes. However, standard Federated Learning (FL),\na natural fit for this distributed task, struggles with performance degradation\nin accuracy and communication efficiency under the non-independent and\nidentically distributed (Non-IID) data conditions inherent to these new\nenvironments. This paper proposes EPFL-REMNet, an efficient personalized\nfederated framework for constructing a high-fidelity digital twin of the 6G\nheterogeneous radio environment. The proposed EPFL-REMNet employs a\"shared\nbackbone + lightweight personalized head\" model, where only the compressed\nshared backbone is transmitted between the server and clients, while each\nclient's personalized head is maintained locally. We tested EPFL-REMNet by\nconstructing three distinct Non-IID scenarios (light, medium, and heavy) based\non radio environment complexity, with data geographically partitioned across 90\nclients. Experimental results demonstrate that EPFL-REMNet simultaneously\nachieves higher digital twin fidelity (accuracy) and lower uplink overhead\nacross all Non-IID settings compared to standard FedAvg and recent\nstate-of-the-art methods. Particularly, it significantly reduces performance\ndisparities across datasets and improves local map accuracy for long-tail\nclients, enhancing the overall integrity of digital twin.", "AI": {"tldr": "EPFL-REMNet is a personalized federated learning framework for Radio Environment Map construction in 6G heterogeneous settings that transmits a compressed shared backbone and keeps lightweight personalized heads locally, achieving higher accuracy and lower uplink overhead than FedAvg and recent baselines under three Non\u2011IID scenarios across 90 clients.", "motivation": "As REM moves from homogeneous 5G to heterogeneous B5G/6G environments, federated learning faces accuracy and communication-efficiency degradation under naturally Non\u2011IID client data; a personalized, communication\u2011efficient FL design is needed to build high\u2011fidelity digital twins.", "method": "Use a 'shared backbone + lightweight personalized head' model: compress the shared backbone and transmit it between server and clients while keeping each client's personalized head local. Evaluate under three Non\u2011IID complexity levels (light/medium/heavy) with geographical partitioning across 90 clients, comparing to FedAvg and state\u2011of\u2011the\u2011art methods.", "result": "EPFL-REMNet improves digital twin fidelity (accuracy) and reduces uplink communication overhead across all Non\u2011IID settings. It reduces performance disparity among clients and particularly improves accuracy for long\u2011tail clients.", "conclusion": "Personalized model partitioning with compressed backbone updates is an effective strategy for REM construction in heterogeneous 6G scenarios, yielding communication savings and better per\u2011client performance than standard federated approaches, though further details and evaluations would strengthen the claim."}}
{"id": "2511.05362", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.05362", "abs": "https://arxiv.org/abs/2511.05362", "authors": ["Lucian Trestioreanu", "Flaviene Scheidt", "Wazen Shbair", "Jerome Francois", "Damien Magoni", "Radu State"], "title": "To Squelch or not to Squelch: Enabling Improved Message Dissemination on the XRP Ledger", "comment": "7 pages", "summary": "With the large increase in the adoption of blockchain technologies, their\nunderlying peer-to-peer networks must also scale with the demand. In this\ncontext, previous works highlighted the importance of ensuring efficient and\nresilient communication for the underlying consensus and replication\nmechanisms. However, they were mainly focused on mainstream,\nProof-of-Work-based Distributed Ledger Technologies like Bitcoin or Ethereum.\n  In this paper, the problem is investigated in the context of\nconsensus-validation based blockchains, like the XRP Ledger. The latter relies\non a Federated Byzantine Agreement (FBA) consensus mechanism which is proven to\nhave a good scalability in regards to transaction throughput. However, it is\nknown that significant increases in the size of the XRP Ledger network would be\nchallenging to achieve. The main reason is the flooding mechanism used to\ndisseminate the messages related to the consensus protocol, which creates many\nduplicates in the network. Squelching is a recent solution proposed for\nlimiting this duplication, however, it was never evaluated quantitatively in\nreal-life scenarios involving the XRPL production network. In this paper, our\naim is to assess this mechanism using a real-life controllable testbed and the\nXRPL production network, to assess its benefit and compare it to alternative\nsolutions relying on Named Data Networking and on a gossip-based approach.", "AI": {"tldr": "This paper evaluates squelching \u2014 a duplication-reduction flooding optimization \u2014 for the XRP Ledger (an FBA-based blockchain) using a controllable testbed and the XRPL production network, and compares it to Named Data Networking and gossip-based dissemination alternatives.", "motivation": "While consensus-validation blockchains like XRPL scale well in transaction throughput, their peer-to-peer flooding for consensus messages produces many duplicates that hinder network scaling. Prior work focused on PoW chains; squelching has been proposed but not quantitatively evaluated on the XRPL production network.", "method": "Deploy a real-life controllable testbed interacting with the XRPL production network to measure the impact of squelching. Compare squelching to two alternative approaches \u2014 Named Data Networking (NDN)-based dissemination and a gossip-based method \u2014 using metrics that capture duplication, message latency, bandwidth use, and resilience.", "result": "Quantitative measurements showing how squelching reduces duplicate messages and bandwidth use compared to baseline flooding. Comparative results indicate trade-offs: squelching lowers duplication with modest latency effects, while NDN and gossip approaches offer different bandwidth/latency/resilience profiles.", "conclusion": "Squelching is a practical and effective mitigation for duplicate flooding in XRPL-like FBA networks and can improve scalability; however, alternative designs (NDN, gossip) present viable trade-offs and merit further exploration for larger-scale deployments."}}
