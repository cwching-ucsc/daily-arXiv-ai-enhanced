<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 1]
- [cs.DC](#cs.DC) [Total: 3]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Towards an Agentic Workflow for Internet Measurement Research](https://arxiv.org/abs/2511.10611)
*Alagappan Ramanathan,Eunju Kang,Dongsu Han,Sangeetha Abdu Jyothi*

Main category: cs.NI

TL;DR: ArachNet is a system of four specialized LLM agents that autonomously generate end-to-end Internet measurement workflows by emulating expert decomposition and composition patterns; validated on Internet resilience scenarios, it produces workflows and outputs comparable to specialists and automates complex multi-tool integration.


<details>
  <summary>Details</summary>
Motivation: Internet measurement requires composing many specialized tools and domain expertise; operators need rapid, expert-level diagnostics for disruptions, but building workflows is manual, slow, and expertise-intensive.

Method: ArachNet decomposes measurement tasks into subproblems and assigns them to four agents that sequentially mirror expert workflow stages (problem decomposition → design → implementation → execution/analysis). Agents produce compositional workflows that integrate multiple measurement frameworks and toolchains, with automation of the systematic reasoning experts use.

Result: On progressively harder Internet resilience scenarios, ArachNet independently generated workflows matching expert reasoning and produced analytical outputs similar to specialist solutions; it managed complex multi-framework integration that normally takes days.

Conclusion: Automating expert-style, compositional measurement reasoning with LLM agents can lower barriers to constructing research-quality Internet measurement workflows, enabling faster diagnostics and broader access while retaining technical rigor.

Abstract: Internet measurement research faces an accessibility crisis: complex analyses require custom integration of multiple specialized tools that demands specialized domain expertise. When network disruptions occur, operators need rapid diagnostic workflows spanning infrastructure mapping, routing analysis, and dependency modeling. However, developing these workflows requires specialized knowledge and significant manual effort.
  We present ArachNet, the first system demonstrating that LLM agents can independently generate measurement workflows that mimics expert reasoning. Our core insight is that measurement expertise follows predictable compositional patterns that can be systematically automated. ArachNet operates through four specialized agents that mirror expert workflow, from problem decomposition to solution implementation. We validate ArachNet with progressively challenging Internet resilience scenarios. The system independently generates workflows that match expert-level reasoning and produce analytical outputs similar to specialist solutions. Generated workflows handle complex multi-framework integration that traditionally requires days of manual coordination. ArachNet lowers barriers to measurement workflow composition by automating the systematic reasoning process that experts use, enabling broader access to sophisticated measurement capabilities while maintaining the technical rigor required for research-quality analysis.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [MoFa: A Unified Performance Modeling Framework for LLM Pretraining](https://arxiv.org/abs/2511.09837)
*Lu Zhao,Rong Shi,Shaoqing Zhang,Shangchao Su,Ziqing Yin,Zhiyan Cui,Hongfeng Sun,Baoguo He,Yueqiang Chen,Liang Dong,Xiyuan Li,Lingbin Wang,Lijun Ma,Qiang Huang,Ting Liu,Chong Wang,Can Wei*

Main category: cs.DC

TL;DR: MoFa is a performance modeling and tuning framework for large-scale LLM pretraining that jointly models multi-dimensional parallelization choices and fault-tolerance overheads (e.g., checkpoint recovery), enabling accurate prediction and automated tuning across large cluster scenarios.


<details>
  <summary>Details</summary>
Motivation: As LLMs scale to trillions of parameters, training requires massive distributed clusters and hybrid parallelization. The combinatorial search space of parallel strategies and overlooked overheads (notably fault-tolerance and checkpoint recovery) make manual tuning infeasible and existing models inaccurate.

Method: MoFa builds an enhanced cost model that incorporates key optimization features of hybrid parallelism and a fault-tolerance model informed by historical cluster reliability data. A MoFa-based tuning system searches for optimal configurations and identifies bottlenecks under varied deployment scenarios.

Result: Modeling evaluations show high prediction accuracy across scenarios. Tuning experiments reveal the primary factors affecting pretraining performance for different configurations and demonstrate that considering fault-tolerance changes optimal choices and performance estimates.

Conclusion: MoFa provides a unified approach to model pretraining performance including fault tolerance, enabling better prediction and automated tuning for large-scale LLM pretraining; it offers practical guidance for system design and deployment.

Abstract: The exponential growth in LLM scales, with parameters soaring from billions to trillions, has necessitated distributed pretraining across large clusters comprising thousands to tens of thousands of devices. While hybrid parallelization strategies enable such pretraining, the vast combinatorial strategy space introduces significant optimization challenges. Traditional manual tuning methods incur prohibitive trial-and-error costs, and existing performance modeling approaches exhibit critical limitations: they fail to comprehensively account for prevalent optimization features and ignore the substantial overhead imposed by essential fault tolerance mechanisms like checkpoint recovery in long-duration pretraining. To address these gaps, we propose MoFa, a novel pretraining performance modeling framework that unifies multi-dimensional optimization features and fault tolerance. MoFa incorporates an enhanced cost model to accurately capture the effects of key optimizations and integrates a fault tolerance model based on historical cluster reliability data. Besides, a MoFa-based tuning system is developed to explore optimal pretraining performance and potential bottlenecks in various scenarios. Extensive modeling evaluations demonstrate that MoFa can achieve high prediction accuracy across various scenarios. In addition, through comprehensive tuning experiments, our framework systematically reveals the key factors influencing pretraining performance under different configurations, which provides solid a priori guidance for LLM pretraining system design and deployment.

</details>


### [3] [Lit Silicon: A Case Where Thermal Imbalance Couples Concurrent Execution in Multiple GPUs](https://arxiv.org/abs/2511.09861)
*Marco Kurzynski,Shaizeen Aga,Di Wu*

Main category: cs.DC

TL;DR: This paper identifies a node-level performance variation in multi-GPU LLM training caused by thermal imbalance interacting with concurrent computation-communication (C3). They call it the "Lit Silicon" effect, present analytical performance and power models, propose detection and mitigation, and evaluate three power-management strategies on AMD MI300X systems, reporting up to 6% performance and 4% power improvement.


<details>
  <summary>Details</summary>
Motivation: Large-scale GPU clusters run LLM training where node-level performance variability reduces efficiency. Overlap of computation and communication (C3) exposes kernel-level sensitivity to stragglers created by thermal imbalance across GPUs. The paper seeks to characterize this phenomenon and find low-cost mitigation strategies.

Method: Empirical analysis of single-node multi-GPU LLM training to correlate kernel-level variation with C3 and thermal imbalance. Development of analytical performance and power models for the Lit Silicon effect. Design of light-weight detection and mitigation techniques, and evaluation of three power-management approaches (power optimization under GPU TDP, node-level GPU power capping for performance, and node-level CPU power sloshing) across two workloads and two frameworks on AMD MI300X machines.

Result: Identification and quantification of the Lit Silicon effect. Models indicate potential system-level gains. Experimental results show up to 6% performance improvement and 4% power reduction across tested setups.

Conclusion: Thermally driven straggling coupled with C3 creates node-level inefficiency in multi-GPU training. Simple detection and mitigation at the node power-management layer can recover modest performance and power benefits with minimal deployment cost, offering datacenter-level savings if scaled.

Abstract: GPU systems are increasingly powering modern datacenters at scale. Despite being highly performant, GPU systems suffer from performance variation at the node and cluster levels. Such performance variation significantly impacts both high-performance computing and artificial intelligence workloads, such as cutting-edge large language models (LLMs). We analyze the performance of a single-node multi-GPU system running LLM training, and observe that the kernel-level performance variation is highly correlated with concurrent computation communication (C3), a technique to overlap computation and communication across GPUs for performance gains. We then take a further step to reason that thermally induced straggling coupling with C3 impacts performance variation, coined as the Lit Silicon effect. Lit Silicon describes that in a multi-GPU node, thermal imbalance across GPUs introduces node-level straggler GPUs, which in turn slow down the leader GPUs. Lit Silicon leads to node-level performance variation and inefficiency, impacting the entire datacenter from the bottom up. We propose analytical performance and power models for Lit Silicon, to understand the potential system-level gains. We further design simple detection and mitigation techniques to effectively address the Lit Silicon problem, and evaluate three different power management solutions, including power optimization under GPU thermal design power, performance optimization under node-level GPU power capping, and performance optimization under node-level CPU power sloshing. We conduct experiments on two workloads on two AMD InstinctTM MI300X GPU systems under two LLM training frameworks, and observe up to 6% performance and 4% power improvements, potentially saving hundreds of millions of dollars in datacenters. Our solution is almost free lunch and can be effortlessly adopted in datacenters as a new node-level power management layer.

</details>


### [4] [Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs](https://arxiv.org/abs/2511.10480)
*Changhai Man,Joongun Park,Hanjiang Wu,Huan Xu,Srinivas Sridharan,Tushar Krishna*

Main category: cs.DC

TL;DR: STAGE is a framework that synthesizes high-fidelity, tensor-level execution traces for LLM workloads, enabling scalable exploration of parallelization strategies and system configurations up to 32K GPUs without requiring access to real large-scale infrastructure.


<details>
  <summary>Details</summary>
Motivation: Collecting execution traces from real large-scale systems is limited to major providers and cannot easily model future, larger, or hypothetical configurations. A scalable, expressive way to synthesize traces is needed for pre-deployment optimization and design-space exploration of LLM training and inference systems.

Method: STAGE (Symbolic Tensor grAph GEnerator) generates synthetic execution traces by symbolically modeling tensor computations, memory usage, and communication under a comprehensive set of parallelization strategies across distributed hardware. It produces tensor-level detail for compute, memory, and network operations so traces reflect realistic LLM workloads at scale.

Result: STAGE can synthesize high-fidelity traces that preserve tensor-level accuracy in compute, memory, and communication and scale to simulations representing over 32K GPUs. It enables systematic exploration across many LLM architectures and system configurations.

Conclusion: STAGE provides a publicly available, scalable tool to generate realistic distributed execution traces, enabling pre-deployment optimization and research into distributed ML systems without requiring access to large-scale physical infrastructure.

Abstract: Optimizing the performance of large language models (LLMs) on large-scale AI training and inference systems requires a scalable and expressive mechanism to model distributed workload execution. Such modeling is essential for pre-deployment system-level optimizations (e.g., parallelization strategies) and design-space explorations. While recent efforts have proposed collecting execution traces from real systems, access to large-scale infrastructure remains limited to major cloud providers. Moreover, traces obtained from existing platforms cannot be easily adapted to study future larger-scale system configurations. We introduce Symbolic Tensor grAph GEnerator(STAGE), a framework that synthesizes high-fidelity execution traces to accurately model LLM workloads. STAGE supports a comprehensive set of parallelization strategies, allowing users to systematically explore a wide spectrum of LLM architectures and system configurations. STAGE demonstrates its scalability by synthesizing high-fidelity LLM traces spanning over 32K GPUs, while preserving tensor-level accuracy in compute, memory, and communication. STAGE will be publicly available to facilitate further research in distributed machine learning systems.

</details>
