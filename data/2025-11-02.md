<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for Efficient MoE Inference](https://arxiv.org/abs/2510.26730)
*Zixu Shen,Kexin Chu,Yifan Zhang,Dawei Xiang,Runxin Wu,Wei Zhang*

Main category: cs.DC

TL;DR: ExpertFlow is a runtime system for Mixture-of-Experts (MoE) inference that reduces latency and GPU memory pressure by adaptively prefetching expert parameters and using cache-aware routing. It dynamically adjusts the cross-layer prediction horizon using runtime statistics and fuses pregating signals with intermediate compute states to anticipate which experts will be needed, cutting model stall time to under 0.1% compared to a baseline.


<details>
  <summary>Details</summary>
Motivation: Large language models face GPU memory limits. MoE reduces memory and compute by activating few experts per token, but independent, per-layer expert selection causes frequent host↔GPU parameter transfers and high latency. Fixed-step cross-layer predictions are not robust across hardware and workloads.

Method: ExpertFlow uses adaptive expert prefetching (dynamically adjusting prediction horizon based on runtime metrics like transfer bandwidth, parameter size, and model feedback) and cache-aware routing. It employs a hybrid cross-layer prediction that merges pregating signals with intermediate computational states to better anticipate future expert usage, aligning prefetches with actual needs to reduce cache misses and swap-ins.

Result: In evaluation, ExpertFlow reduced model stall time to less than 0.1% of the baseline and decreased latency caused by expert swap-ins; claimed improvements arise from fewer cache misses and more accurate prefetches.

Conclusion: ExpertFlow offers an adaptable, feedback-driven runtime approach that reduces MoE inference stalls and memory-transfer latency. It appears effective under constrained GPU memory, though further evaluation details and analysis of overheads and generality are needed.

Abstract: The expansion of large language models is increasingly limited by the
constrained memory capacity of modern GPUs. To mitigate this,
Mixture-of-Experts (MoE) architectures activate only a small portion of
parameters during inference, significantly lowering both memory demand and
computational overhead. However, conventional MoE inference approaches, which
select active experts independently at each layer, often introduce considerable
latency because of frequent parameter transfers between host and GPU memory. In
addition, current cross-layer prediction strategies, which are typically based
on fixed steps, lack adaptability across different hardware platforms and
workloads, thereby reducing their robustness and effectiveness.
  To address these challenges, we present ExpertFlow, a runtime system for MoE
inference that combines adaptive expert prefetching and cache-aware routing.
ExpertFlow continuously adjusts its prediction horizon for expert activation by
leveraging runtime statistics such as transfer bandwidth, parameter
dimensionality, and model feedback signals. Furthermore, it incorporates a
hybrid cross-layer prediction scheme that fuses pregating information with
intermediate computational states to anticipate future expert needs. By
adaptively refining prefetching decisions and aligning them with actual usage
behavior, ExpertFlow effectively decreases cache misses and removes latency
caused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces
model stall time to less than 0.1% of the baseline, highlighting its capability
to optimize MoE inference under stringent memory constraints.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [2] [Wireless Memory Approximation for Energy-efficient Task-specific IoT Data Retrieval](https://arxiv.org/abs/2510.26473)
*Junya Shiraishi,Shashi Raj Pandey,Israel Leyva-Mayorga,Petar Popovski*

Main category: cs.NI

TL;DR: Proposes two techniques—wireless memory activation and wireless memory approximation—to reduce DRAM refresh energy for ML model storage on IoT devices, achieving lower energy than always-on while meeting retrieval accuracy constraints.


<details>
  <summary>Details</summary>
Motivation: Periodic DRAM refresh wastes energy during standby on resource-constrained IoT devices used for ML inference; reducing refresh or selectively activating memory can save power without hurting accuracy.

Method: Introduce (1) wireless memory activation: wake DRAM only when needed via a wireless control mechanism considering timing of model use; (2) wireless memory approximation: selectively relax refresh/accuracy trade-offs based on model relevance and timing. Evaluate with numerical experiments comparing energy consumption and retrieval accuracy against an always-on baseline.

Result: Numerical results indicate the proposed scheme consumes less energy than always-on while satisfying a retrieval accuracy constraint; however, details on workloads, baselines, and hardware assumptions are not stated in the abstract.

Conclusion: The approach is promising for lowering standby DRAM energy in IoT ML devices, but the paper needs more experimental detail (hardware validation, energy model, latency/robustness trade-offs) to fully assess practicality and generality.

Abstract: The use of Dynamic Random Access Memory (DRAM) for storing Machine Learning
(ML) models plays a critical role in accelerating ML inference tasks in the
next generation of communication systems. However, periodic refreshment of DRAM
results in wasteful energy consumption during standby periods, which is
significant for resource-constrained Internet of Things (IoT) devices. To solve
this problem, this work advocates two novel approaches: 1) wireless memory
activation and 2) wireless memory approximation. These enable the wireless
devices to efficiently manage the available memory by considering the timing
aspects and relevance of ML model usage; hence, reducing the overall energy
consumption. Numerical results show that our proposed scheme can realize
smaller energy consumption than the always-on approach while satisfying the
retrieval accuracy constraint.

</details>
