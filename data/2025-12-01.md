<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 4]
- [cs.DC](#cs.DC) [Total: 6]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Sensing and Understanding the World over Air: A Large Multimodal Model for Mobile Networks](https://arxiv.org/abs/2511.21707)
*Zhuoran Duan,Yuhao Wei,Guoshun Nan,Zijun Wang,Yan Yan,Lihua Xiong,Yuhan Ran,Ji Zhang,Jian Li,Qimei Cui,Xiaofeng Tao,Tony Q. S. Quek*

Main category: cs.NI

TL;DR: Proposes a wireless-native multimodal large model (WMLM): a GPT-style model trained on large real-world data using wireless signals as an anchor modality with contrastive learning. Reports superior performance versus small-scale and existing multi-modal models and argues wireless signals can serve as a universal modality for future wireless-network intelligence.


<details>
  <summary>Details</summary>
Motivation: Enable unified sensing, communication and intelligence by building domain-specific multi-modal LMs that can understand the physical world through wireless-native data; close the gap since WMLMs are currently underexplored.

Method: Summarizes WMLM characteristics and existing approaches, then proposes a wireless-native multimodal training paradigm. Implements a GPT-style WMLM and trains it on a large real-world dataset, using wireless signals as the anchor modality for contrastive multi-modal alignment.

Result: The trained WMLM outperforms prior small-scale wireless models and larger general multi-modal models on the reported tasks, demonstrating feasibility of using wireless signals as a universal modality for alignment and downstream tasks.

Conclusion: WMLMs are a promising new paradigm for future wireless networks; wireless signals can be an effective anchor modality, and the approach scales to large real-world data, but further research is needed on datasets, benchmarks, and practical deployment.

Abstract: Large models (LMs), such as ChatGPT, have made a significant impact across diverse domains and hold great potential to facilitate the evolution of network intelligence. Wireless-native multi-modal large models (WMLMs) can sense and understand the physical world through multi-modal data, serving as a key enabler that integrates communication, sensing, and intelligence, and thus they can boost various smart services to billions of users. However, research on WMLMs remains in its infancy, and the construction of domain-specific multi-modal large models for wireless networks is still underexplored. In this paper, we outlines the key characteristics of WMLMs and summarizes existing methods, on the basis of which a wireless-native multimodal training paradigm is proposed. Specifically, we constructed a GPT-style WMLM model and trained it on a real-world large-scale dataset, leveraging wireless signals as an anchor modality for contrastive learning. Our approach demonstrates outstanding performance compared with existing small-scale models and large multi-modal models, validating the feasibility of using wireless signals as a universal modality and highlighting WMLM's potential to emerge as a new paradigm for future wireless networks.

</details>


### [2] [Secure Command, Control and Communications Systems (C3) for Army UxVs](https://arxiv.org/abs/2511.21936)
*T. Rebolo,A. Grilo,C. Ribeiro*

Main category: cs.NI

TL;DR: Paper proposes NC2S, a zero-trust secure command-and-control architecture for unmanned vehicles using mTLS (ECDSA/ECDH) and HMAC, with protocols for credential and handover; validated over Wi‑Fi and tactical radios showing higher latency on radios but stable links.


<details>
  <summary>Details</summary>
Motivation: Commercial UxV solutions use insecure protocols (e.g., MAVLink) lacking authentication and encryption, creating vulnerabilities in military and tactical deployments. The authors aim to provide CIA properties while enabling real-time control delegation among commanders, GCSs, and UxVs.

Method: Design and implement NC2S: hierarchical credential-based privileges, mutual TLS with ECDSA certs and ECDH key exchange, HMAC for message integrity; develop lightweight protocols for credential management, key renewal, and control handover; prototype and experimental validation over Wi‑Fi and Rohde&Schwarz HR‑5000H radios.

Result: Prototype shows HR‑5000H tactical radios introduce latencies roughly two orders of magnitude higher than broadband (Wi‑Fi/5G) but maintain stable communication with minimal message loss, supporting NC2S links among commander terminals and GCSs.

Conclusion: NC2S provides a feasible secure C2 architecture for UxVs with zero‑trust controls and robust performance over both broadband and tactical radio links, despite increased latency on tactical radios.

Abstract: Unmanned Vehicles (UxVs) are increasingly used in modern military operations for reconnaissance, surveillance, and strike missions, enhancing situational awareness while reducing risk to personnel. Their affordability and rapid deployment have encouraged the adoption of commercial solutions. However, many rely on insecure protocols such as MAVLink, which lack authentication and encryption mechanisms. This paper designed, implemented, and evaluated a new secure command-and-control architecture that ensures confidentiality, integrity, and authentication (CIA) while supporting real-time control delegation between Ground Control Stations (GCSs). The proposed solution, named New Command and Control System (NC2S), enforces a zero-trust model integrating hierarchical credential-based privileges to regulate access and control among Tactical Commanders (TC), GCSs, and UxVs. It employs mutual Transport Layer Security (mTLS) with Elliptic Curve Digital Signature Algorithm (ECDSA) certificates and Elliptic Curve Diffie-Hellman (ECDH) key exchange, while message integrity is ensured through Hash-based Message Authentication Codes (HMAC). Multiple lightweight protocols were developed for credential management, key renewal, and control handover. The NC2S prototype was experimentally validated over Wi-Fi and Rohde&Schwarz HR-5000H tactical radios. Results showed that HR-5000H links introduce latencies roughly two orders of magnitude higher than broadband technologies (e.g., Wi-Fi or 5G&Beyond technologies) but are still able to maintain stable communication with minimal message loss, making them suitable for the NC2S links among TC terminals and GCSs.

</details>


### [3] [AutoRec: Accelerating Loss Recovery for Live Streaming in a Multi-Supplier Market](https://arxiv.org/abs/2511.22046)
*Tong Li,Xu Yan,Bo Wu,Cheng Luo,Fuyu Wang,Jiuxiang Zhu,Haoyi Fang,Xinle Du,Ke Xu*

Main category: cs.NI

TL;DR: AutoRec is a server-side enhancement to ARQ-based live streaming that leverages frequent on-off streaming behaviors to reduce loss recovery latency without client changes. It permits configurable tradeoffs between recovery latency and overhead, adapts to network dynamics, is implemented on QUIC, and shows positive results in testbed and real deployments.


<details>
  <summary>Details</summary>
Motivation: CDN vendors face constraints upgrading both server and client; hence modern live streaming still relies on ARQ for loss recovery. Retransmission losses inflate recovery latency, which degrades client-side QoE (e.g., video freezing). The goal is to reduce recovery latency without requiring client modifications.

Method: Large-scale measurement of up to 50 million live streams to characterize loss dynamics and on-off mode switching. Design of AutoRec, a server-side mechanism that exploits on-off switching to speed recovery, lets users set overhead vs latency tolerance, and adaptively adjusts strategies to meet latency targets while controlling overhead. Implemented AutoRec on QUIC and evaluated in controlled testbeds and real-world commercial deployments.

Result: Measurements show dynamic loss patterns and frequent on-off switching. AutoRec reduces loss recovery latency while keeping overhead bounded and meets user-configured tolerance levels. Testbed and production deployments demonstrate practicability and profitability (specific quantitative gains not given in the abstract).

Conclusion: AutoRec is a practical, deployable server-side approach to reduce live streaming recovery latency and improve QoE without client changes by adaptively exploiting on-off streaming dynamics.

Abstract: Due to the limited permissions for upgrading dualside (i.e., server-side and client-side) loss tolerance schemes from the perspective of CDN vendors in a multi-supplier market, modern large-scale live streaming services are still using the automatic-repeat-request (ARQ) based paradigm for loss recovery, which only requires server-side modifications. In this paper, we first conduct a large-scale measurement study with up to 50 million live streams. We find that loss shows dynamics and live streaming contains frequent on-off mode switching in the wild. We further find that the recovery latency, enlarged by the ubiquitous retransmission loss, is a critical factor affecting live streaming's client-side QoE (e.g., video freezing). We then propose an enhanced recovery mechanism called AutoRec, which can transform the disadvantages of on-off mode switching into an advantage for reducing loss recovery latency without any modifications on the client side. AutoRec allows users to customize overhead tolerance and recovery latency tolerance and adaptively adjusts strategies as the network environment changes to ensure that recovery latency meets user demands whenever possible while keeping overhead under control. We implement AutoRec upon QUIC and evaluate it via testbed and real-world commercial services deployments. The experimental results demonstrate the practicability and profitability of AutoRec.

</details>


### [4] [Optimizing NetGPT via Routing-Based Synergy and Reinforcement Learning](https://arxiv.org/abs/2511.22217)
*Yuxuan Chen,Rongpeng Li,Xianfu Chen,Celimuge Wu,Chenghui Peng,Zhifeng Zhao,Honggang Zhang*

Main category: cs.NI

TL;DR: Proposes NetGPT, a cloud-edge system that routes structured tool-calling LLM requests between edge and cloud via a provably optimal bandwidth/RTT-dependent threshold policy, while improving the edge model with schema-preserving RL anchored by supervised finetuning. Demonstrates better cost-quality tradeoffs and reduced offloading in experiments.


<details>
  <summary>Details</summary>
Motivation: Edge LLM agents give low latency for routine tasks but lack capability for complex requests; cloud LLMs are better but costlier and slower. The work aims to optimize routing decisions under varying network conditions and to reduce future offloading by strengthening the edge model using cloud responses.

Method: Designs a scoring-based network-aware routing policy and proves the optimal routing has a unique fallback threshold monotone in bandwidth and RTT. Collects cloud-offloaded request-response pairs to train the edge agent using a composite objective: a reverse-KL trust-region step and a forward-KL realignment toward an SFT prior (schema-preserving RL). Updates routing policy and edge agent jointly.

Result: Theory: existence and uniqueness of a fallback threshold with monotone dependence on network parameters. Empirics: experiments across network states and pricing show smooth quality-cost tradeoffs, dynamic thresholds outperform fixed policies, and iterative training reduces offloading while preserving task success and schema correctness.

Conclusion: NetGPT effectively balances latency, cost, and quality via provably structured routing and targeted on-edge improvements, enabling coherent updates that reduce reliance on cloud models without degrading output schema correctness.

Abstract: Large language model (LLM) agents at the network edge offer low-latency execution for routine queries. In contrast, complex requests often require the superior capability of cloud models, incurring higher latency and cost. To navigate this quality-cost trade-off under dynamic network conditions, we propose a cloud-edge synergy for NetGPT that integrates network-aware routing with on-edge self-improvement. Specifically, our framework routes structured tool-calling requests to cloud or edge agents via a novel scoring policy. We prove that, under mild regularity assumptions, the optimal routing rule admits a unique fallback threshold with monotone dependence on bandwidth and round-trip time (RTT). Concurrently, based on the dataset collected from requests routed to the cloud and corresponding responses, we instantiate a schema-preserving reinforcement learning (RL) to improve the capability of the edge agent. We analyze a supervised finetuning (SFT)-anchored composite objective that combines a reverse-KL trust-region step with a forward-KL realignment toward the SFT prior, explaining stability and constraining policy drift. Both the network-aware routing policy and the edge agent are updated coherently. Experiments across controlled network states and pricing schedules demonstrate smooth quality-cost frontiers, consistent gains of dynamic fallback thresholds over fixed policies, and sustained reductions in offloading while maintaining task success and schema-correct outputs.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [OOCO: Latency-disaggregated Architecture for Online-Offline Co-locate LLM Serving](https://arxiv.org/abs/2511.21862)
*Siyu Wu,Zihan Tang,Yuting Zeng,Hui Chen,Guiguang Ding,Tongxuan Liu,Ke Zhang,Hailong Yang*

Main category: cs.DC

TL;DR: They split serving clusters into latency-strict and latency-relaxed pools, schedule tasks with a Roofline-guided bottleneck scheduler, and add fast preemption to enforce online SLOs. This mitigates Prefill/Decode imbalance in disaggregated LLM serving and yields up to 3x offline throughput while preserving online latency SLOs.


<details>
  <summary>Details</summary>
Motivation: Co-locating latency-sensitive online inference and cost-sensitive offline decode workloads raises resource utilization but causes severe Prefill/Decode (P/D) load imbalance in disaggregated systems. Existing dynamic adjustment mechanisms lag behind bursty online traffic and can violate SLOs.

Method: Introduce a latency-constraint disaggregated architecture that partitions cluster resources into latency-strict and latency-relaxed pools. Use a bottleneck-based scheduler guided by a Roofline-like performance model to place tasks according to current bottlenecks, and implement a fast preemption mechanism to immediately reclaim resources for online requests to guarantee SLOs.

Result: On real-world traces, the system improves offline decode throughput by up to 3x compared to existing offline-focused approaches while maintaining online request SLOs. The combination of pool partitioning, bottleneck scheduling, and fast preemption enables flexible placement and protection of latency-critical work.

Conclusion: Partitioning resources by latency requirement and combining model-guided bottleneck scheduling with strict preemption effectively addresses P/D imbalance in disaggregated LLM serving. The design increases offline throughput without sacrificing online latency guarantees, though it depends on accurate bottleneck identification, preemption overheads, and appropriate pool sizing.

Abstract: Large Language Models (LLMs) are increasingly deployed in both latency-sensitive online services and cost-sensitive offline workloads. Co-locating these workloads on shared serving instances can improve resource utilization, but directly applying this approach to Prefill/Decode (P/D) disaggregated systems introduces severe load imbalance, as fluctuating request mixes alter the intrinsic P/D ratio. Existing dynamic adjustment techniques cannot keep up with the bursty traffic patterns of online services.
  We propose a latency-constraint disaggregated architecture, which separates cluster resources into latency-strict and latency-relaxed pools based on task latency requirements. This design enables flexible placement of offline decode tasks, mitigating P/D imbalance while preserving online performance. To fully exploit this flexibility, we propose (1) a bottleneck-based scheduler guided by a Roofline-based performance model for performance bottleneck based scheduling, and (2) a fast preemption mechanism that strictly enforces Service Level Objectives (SLOs) for online requests.
  Experiments on real-world traces show that compared to existing offline system approaches, our method improves offline throughput by up to 3x, while maintaining online request SLOs.

</details>


### [6] [PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel](https://arxiv.org/abs/2511.22333)
*Jinjun Yi,Zhixin Zhao,Yitao Hu,Ke Yan,Weiwei Sun,Hao Wang,Laiping Zhao,Yuhao Zhang,Wenxin Li,Keqiu Li*

Main category: cs.DC

TL;DR: PAT is a prefix-aware attention kernel for LLM decoding that packs queries sharing prefixes, runs a customized multi-tile kernel, and uses multi-stream forwarding and KV splitting with an online merge. As a vLLM plugin it reduces attention latency by ~67% and TPOT by 13.6–83.4% vs state-of-the-art.


<details>
  <summary>Details</summary>
Motivation: Decode-time attention is memory-bound because repeated KV cache loads dominate cost. Real workloads have hierarchical shared prefixes (system prompts, templates, RAG) which prior kernels fail to exploit efficiently: one-query-per-CTA repeats reads, and one-size-fits-all tiling wastes on-chip resources and creates bubbles.

Method: Introduce a pack-forward-merge execution: pack queries by shared prefix to avoid repeated KV loads; execute a customized multi-tile kernel for better resource utilization; apply multi-stream forwarding and KV splitting to reduce idle/bubble time; merge step performs online softmax with low overhead. Implemented as a plugin for vLLM.

Result: On synthetic and real workloads, PAT reduces attention latency by 67.4% on average and improves TPOT by 13.6–83.4% under comparable configurations versus leading attention kernels.

Conclusion: PAT effectively leverages prefix sharing to reduce memory bandwidth pressure and stalls in decode attention, delivering large latency and throughput improvements with practical integration into vLLM.

Abstract: LLM serving is increasingly dominated by decode attention, which is a memory-bound operation due to massive KV cache loading from global memory. Meanwhile, real-world workloads exhibit substantial, hierarchical shared prefixes across requests (e.g., system prompts, tools/templates, RAG). Existing attention implementations fail to fully exploit prefix sharing: *one-query-per-CTA* execution repeatedly loads shared prefix KV cache, while *one-size-fits-all* tiling leaves on-chip resources idle and exacerbates bubbles for uneven KV lengths. These choices amplify memory bandwidth pressure and stall memory-bound decode attention.
  This paper introduces PAT, a prefix-aware attention kernel implementation for LLM decoding that organizes execution with a pack-forward-merge paradigm. PAT packs queries by shared prefix to reduce repeated memory accesses, runs a customized multi-tile kernel to achieve high resource efficiency. It further applies practical multi-stream forwarding and KV splitting to reduce resource bubbles. The final merge performs online softmax with negligible overhead. We implement PAT as an off-the-shelf plugin for vLLM. Evaluation on both real-world and synthetic workloads shows that PAT reduces attention latency by 67.4% on average and TPOT by 13.6-83.4% under the same configurations against state-of-the-art attention kernels.

</details>


### [7] [OmniInfer: System-Wide Acceleration Techniques for Optimizing LLM Serving Throughput and Latency](https://arxiv.org/abs/2511.22481)
*Jun Wang,Yunxiang Yao,Wenwei Kuang,Runze Mao,Zhenhao Sun,Zhuang Tao,Ziyang Zhang,Dengyu Li,Jiajun Chen,Zhili Wang,Kai Cui,Congzhi Cai,Longwen Lan,Ken Zhang*

Main category: cs.DC

TL;DR: OmniInfer is a unified system-level framework that accelerates LLM serving by optimizing expert placement, attention sparsity, and request scheduling. It combines OmniPlacement (Mixture-of-Experts scheduling), OmniAttn (sparse attention acceleration), and OmniProxy (disaggregation-aware scheduling) atop vLLM, achieving large end-to-end gains (616 QPM; TPOT -36%, TTFT -38%) on a 10-node Ascend 910C cluster.


<details>
  <summary>Details</summary>
Motivation: Large Language Models impose heavy compute, strict latency requirements, and throughput limits on serving systems. Existing stacks often optimize isolated components, missing cross-phase, cross-node, and sparsity-aware opportunities for end-to-end improvement.

Method: OmniInfer integrates three coordinated components: (1) OmniPlacement performs load-aware MoE expert placement and scheduling to balance hotspots; (2) OmniAttn accelerates sparse attention and compresses caches to reduce memory/compute overhead during prefill/decode; (3) OmniProxy implements disaggregation-aware request scheduling to coordinate resource use across nodes. The system is implemented on top of vLLM and applies adaptive resource disaggregation and global coordination across prefill and decode phases.

Result: On DeepSeek-R1 using a 10-node Ascend 910C cluster, OmniInfer achieves 616 QPM, reduces TPOT by 36%, and with OmniProxy reduces TTFT by 38%. The project is open-sourced.

Conclusion: OmniInfer demonstrates that joint optimization of expert placement, sparse-attention acceleration, and request scheduling can substantially improve end-to-end LLM serving efficiency; the open-source release supports broader validation and adoption.

Abstract: Large Language Models drive a wide range of modern AI applications but impose substantial challenges on large-scale serving systems due to intensive computation, strict latency constraints, and throughput bottlenecks. We introduce OmniInfer, a unified system-level acceleration framework designed to maximize end-to-end serving efficiency through fine-grained optimization of expert placement, cache compression, and scheduling. OmniInfer integrates three complementary components: OmniPlacement for load-aware Mixture-of-Experts scheduling, OmniAttn for sparse attention acceleration, and OmniProxy for disaggregation-aware request scheduling. Built atop vLLM, OmniInfer delivers system-wide performance gains through adaptive resource disaggregation, efficient sparsity exploitation, and global coordination across prefill and decode phases. Evaluated on DeepSeek-R1 within a 10-node Ascend 910C cluster, OmniInfer achieves 616 QPM, where the unified framework reduces TPOT by 36\%, and the superimposition of OmniProxy further slashes TTFT by 38\%. The project is open-sourced at [this https URL](https://gitee.com/omniai/omniinfer).

</details>


### [8] [DisCEdge: Distributed Context Management for Large Language Models at the Edge](https://arxiv.org/abs/2511.22599)
*Mohammadreza Malekabbasi,Minghe Wang,David Bermbach*

Main category: cs.DC

TL;DR: DisCEdge stores and replicates user context in tokenized form across geo-distributed edge nodes to reduce redundant computation and network overhead. An open-source prototype on commodity hardware shows up to 14.46% median response-time improvement, up to 15% reduction in inter-node sync overhead versus raw-text replication, and a 90% median reduction in client request size versus client-side storage while maintaining data consistency.


<details>
  <summary>Details</summary>
Motivation: Edge deployment of LLM services improves latency and privacy, but stateless LLMs make cross-node user-context management hard. Client-side context storage causes extra network latency/bandwidth and undermines edge benefits. A more efficient, consistent, distributed context store is needed.

Method: DisCEdge tokenizes user context and stores token sequences on edge nodes, replicating these sequences across nodes rather than raw text. Token-level storage avoids repeated tokenization and reduces data volume for replication. The authors implement an open-source prototype and evaluate it in a realistic geo-distributed edge setup on commodity hardware.

Result: Compared to a raw-text-based distributed context system, DisCEdge improves median response times by up to 14.46% and lowers median inter-node synchronization overhead by up to 15%. Compared to client-side context storage, it reduces median client request size by ~90%. The system also claims to guarantee data consistency.

Conclusion: Tokenized, replicated context storage at the edge is an effective way to reduce latency, bandwidth, and synchronization overhead for LLM services while maintaining consistency; however, practical challenges around tokenizer-dependence, model-versioning, security, and the exact consistency model need clarification and further evaluation.

Abstract: Deploying Large Language Model (LLM) services at the edge benefits latency-sensitive and privacy-aware applications. However, the stateless nature of LLMs makes managing user context (e.g., sessions, preferences) across geo-distributed edge nodes challenging. Existing solutions, such as client-side context storage, often introduce network latency and bandwidth overhead, undermining the advantages of edge deployment.
  We propose DisCEdge, a distributed context management system that stores and replicates user context in tokenized form across edge nodes. By maintaining context as token sequences rather than raw text, our system avoids redundant computation and enables efficient data replication. We implement and evaluate an open-source prototype in a realistic edge environment with commodity hardware. We show DisCEdge improves median response times by up to 14.46% and lowers median inter-node synchronization overhead by up to 15% compared to a raw-text-based system. It also reduces client request sizes by a median of 90% compared to client-side context management, while guaranteeing data consistency.

</details>


### [9] [Serving Heterogeneous LoRA Adapters in Distributed LLM Inference Systems](https://arxiv.org/abs/2511.22880)
*Shashwat Jaiswal,Shrikara Arun,Anjaly Parayil,Ankur Mallick,Spyros Mastorakis,Alind Khare,Chloi Alverti,Renee St Amant,Chetan Bansal,Victor Rühle,Josep Torrellas*

Main category: cs.DC

TL;DR: LoRAServe dynamically rebalances heterogeneous LoRA adapters across GPUs and uses GPU Direct RDMA for remote access to mitigate rank-induced performance skew in multi-tenant LLM serving, achieving up to 2× throughput, up to 9× lower TTFT, and up to 50% GPU savings on production traces.


<details>
  <summary>Details</summary>
Motivation: LoRA adapters vary in rank (size) in large multi-tenant deployments; co-batching heterogeneous adapters without accounting for rank causes severe performance skew, underutilized GPUs, and SLO misses. Existing serving optimizations don’t address this heterogeneity.

Method: A workload-aware framework that monitors adapter usage and rank distribution, dynamically places and rebalances adapters across GPUs to equalize load, and routes requests to local or remote adapters using GPU Direct RDMA to access remote adapter weights with low overhead.

Result: On production traces from Company X, LoRAServe increases throughput up to 2×, reduces time-to-first-token (TTFT) up to 9×, and lowers GPU count by up to 50% under SLO constraints compared to state-of-the-art serving systems.

Conclusion: Targeting rank diversity with dynamic placement and RDMA-based remote access markedly improves efficiency and latency for multi-tenant LoRA serving; however, trade-offs remain around migration overhead, network load, and system complexity that should be quantified.

Abstract: Low-Rank Adaptation (LoRA) has become the de facto method for parameter-efficient fine-tuning of large language models (LLMs), enabling rapid adaptation to diverse domains. In production, LoRA-based models are served at scale, creating multi-tenant environments with hundreds of adapters sharing a base model. However, state-of-the-art serving systems co-batch heterogeneous adapters without accounting for rank (size) variability, leading to severe performance skew, which ultimately requires adding more GPUs to satisfy service-level objectives (SLOs). Existing optimizations, focused on loading, caching, and kernel execution, ignore this heterogeneity, leaving GPU resources underutilized. We present LoRAServe, a workload-aware dynamic adapter placement and routing framework designed to tame rank diversity in LoRA serving. By dynamically rebalancing adapters across GPUs and leveraging GPU Direct RDMA for remote access, LoRAServe maximizes throughput and minimizes tail latency under real-world workload drift. Evaluations on production traces from Company X show that LoRAServe elicits up to 2$\times$ higher throughput, up to 9$\times$ lower TTFT, while using up to 50% fewer GPUs under SLO constraints compared to state-of-the-art systems.

</details>


### [10] [Communication-Computation Pipeline Parallel Split Learning over Wireless Edge Networks](https://arxiv.org/abs/2511.23167)
*Chenyu Liu,Zhaoyang Zhang,Zirui Chen,Zhaohui Yang*

Main category: cs.DC

TL;DR: The paper proposes C^2P^2SL, which applies pipeline parallelism to split learning in wireless networks by splitting batches into micro-batches and overlapping communication and computation between UEs and BS. It formulates joint optimization of cut layer and resource allocation, solves it via alternating optimization, and shows over 38% reduction in training time while preserving accuracy.


<details>
  <summary>Details</summary>
Motivation: Split learning preserves user data privacy but is sequential in its communication-computation steps, which limits efficiency in wireless settings with heterogeneous UEs. The motivation is to accelerate SL by exploiting parallelism to overlap communication and computation and reduce training time.

Method: Introduce communication-computation pipeline parallel split learning (C^2P^2SL): split each batch into micro-batches and treat UE and BS tasks as a pipeline so communication and computation overlap across micro-batches. Model heterogeneity and cut-layer position, formulate a joint optimization problem for task splitting and resource allocation, and solve it via alternating optimization.

Result: Experimental results claim C^2P^2SL reduces system training time by over 38% while maintaining convergence accuracy across different communication conditions.

Conclusion: Applying pipeline parallelism to split learning in wireless networks effectively improves training efficiency. Jointly optimizing cut layer and resource allocation further adapts to UE heterogeneity, yielding significant training time reductions without hurting accuracy.

Abstract: Split learning (SL) offloads main computing tasks from multiple resource-constrained user equippments (UEs) to the base station (BS), while preserving local data privacy. However, its computation and communication processes remain sequential, resulting in limited system efficiency. To overcome this limitation, this paper applies pipeline parallelism (PP) of distributed training to SL in wireless networks, proposing the so-called communication-computation pipeline parallel split learning (C$^2$P$^2$SL). By considering the communicating and computing processes of UEs and BS as an overall pipeline, C$^2$P$^2$SL achieves pipeline parallelization among different micro-batches which are split from each batch of data samples. The overlap of communication and computation in this way significantly reduces the total training time. Given that training efficiency is affected by position of cutting layer and heterogeneity of the UEs, we formulate a joint optimization problem of task split and resource allocation, and design a solution based on alternating optimization. Experimental results demonstrate that C$^2$P$^2$SL significantly reduces system training time by over 38\% while maintaining convergence accuracy under different communication conditions.

</details>
