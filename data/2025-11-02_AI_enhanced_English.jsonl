{"id": "2510.26730", "categories": ["cs.DC", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.26730", "abs": "https://arxiv.org/abs/2510.26730", "authors": ["Zixu Shen", "Kexin Chu", "Yifan Zhang", "Dawei Xiang", "Runxin Wu", "Wei Zhang"], "title": "ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for Efficient MoE Inference", "comment": "12 pages, 11 figures", "summary": "The expansion of large language models is increasingly limited by the\nconstrained memory capacity of modern GPUs. To mitigate this,\nMixture-of-Experts (MoE) architectures activate only a small portion of\nparameters during inference, significantly lowering both memory demand and\ncomputational overhead. However, conventional MoE inference approaches, which\nselect active experts independently at each layer, often introduce considerable\nlatency because of frequent parameter transfers between host and GPU memory. In\naddition, current cross-layer prediction strategies, which are typically based\non fixed steps, lack adaptability across different hardware platforms and\nworkloads, thereby reducing their robustness and effectiveness.\n  To address these challenges, we present ExpertFlow, a runtime system for MoE\ninference that combines adaptive expert prefetching and cache-aware routing.\nExpertFlow continuously adjusts its prediction horizon for expert activation by\nleveraging runtime statistics such as transfer bandwidth, parameter\ndimensionality, and model feedback signals. Furthermore, it incorporates a\nhybrid cross-layer prediction scheme that fuses pregating information with\nintermediate computational states to anticipate future expert needs. By\nadaptively refining prefetching decisions and aligning them with actual usage\nbehavior, ExpertFlow effectively decreases cache misses and removes latency\ncaused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces\nmodel stall time to less than 0.1% of the baseline, highlighting its capability\nto optimize MoE inference under stringent memory constraints.", "AI": {"tldr": "ExpertFlow is a runtime system for Mixture-of-Experts (MoE) inference that reduces latency and GPU memory pressure by adaptively prefetching expert parameters and using cache-aware routing. It dynamically adjusts the cross-layer prediction horizon using runtime statistics and fuses pregating signals with intermediate compute states to anticipate which experts will be needed, cutting model stall time to under 0.1% compared to a baseline.", "motivation": "Large language models face GPU memory limits. MoE reduces memory and compute by activating few experts per token, but independent, per-layer expert selection causes frequent host\u2194GPU parameter transfers and high latency. Fixed-step cross-layer predictions are not robust across hardware and workloads.", "method": "ExpertFlow uses adaptive expert prefetching (dynamically adjusting prediction horizon based on runtime metrics like transfer bandwidth, parameter size, and model feedback) and cache-aware routing. It employs a hybrid cross-layer prediction that merges pregating signals with intermediate computational states to better anticipate future expert usage, aligning prefetches with actual needs to reduce cache misses and swap-ins.", "result": "In evaluation, ExpertFlow reduced model stall time to less than 0.1% of the baseline and decreased latency caused by expert swap-ins; claimed improvements arise from fewer cache misses and more accurate prefetches.", "conclusion": "ExpertFlow offers an adaptable, feedback-driven runtime approach that reduces MoE inference stalls and memory-transfer latency. It appears effective under constrained GPU memory, though further evaluation details and analysis of overheads and generality are needed."}}
{"id": "2510.26473", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.26473", "abs": "https://arxiv.org/abs/2510.26473", "authors": ["Junya Shiraishi", "Shashi Raj Pandey", "Israel Leyva-Mayorga", "Petar Popovski"], "title": "Wireless Memory Approximation for Energy-efficient Task-specific IoT Data Retrieval", "comment": null, "summary": "The use of Dynamic Random Access Memory (DRAM) for storing Machine Learning\n(ML) models plays a critical role in accelerating ML inference tasks in the\nnext generation of communication systems. However, periodic refreshment of DRAM\nresults in wasteful energy consumption during standby periods, which is\nsignificant for resource-constrained Internet of Things (IoT) devices. To solve\nthis problem, this work advocates two novel approaches: 1) wireless memory\nactivation and 2) wireless memory approximation. These enable the wireless\ndevices to efficiently manage the available memory by considering the timing\naspects and relevance of ML model usage; hence, reducing the overall energy\nconsumption. Numerical results show that our proposed scheme can realize\nsmaller energy consumption than the always-on approach while satisfying the\nretrieval accuracy constraint.", "AI": {"tldr": "Proposes two techniques\u2014wireless memory activation and wireless memory approximation\u2014to reduce DRAM refresh energy for ML model storage on IoT devices, achieving lower energy than always-on while meeting retrieval accuracy constraints.", "motivation": "Periodic DRAM refresh wastes energy during standby on resource-constrained IoT devices used for ML inference; reducing refresh or selectively activating memory can save power without hurting accuracy.", "method": "Introduce (1) wireless memory activation: wake DRAM only when needed via a wireless control mechanism considering timing of model use; (2) wireless memory approximation: selectively relax refresh/accuracy trade-offs based on model relevance and timing. Evaluate with numerical experiments comparing energy consumption and retrieval accuracy against an always-on baseline.", "result": "Numerical results indicate the proposed scheme consumes less energy than always-on while satisfying a retrieval accuracy constraint; however, details on workloads, baselines, and hardware assumptions are not stated in the abstract.", "conclusion": "The approach is promising for lowering standby DRAM energy in IoT ML devices, but the paper needs more experimental detail (hardware validation, energy model, latency/robustness trade-offs) to fully assess practicality and generality."}}
