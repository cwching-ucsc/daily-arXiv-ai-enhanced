<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 2]
- [cs.DC](#cs.DC) [Total: 4]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [A New Broadcast Model for Several Network Topologies](https://arxiv.org/abs/2510.18058)
*Hongbo Lu,Junsung Hwang,Bernard Tenreiro,Nabila Jaman Tripti,Darren Hamilton,Yuefan Deng*

Main category: cs.NI

TL;DR: 提出一種名為 Broadcast by Balanced Saturation (BBS) 的廣播演算法，透過平衡飽和（balanced saturation）與精確通訊週期維持節點活躍度，旨在提高各種網路拓撲下的通訊效率與降低延遲，模擬結果顯示在多數拓撲上優於一般廣播演算法。


<details>
  <summary>Details</summary>
Motivation: 在大規模系統（如超級電腦）中，廣播面臨拓撲限制、頻寬瓶頸與同步開銷。目標是最大化節點利用率並持續保持節點活動以加速資料傳播，從而降低整體延遲。

Method: 設計一個可重複、步驟化的通訊週期（precise communication cycle），以平衡飽和策略安排節點發送與接收，確保每個通訊循環中節點保持高利用率並有效分配頻寬與傳播路徑。透過模擬在多種拓撲上驗證效能。

Result: 模擬結果指出 BBS 在不同拓撲下通常顯著優於常見的一般廣播演算法（延遲降低、節點利用度提升），在多個案例中有大幅改進。

Conclusion: BBS 提供一個通用且具韌性的廣播框架，能在多樣拓撲中提升廣播效能，具有取代或重塑既有廣播策略的潛力。

Abstract: We present Broadcast by Balanced Saturation (BBS), a general broadcast
algorithm designed to optimize communication efficiency across diverse network
topologies. BBS maximizes node utilization, addressing challenges in broadcast
operations such as topology constraints, bandwidth limitations, and
synchronization overhead, particularly in large-scale systems like
supercomputers. The algorithm ensures sustained activity with nodes throughout
the broadcast, thereby enhancing data propagation and significantly reducing
latency. Through a precise communication cycle, BBS provides a repeatable,
streamlined, stepwise broadcasting framework. Simulation results across various
topologies demonstrate that the BBS algorithm consistently outperforms common
general broadcast algorithms, often by a substantial margin. These findings
suggest that BBS is a versatile and robust framework with the potential to
redefine broadcast strategies across network topologies.

</details>


### [2] [On AI Verification in Open RAN](https://arxiv.org/abs/2510.18417)
*Rahul Soundrarajan,Claudio Fiandrino,Michele Polese,Salvatore D'Oro,Leonardo Bonati,Tommaso Melodia*

Main category: cs.NI

TL;DR: Lightweight DT-based verifiers provide near-real-time consistency checks for DRL agents in Open RAN slicing/scheduling, trading off some verification completeness for runtime feasibility.


<details>
  <summary>Details</summary>
Motivation: Open RAN enables AI-driven control (DRL) across heterogeneous, multi-vendor RAN; however, opaque AI decisions threaten reliability of network operations and require verification beyond XAI explanations.

Method: Use interpretable Decision Tree models as runtime verifiers to validate DRL agent actions for slicing and scheduling. Integrate DT-verifier into an Open RAN scalable architecture to run near-real-time consistency checks; compare with computationally expensive state-of-the-art verifiers and demonstrate feasibility with a DT-based slice-verifier.

Result: Feasibility demonstrated: DT-based verifiers can perform near-real-time checks that are otherwise infeasible with heavy verifiers. The paper provides architectural integration and a working slice-verifier prototype/evaluation.

Conclusion: DT-based, interpretable verifiers are a practical step toward trustworthy AI in Open RAN, but future challenges remain (scalability, robustness, completeness of verification, maintenance under non-stationarity).

Abstract: Open RAN introduces a flexible, cloud-based architecture for the Radio Access
Network (RAN), enabling Artificial Intelligence (AI)/Machine Learning
(ML)-driven automation across heterogeneous, multi-vendor deployments. While
EXplainable Artificial Intelligence (XAI) helps mitigate the opacity of AI
models, explainability alone does not guarantee reliable network operations. In
this article, we propose a lightweight verification approach based on
interpretable models to validate the behavior of Deep Reinforcement Learning
(DRL) agents for RAN slicing and scheduling in Open RAN. Specifically, we use
Decision Tree (DT)-based verifiers to perform near-real-time consistency checks
at runtime, which would be otherwise unfeasible with computationally expensive
state-of-the-art verifiers. We analyze the landscape of XAI and AI
verification, propose a scalable architectural integration, and demonstrate
feasibility with a DT-based slice-verifier. We also outline future challenges
to ensure trustworthy AI adoption in Open RAN.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Deploying Atmospheric and Oceanic AI Models on Chinese Hardware and Framework: Migration Strategies, Performance Optimization and Analysis](https://arxiv.org/abs/2510.17852)
*Yuze Sun,Wentao Luo,Yanfei Xiang,Jiancheng Pan,Jiahao Li,Quan Zhang,Xiaomeng Huang*

Main category: cs.DC

TL;DR: 提出一個將大型大氣與海洋 AI 模型從 PyTorch 遷移到 MindSpore，並針對國產處理器進行軟硬體適配、記憶體優化與並行化的框架，並與 GPU 實現進行多指標比較。


<details>
  <summary>Details</summary>
Motivation: 現有氣候與天氣 AI 模型高度依賴 GPU 與外國生態系，限制了在國內硬體與框架上的可移植性與自主性；因此需要一套能在國產芯片上高效訓練與推理的遷移與優化流程。

Method: 設計軟硬體適配層（包括運算核替換與算子實現）、記憶體與通訊優化（內存佈局、checkpoint、混合精度）、以及並行化策略（數據/模型並行與分佈式通信優化）；將模型從 PyTorch 轉譯到 MindSpore，並在國產芯片上進行微調與基準測試。

Result: 實驗顯示在不降低原模型精度的前提下，遷移與優化後能在國產芯片上達到可比較的訓練/推理效率並提升系統依賴性與能效表現，證明了國產硬體作為科學計算替代方案的可行性。

Conclusion: 該框架提供了實務指引與經驗，幫助將氣象海洋領域的大型 AI 模型部署到國內生態系統，促進技術自主並提升運行效率。

Abstract: With the growing role of artificial intelligence in climate and weather
research, efficient model training and inference are in high demand. Current
models like FourCastNet and AI-GOMS depend heavily on GPUs, limiting hardware
independence, especially for Chinese domestic hardware and frameworks. To
address this issue, we present a framework for migrating large-scale
atmospheric and oceanic models from PyTorch to MindSpore and optimizing for
Chinese chips, and evaluating their performance against GPUs. The framework
focuses on software-hardware adaptation, memory optimization, and parallelism.
Furthermore, the model's performance is evaluated across multiple metrics,
including training speed, inference speed, model accuracy, and energy
efficiency, with comparisons against GPU-based implementations. Experimental
results demonstrate that the migration and optimization process preserves the
models' original accuracy while significantly reducing system dependencies and
improving operational efficiency by leveraging Chinese chips as a viable
alternative for scientific computing. This work provides valuable insights and
practical guidance for leveraging Chinese domestic chips and frameworks in
atmospheric and oceanic AI model development, offering a pathway toward greater
technological independence.

</details>


### [4] [Efficient Multi-Worker Selection based Distributed Swarm Learning via Analog Aggregation](https://arxiv.org/abs/2510.18152)
*Zhuoyu Yao,Yue Wang,Songyang Zhang,Yingshu Li,Zhipeng Cai,Zhi Tian*

Main category: cs.DC

TL;DR: 提出 DSL-OTA：將類比空中（over-the-air）聚合整合到分散式群體學習（DSL），透過多工作者選擇提升通信效率、協作性與隱私保護，並在理論與模擬上展示更快收斂與更低通信成本。


<details>
  <summary>Details</summary>
Motivation: Federated/Swarm learning 在邊緣裝置協同訓練時受限於傳輸資源與複雜通道環境；現有 DSL 常仰賴單一最佳工作者，造成效能與隱私風險。需要一種能減少通信負擔、加強協作並保護資料的方法。

Method: 提出 DSL-OTA：利用 OTA 類比訊號疊加做模型參數聚合，結合多工作者選擇策略取代單一貢獻者，並設計相應的同步、功率控制與隱私保護機制。理論上推導收斂性與通信成本分析。

Result: 理論證明 DSL-OTA 在收斂速度與通信效率上具優勢；模擬顯示在同質與異質資料分布下，DSL-OTA 相較現有方法可達到更好的學習效能與通信節省。

Conclusion: DSL-OTA 能有效降低通信成本、加快模型收斂並提高協作及隱私性，是在資源受限邊緣網路中實現群體學習的可行方案。

Abstract: Recent advances in distributed learning systems have introduced effective
solutions for implementing collaborative artificial intelligence techniques in
wireless communication networks. Federated learning approaches provide a
model-aggregation mechanism among edge devices to achieve collaborative
training, while ensuring data security, communication efficiency, and sharing
computational overheads. On the other hand, limited transmission resources and
complex communication environments remain significant bottlenecks to the
efficient collaborations among edge devices, particularly within large-scale
networks. To address such issues, this paper proposes an over-the-air (OTA)
analog aggregation method designed for the distributed swarm learning (DSL),
termed DSL-OTA, aiming to enhance communication efficiency, enable effective
cooperation, and ensure privacy preserving. Incorporating multi-worker
selection strategy with over-the-air aggregation not only makes the standard
DSL based on single best worker contributing to global model update to become
more federated, but also secures the aggregation from potential risks of data
leakage. Our theoretical analyses verify the advantages of the proposed DSL-OTA
algorithm in terms of fast convergence rate and low communication costs.
Simulation results reveal that our DSL-OTA outperforms the other existing
methods by achieving better learning performance under both homogeneous and
heterogeneous dataset settings.

</details>


### [5] [A Distributed Framework for Causal Modeling of Performance Variability in GPU Traces](https://arxiv.org/abs/2510.18300)
*Ankur Lahiry,Ayush Pokharel,Banooqa Banday,Seth Ockerman,Amal Gueroudji,Mohammad Zaeed,Tanzima Z. Islam,Line Pouchard*

Main category: cs.DC

TL;DR: 提出一個端到端並行效能分析框架，能同時分割與處理多個大型 GPU trace，結合因果圖 (causal graph) 與並行協調圖表來揭示執行流程間的效能變異與相依性，實驗顯示在可擴充性上提升約 67%。


<details>
  <summary>Details</summary>
Motivation: 單一大型 GPU trace 體積龐大且結構複雜，逐一分析耗時且計算成本高。隨著需要分析多筆 trace 的情況日益常見（例如大規模叢集或長期監控），需要一套能夠並行處理多筆 trace、降低延遲並保留依賴資訊的框架。

Method: 對 trace 資料進行分割（partition）後並行處理；利用因果圖方法抽取事件間的相依關係，並以所謂的「並行協調圖表」（parallel coordinating chart）來呈現與比較不同行程或不同 trace 的性能差異與變異來源。整體流程為端到端管線化以利擴充與獨立分析多筆 trace。

Result: 實驗結果指出此管線在可擴充性上相較基準方法提升約 67%，並能獨立分析多個大型 trace，揭示了執行流程間的性能變異與相依性。

Conclusion: 所提框架能有效提升多筆大型 GPU trace 的分析可擴充性並提供因果/協調視角來理解性能瓶頸，為大規模異質 HPC 平台的效能偵錯與優化提供實用工具。

Abstract: Large-scale GPU traces play a critical role in identifying performance
bottlenecks within heterogeneous High-Performance Computing (HPC)
architectures. However, the sheer volume and complexity of a single trace of
data make performance analysis both computationally expensive and
time-consuming. To address this challenge, we present an end-to-end parallel
performance analysis framework designed to handle multiple large-scale GPU
traces efficiently. Our proposed framework partitions and processes trace data
concurrently and employs causal graph methods and parallel coordinating chart
to expose performance variability and dependencies across execution flows.
Experimental results demonstrate a 67% improvement in terms of scalability,
highlighting the effectiveness of our pipeline for analyzing multiple traces
independently.

</details>


### [6] [SLICE: SLO-Driven Scheduling for LLM Inference on Edge Computing Devices](https://arxiv.org/abs/2510.18544)
*Pan Zhou,Yiming Lei,Ling Liu,Xiaoqiong Xu,Ying Cai,Daji Ergu,Hongfang Yu,Yueyue Dai*

Main category: cs.DC

TL;DR: 提出 SLICE，一種針對邊緣設備差異化 SLO（TTFT、TPOT、端到端延遲）設計的排程與生成速率動態控制方案，可大幅提升 LLM 推理的 SLO 達成率。


<details>
  <summary>Details</summary>
Motivation: 邊緣設備上的應用（如機器人、車輛）有嚴苛且多樣的延遲需求（TTFT、TPOT、端到端延遲），而現有排程系統僅以最大化輸出 token 吞吐量為目標，導致對延遲敏感任務的 SLO 違規率高。

Method: 提出 SLICE：結合一個以效用最大化為目標的請求排程演算法與一個動態迭代的生成速率控制機制，將不同 SLO 需求納入排程與速率調節，實現對生成過程的細粒度控制與優先權分配。

Result: 實驗顯示，與 Orca 和 FastServe 相比，SLICE 在 SLO 達成率上最高可達 35× 優勢，任務完畢時間上約 3.4× 改善，對延遲敏感工作負載有明顯提升。

Conclusion: SLICE 有效解決邊緣 LLM 服務的差異化 SLO 問題，透過排程與生成速率協同降低延遲違規；後續可探討多模型、多裝置協調、能耗與系統開銷等延伸議題。

Abstract: Large Language Models (LLMs), as the foundational architecture for
next-generation interactive AI applications, not only power intelligent
dialogue systems but also drive the evolution of embodied intelligence on edge
devices, including humanoid robots, smart vehicles, and other scenarios. The
applications running on these edge devices impose differentiated Service Level
Objectives (SLO) requirements on LLM services, specifically manifested as
distinct constraints on Time to First Token (TTFT) and Time Per Output Token
(TPOT) as well as end-to-end latency. Notably, edge devices typically handle
real-time tasks that are extremely sensitive to latency, such as machine
control and navigation planning. However, existing scheduling service systems
still prioritize maximizing output token throughput as the sole optimization
objective, failing to adequately address the diversity of SLO requirements.
This ultimately results in persistently high violation rates for end-to-end
latency or TPOT related SLOs.
  This paper proposes SLICE, an innovative scheduling solution designed for
edge computing scenarios with differentiated SLO requirements. By combining a
utility-maximizing request scheduling algorithm with a dynamic iterative
control mechanism for generation rates, SLICE significantly improves LLM
inference service SLO attainment. Experimental results demonstrate that
compared to state-of-the-art solutions Orca and FastServe, SLICE achieves up to
35x higher SLO attainment and 3.4x advantage in task completion time than the
other two solutions.

</details>
