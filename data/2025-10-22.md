<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 2]
- [cs.DC](#cs.DC) [Total: 2]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [A New Broadcast Model for Several Network Topologies](https://arxiv.org/abs/2510.18058)
*Hongbo Lu,Junsung Hwang,Bernard Tenreiro,Nabila Jaman Tripti,Darren Hamilton,Yuefan Deng*

Main category: cs.NI

TL;DR: 提出一種名為 BBS（Broadcast by Balanced Saturation）的廣播演算法，透過平衡節點利用率與精確通訊週期來提升不同網路拓樸下的廣播效率，模擬結果顯示普遍優於常見廣播演算法。


<details>
  <summary>Details</summary>
Motivation: 解決大型系統（如超級電腦）中廣播面臨的拓樸限制、頻寬瓶頸與同步開銷，並提升節點在整個廣播過程中的活躍度以縮短延遲。

Method: 設計一個具重複性且步驟化的通訊週期，使節點持續參與資料轉播以達到飽和且平衡的資源使用（balanced saturation）；在多種拓樸上以模擬實驗驗證效能。

Result: 模擬顯示 BBS 在多種網路拓樸上，通常以顯著幅度優於一般的通用廣播演算法，延遲降低且節點利用率提高。

Conclusion: BBS 是一套具通用性與韌性的廣播框架，有潛力改進不同拓樸下的廣播策略，使大型系統的資料傳播更高效。

Abstract: We present Broadcast by Balanced Saturation (BBS), a general broadcast
algorithm designed to optimize communication efficiency across diverse network
topologies. BBS maximizes node utilization, addressing challenges in broadcast
operations such as topology constraints, bandwidth limitations, and
synchronization overhead, particularly in large-scale systems like
supercomputers. The algorithm ensures sustained activity with nodes throughout
the broadcast, thereby enhancing data propagation and significantly reducing
latency. Through a precise communication cycle, BBS provides a repeatable,
streamlined, stepwise broadcasting framework. Simulation results across various
topologies demonstrate that the BBS algorithm consistently outperforms common
general broadcast algorithms, often by a substantial margin. These findings
suggest that BBS is a versatile and robust framework with the potential to
redefine broadcast strategies across network topologies.

</details>


### [2] [On AI Verification in Open RAN](https://arxiv.org/abs/2510.18417)
*Rahul Soundrarajan,Claudio Fiandrino,Michele Polese,Salvatore D'Oro,Leonardo Bonati,Tommaso Melodia*

Main category: cs.NI

TL;DR: 提出以決策樹為基礎的輕量驗證器，在 Open RAN 中對深度強化學習的切片與排程行為進行近實時一致性檢查，藉此補強 XAI 解釋性但不足以保證網路可靠性的缺口。


<details>
  <summary>Details</summary>
Motivation: Open RAN 引入多供應商、雲化與 AI/ML 自動化，使得 DRL 等黑箱模型在關鍵的 RAN 切片與排程決策中被採用；然而僅有可解釋性（XAI）仍不足以確保運作正確與安全，需有可執行的行為驗證機制。

Method: 提出以可解釋模型（Decision Tree）作為輕量驗證器，在運行時對 DRL agent 的決策執行近即時的一致性檢查；分析 XAI 與 AI 驗證的現況，設計可擴展的架構整合方案，並以 DT-based slice-verifier 實作驗證可行性。

Result: 展示 DT 驗證器能在近實時執行一致性檢查，較市面上計算昂貴的驗證器更適合運行時使用；驗證器與系統架構展現出可擴充性與實務可行性。

Conclusion: 以可解釋的輕量驗證器補強 XAI，能提高 Open RAN 中 DRL 決策的可靠度；但需面對模型表達力、驗證準確度、攻擊防護與整合標準化等未來挑戰。

Abstract: Open RAN introduces a flexible, cloud-based architecture for the Radio Access
Network (RAN), enabling Artificial Intelligence (AI)/Machine Learning
(ML)-driven automation across heterogeneous, multi-vendor deployments. While
EXplainable Artificial Intelligence (XAI) helps mitigate the opacity of AI
models, explainability alone does not guarantee reliable network operations. In
this article, we propose a lightweight verification approach based on
interpretable models to validate the behavior of Deep Reinforcement Learning
(DRL) agents for RAN slicing and scheduling in Open RAN. Specifically, we use
Decision Tree (DT)-based verifiers to perform near-real-time consistency checks
at runtime, which would be otherwise unfeasible with computationally expensive
state-of-the-art verifiers. We analyze the landscape of XAI and AI
verification, propose a scalable architectural integration, and demonstrate
feasibility with a DT-based slice-verifier. We also outline future challenges
to ensure trustworthy AI adoption in Open RAN.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Deploying Atmospheric and Oceanic AI Models on Chinese Hardware and Framework: Migration Strategies, Performance Optimization and Analysis](https://arxiv.org/abs/2510.17852)
*Yuze Sun,Wentao Luo,Yanfei Xiang,Jiancheng Pan,Jiahao Li,Quan Zhang,Xiaomeng Huang*

Main category: cs.DC

TL;DR: 提出將大型氣象海洋 AI 模型從 PyTorch 遷移到 MindSpore，並針對中國國產晶片進行軟硬體適配與優化；在保持模型精度的同時，提升運行效率並降低對 GPU 的依賴。


<details>
  <summary>Details</summary>
Motivation: 現有先進氣象/海洋 AI 模型高度依賴 GPU 與 PyTorch，導致對國產硬體與生態系統的相容性差、技術自主性受限。需要一套流程將模型、訓練與推理移植到國內框架與晶片上，並評估效能與能耗。

Method: 建立一個遷移與優化框架：將模型從 PyTorch 轉為 MindSpore；針對國產晶片做軟硬體適配（運算圖、算子實現）、記憶體優化（記憶體復用、分段載入）、並行策略（數據/模型並行）；以訓練速度、推理速度、模型準確度與能效等指標與 GPU 實現比較。

Result: 實驗顯示，在保留原模型準確度的同時，透過針對性優化能在國產晶片上達到可觀的運行效率與能耗優勢，降低系統相依性，展示中國晶片在科學計算上的可行性。

Conclusion: 該工作為在大氣與海洋 AI 領域採用國產晶片與框架提供了實務流程與初步證據，促進技術自主化；後續仍需更廣泛的基準測試、開源復現與硬體多樣性驗證。

Abstract: With the growing role of artificial intelligence in climate and weather
research, efficient model training and inference are in high demand. Current
models like FourCastNet and AI-GOMS depend heavily on GPUs, limiting hardware
independence, especially for Chinese domestic hardware and frameworks. To
address this issue, we present a framework for migrating large-scale
atmospheric and oceanic models from PyTorch to MindSpore and optimizing for
Chinese chips, and evaluating their performance against GPUs. The framework
focuses on software-hardware adaptation, memory optimization, and parallelism.
Furthermore, the model's performance is evaluated across multiple metrics,
including training speed, inference speed, model accuracy, and energy
efficiency, with comparisons against GPU-based implementations. Experimental
results demonstrate that the migration and optimization process preserves the
models' original accuracy while significantly reducing system dependencies and
improving operational efficiency by leveraging Chinese chips as a viable
alternative for scientific computing. This work provides valuable insights and
practical guidance for leveraging Chinese domestic chips and frameworks in
atmospheric and oceanic AI model development, offering a pathway toward greater
technological independence.

</details>


### [4] [A Distributed Framework for Causal Modeling of Performance Variability in GPU Traces](https://arxiv.org/abs/2510.18300)
*Ankur Lahiry,Ayush Pokharel,Banooqa Banday,Seth Ockerman,Amal Gueroudji,Mohammad Zaeed,Tanzima Z. Islam,Line Pouchard*

Main category: cs.DC

TL;DR: 提出一個可並行處理多份大型 GPU 追蹤（trace）的端到端分析框架，透過資料切分與並行處理、因果圖（causal graph）方法與平行協調圖（parallel coordinating chart）來揭示執行流程間的性能變異與相依性，實驗顯示可達約 67% 的可擴展性提升。


<details>
  <summary>Details</summary>
Motivation: 單筆或多筆大型 GPU 追蹤資料量龐大且結構複雜，傳統逐筆或單機分析在時效性與計算資源上難以承受；尤其在異質 HPC 架構下，識別跨流程性能瓶頸與相依性需要可擴展且能處理多 trace 的方法。

Method: 設計一個端到端管線：先將 trace 資料分割為可並行處理的區段（partition），對各區段採用並行處理策略；利用因果圖方法推斷事件間的依賴關係，並以所謂的平行協調圖來表達不同執行流之間的時間對應與變異；整體以多工作並行執行來加速多筆 trace 的分析。

Result: 在實驗評估中，該框架相較於基線方法展現了約 67% 的可擴展性改善（scalability improvement），表示在同等資源下能更有效地同時分析多份大型 trace，且能揭示出跨流程的性能變異與相依性。

Conclusion: 提出之可並行多 trace 分析管線在擴展性與揭露性能相依性方面具體成效，適合用於大型、異質 HPC 環境的 GPU 追蹤分析；仍需更多細節（如 I/O 或記憶體瓶頸、具體加速基準、適用資料集範圍）以利全面評估與實作複現。

Abstract: Large-scale GPU traces play a critical role in identifying performance
bottlenecks within heterogeneous High-Performance Computing (HPC)
architectures. However, the sheer volume and complexity of a single trace of
data make performance analysis both computationally expensive and
time-consuming. To address this challenge, we present an end-to-end parallel
performance analysis framework designed to handle multiple large-scale GPU
traces efficiently. Our proposed framework partitions and processes trace data
concurrently and employs causal graph methods and parallel coordinating chart
to expose performance variability and dependencies across execution flows.
Experimental results demonstrate a 67% improvement in terms of scalability,
highlighting the effectiveness of our pipeline for analyzing multiple traces
independently.

</details>
