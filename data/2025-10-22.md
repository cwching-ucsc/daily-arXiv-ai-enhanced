<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 4]
- [cs.NI](#cs.NI) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Deploying Atmospheric and Oceanic AI Models on Chinese Hardware and Framework: Migration Strategies, Performance Optimization and Analysis](https://arxiv.org/abs/2510.17852)
*Yuze Sun,Wentao Luo,Yanfei Xiang,Jiancheng Pan,Jiahao Li,Quan Zhang,Xiaomeng Huang*

Main category: cs.DC

TL;DR: The paper presents a framework to migrate large-scale atmospheric/oceanic AI models from PyTorch to MindSpore and optimize them for Chinese domestic chips, achieving similar accuracy while improving operational efficiency and reducing reliance on GPUs.


<details>
  <summary>Details</summary>
Motivation: Existing weather/climate AI models are GPU- and PyTorch-centric, creating hardware and software dependency; the work aims to enable these models to run efficiently on Chinese chips and frameworks for greater independence.

Method: Software-hardware adaptation pipeline including code migration to MindSpore, memory optimization, and parallelism strategies tailored to Chinese chips; evaluation across training/inference speed, accuracy, and energy efficiency compared to GPU-based implementations.

Result: Migration preserves original model accuracy and demonstrates improved operational efficiency and reduced system dependencies when running on Chinese chips; experimental comparisons show competitive training/inference performance and better energy metrics.

Conclusion: The framework offers practical guidance to leverage Chinese domestic chips and frameworks for atmospheric and oceanic AI, suggesting these chips are a viable alternative to GPUs for scientific computing while maintaining model fidelity.

Abstract: With the growing role of artificial intelligence in climate and weather
research, efficient model training and inference are in high demand. Current
models like FourCastNet and AI-GOMS depend heavily on GPUs, limiting hardware
independence, especially for Chinese domestic hardware and frameworks. To
address this issue, we present a framework for migrating large-scale
atmospheric and oceanic models from PyTorch to MindSpore and optimizing for
Chinese chips, and evaluating their performance against GPUs. The framework
focuses on software-hardware adaptation, memory optimization, and parallelism.
Furthermore, the model's performance is evaluated across multiple metrics,
including training speed, inference speed, model accuracy, and energy
efficiency, with comparisons against GPU-based implementations. Experimental
results demonstrate that the migration and optimization process preserves the
models' original accuracy while significantly reducing system dependencies and
improving operational efficiency by leveraging Chinese chips as a viable
alternative for scientific computing. This work provides valuable insights and
practical guidance for leveraging Chinese domestic chips and frameworks in
atmospheric and oceanic AI model development, offering a pathway toward greater
technological independence.

</details>


### [2] [Efficient Multi-Worker Selection based Distributed Swarm Learning via Analog Aggregation](https://arxiv.org/abs/2510.18152)
*Zhuoyu Yao,Yue Wang,Songyang Zhang,Yingshu Li,Zhipeng Cai,Zhi Tian*

Main category: cs.DC

TL;DR: This paper proposes DSL-OTA, an over-the-air analog aggregation scheme with multi-worker selection for distributed swarm learning (DSL). It aims to improve communication efficiency, cooperation, and privacy compared to single-best-worker DSL. The authors provide theoretical convergence and communication-cost analyses and show in simulations (homogeneous and heterogeneous data) that DSL-OTA outperforms baseline methods.


<details>
  <summary>Details</summary>
Motivation: Large-scale edge networks face limited transmission resources, complex channels, and communication bottlenecks that hinder collaborative model training. Standard DSL relying on a single best worker is less federated and vulnerable to data leakage; an OTA aggregation with multi-worker participation can improve efficiency and privacy.

Method: Introduce an analog over-the-air aggregation mechanism combined with a multi-worker selection strategy for DSL. Workers transmit analog-modulated model updates simultaneously so the server receives a superposed aggregate. The server uses this OTA-aggregated signal as the global update. The paper presents theoretical convergence analysis and communication cost models under channel/noise assumptions, and conducts simulations comparing DSL-OTA to existing methods under homogeneous and heterogeneous datasets.

Result: Theoretical results claim faster convergence rates and reduced communication costs for DSL-OTA. Simulation results show improved learning performance relative to baselines across both homogeneous and heterogeneous dataset settings.

Conclusion: DSL-OTA makes DSL more federated and communication-efficient while offering privacy-preserving aggregation. The approach is theoretically justified and empirically shown to beat existing methods in the considered scenarios.

Abstract: Recent advances in distributed learning systems have introduced effective
solutions for implementing collaborative artificial intelligence techniques in
wireless communication networks. Federated learning approaches provide a
model-aggregation mechanism among edge devices to achieve collaborative
training, while ensuring data security, communication efficiency, and sharing
computational overheads. On the other hand, limited transmission resources and
complex communication environments remain significant bottlenecks to the
efficient collaborations among edge devices, particularly within large-scale
networks. To address such issues, this paper proposes an over-the-air (OTA)
analog aggregation method designed for the distributed swarm learning (DSL),
termed DSL-OTA, aiming to enhance communication efficiency, enable effective
cooperation, and ensure privacy preserving. Incorporating multi-worker
selection strategy with over-the-air aggregation not only makes the standard
DSL based on single best worker contributing to global model update to become
more federated, but also secures the aggregation from potential risks of data
leakage. Our theoretical analyses verify the advantages of the proposed DSL-OTA
algorithm in terms of fast convergence rate and low communication costs.
Simulation results reveal that our DSL-OTA outperforms the other existing
methods by achieving better learning performance under both homogeneous and
heterogeneous dataset settings.

</details>


### [3] [SLICE: SLO-Driven Scheduling for LLM Inference on Edge Computing Devices](https://arxiv.org/abs/2510.18544)
*Pan Zhou,Yiming Lei,Ling Liu,Xiaoqiong Xu,Ying Cai,Daji Ergu,Hongfang Yu,Yueyue Dai*

Main category: cs.DC

TL;DR: SLICE is a scheduler for edge LLM inference that optimizes differentiated SLOs (TTFT, TPOT, end-to-end latency) using a utility-maximizing request scheduler combined with a dynamic iterative control for generation rate; experimentally it reports up to 35× higher SLO attainment and 3.4× faster task completion than Orca and FastServe.


<details>
  <summary>Details</summary>
Motivation: Edge devices (humanoids, vehicles, etc.) run latency-sensitive LLM-powered tasks with heterogeneous SLOs (first-token latency, per-token generation latency, and end-to-end response time). Existing LLM serving schedulers focus on maximizing token throughput and therefore fail to meet diverse, stringent latency SLOs for real-time edge workloads.

Method: SLICE integrates a utility-maximizing scheduling algorithm that prioritizes requests based on their SLO utility and a dynamic iterative control mechanism that adaptively adjusts generation rates to meet different SLO constraints. The scheduler balances throughput and latency objectives to reduce SLO violations in edge scenarios.

Result: Compared to Orca and FastServe, SLICE reportedly achieves up to 35× higher SLO attainment and a 3.4× improvement in task completion time, demonstrating substantially reduced violation rates for latency- and TPOT-sensitive workloads.

Conclusion: SLICE appears to be an effective approach to serving LLMs on edge devices with heterogeneous latency SLOs. The paper shows strong empirical benefits, though clarity is needed on the utility formulation, algorithmic complexity, overheads, robustness, and experimental setup to validate generality and reproducibility.

Abstract: Large Language Models (LLMs), as the foundational architecture for
next-generation interactive AI applications, not only power intelligent
dialogue systems but also drive the evolution of embodied intelligence on edge
devices, including humanoid robots, smart vehicles, and other scenarios. The
applications running on these edge devices impose differentiated Service Level
Objectives (SLO) requirements on LLM services, specifically manifested as
distinct constraints on Time to First Token (TTFT) and Time Per Output Token
(TPOT) as well as end-to-end latency. Notably, edge devices typically handle
real-time tasks that are extremely sensitive to latency, such as machine
control and navigation planning. However, existing scheduling service systems
still prioritize maximizing output token throughput as the sole optimization
objective, failing to adequately address the diversity of SLO requirements.
This ultimately results in persistently high violation rates for end-to-end
latency or TPOT related SLOs.
  This paper proposes SLICE, an innovative scheduling solution designed for
edge computing scenarios with differentiated SLO requirements. By combining a
utility-maximizing request scheduling algorithm with a dynamic iterative
control mechanism for generation rates, SLICE significantly improves LLM
inference service SLO attainment. Experimental results demonstrate that
compared to state-of-the-art solutions Orca and FastServe, SLICE achieves up to
35x higher SLO attainment and 3.4x advantage in task completion time than the
other two solutions.

</details>


### [4] [Tokencake: A KV-Cache-centric Serving Framework for LLM-based Multi-Agent Applications](https://arxiv.org/abs/2510.18586)
*Zhuohang Bian,Feiyang Wu,Teng Ma,Youwei Zhuo*

Main category: cs.DC

TL;DR: Tokencake is a KV-cache-focused serving framework for multi-agent LLM workloads with frequent external function calls. It uses Space Scheduler (dynamic memory partitioning to protect critical agents) and Time Scheduler (proactive offload and predictive upload during stalls) to reduce eviction and repurpose idle GPU memory. On benchmarks it reports >47.06% latency reduction and up to 16.9% better effective GPU memory utilization vs vLLM.


<details>
  <summary>Details</summary>
Motivation: Multi-agent applications that perform external function calls create two KV-Cache problems: space contention (evicting critical agents' caches) and time underutilization (GPU memory occupied by agents stalled on long tool calls). These lead to higher latency and poor GPU efficiency.

Method: Agent-aware co-optimization of scheduling and memory management. Space Scheduler dynamically partitions KV cache to shield critical agents from eviction. Time Scheduler proactively offloads KV cache for stalled agents and uses predictive upload to restore caches before agent resumption, reclaiming GPU memory during tool calls.

Result: Evaluation on representative multi-agent benchmarks shows Tokencake reduces end-to-end latency by over 47.06% and improves effective GPU memory utilization by up to 16.9% compared to vLLM.

Conclusion: By treating the KV cache as a first-class resource and combining dynamic partitioning with predictive offload/upload, Tokencake substantially improves latency and GPU memory utilization in multi-agent LLM-serving scenarios and is a promising approach for function-call-heavy workloads.

Abstract: Large Language Models (LLMs) are increasingly deployed in complex multi-agent
applications that use external function calls. This workload creates severe
performance challenges for the KV Cache: space contention leads to the eviction
of critical agents' caches and time underutilization leaves the cache of agents
stalled on long-running tool calls idling in GPU memory. We present Tokencake,
a KV-Cache-centric serving framework that co-optimizes scheduling and memory
management with an agent-aware design. Tokencake's Space Scheduler uses dynamic
memory partitioning to shield critical agents from contention, while its Time
Scheduler employs a proactive offload and predictive upload mechanism to
repurpose GPU memory during function call stalls. Our evaluation on
representative multi-agent benchmarks shows that Tokencake can reduce
end-to-end latency by over 47.06%, improve effective GPU memory utilization by
up to 16.9% compared to vLLM.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [5] [A New Broadcast Model for Several Network Topologies](https://arxiv.org/abs/2510.18058)
*Hongbo Lu,Junsung Hwang,Bernard Tenreiro,Nabila Jaman Tripti,Darren Hamilton,Yuefan Deng*

Main category: cs.NI

TL;DR: BBS is a general broadcast algorithm that maximizes node utilization via a repeatable, stepwise communication cycle to reduce latency and improve data propagation across diverse network topologies; simulation claims show it outperforms common general broadcast algorithms.


<details>
  <summary>Details</summary>
Motivation: Reduce broadcast latency and inefficiency in large-scale systems (e.g., supercomputers) by addressing topology constraints, bandwidth limits, and synchronization overhead; keep nodes active throughout broadcast to improve throughput and utilization.

Method: A ‘‘Balanced Saturation’’ scheduling strategy that enforces a precise, repeatable communication cycle so nodes remain engaged and links are balanced. The algorithm provides a stepwise broadcast framework tailored to different topologies to maximize sustained activity and data propagation.

Result: Simulations across multiple topologies indicate BBS consistently outperforms standard general broadcast algorithms, often by a substantial margin, exhibiting lower latency and higher node/link utilization.

Conclusion: BBS is presented as a versatile, robust broadcast framework with potential to improve or redefine broadcast strategies across varied network topologies.

Abstract: We present Broadcast by Balanced Saturation (BBS), a general broadcast
algorithm designed to optimize communication efficiency across diverse network
topologies. BBS maximizes node utilization, addressing challenges in broadcast
operations such as topology constraints, bandwidth limitations, and
synchronization overhead, particularly in large-scale systems like
supercomputers. The algorithm ensures sustained activity with nodes throughout
the broadcast, thereby enhancing data propagation and significantly reducing
latency. Through a precise communication cycle, BBS provides a repeatable,
streamlined, stepwise broadcasting framework. Simulation results across various
topologies demonstrate that the BBS algorithm consistently outperforms common
general broadcast algorithms, often by a substantial margin. These findings
suggest that BBS is a versatile and robust framework with the potential to
redefine broadcast strategies across network topologies.

</details>


### [6] [JAUNT: Joint Alignment of User Intent and Network State for QoE-centric LLM Tool Routing](https://arxiv.org/abs/2510.18550)
*Enhan Li,Hongyang Du*

Main category: cs.NI

TL;DR: JAUNT is a framework that routes LLM tool calls by jointly aligning user intent and network state to optimize Quality of Experience (QoE). It uses a dual-view strategy where LLMs interpret intent and construct semantic network profiles from numerical metrics, then routes requests accordingly. A benchmark of varied user patterns and network conditions shows QoE gains over baselines.


<details>
  <summary>Details</summary>
Motivation: Existing tool routing for LLMs focuses on semantic/functional matching and ignores network factors (latency, availability) and underspecified user intent, causing fragile QoE in practice. The paper aims to close this gap by incorporating network state into routing decisions and aligning it with interpreted user intent.

Method: Introduce JAUNT: (1) a dual-view alignment—intent interpretation by LLM agents and network profiling that maps numerical performance indicators to semantic descriptors; (2) a routing mechanism that uses the combined semantic space to select tools/services optimizing QoE; (3) a benchmark that mixes diverse user requests and heterogeneous network states for evaluation.

Result: Empirical results on the proposed benchmark show JAUNT significantly improves QoE compared to several baselines that rely on semantic-only or naive routing, demonstrating benefits of joint intent-network alignment.

Conclusion: Aligning user intent with network state in the semantic space enables more robust, QoE-centric tool routing for LLMs. JAUNT provides a practical framework and benchmark that yield improved end-user experience and can guide future LLM service orchestration work.

Abstract: Large Language Models (LLMs) increasingly rely on emerging protocols such as
the Model Context Protocol (MCP) to invoke external tools and services.
However, current tool routing mechanisms remain fragile because they only
consider functional matching between users' queries and tools. In practice,
user intent expressed through queries can be vague or underspecified, and the
actual Quality of Experience (QoE) also depends on external factors such as
link latency and server availability that are not captured by semantics alone.
To address this challenge, we propose JAUNT, a framework for Joint Alignment of
User intent and Network state in QoE-centric Tool routing. JAUNT introduces a
dual-view alignment strategy that interprets user intent while employing LLM
agents to construct network profiles, mapping numerical performance indicators
into the semantic space to guide routing. We further design a benchmark that
integrates diverse user request patterns with heterogeneous network states,
enabling systematic evaluation of QoE outcomes. Experimental results show that
JAUNT significantly improves QoE compared with several baselines, demonstrating
the importance of aligning both intent and network state for scalable LLM
service orchestration.

</details>
