{"id": "2512.03416", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.03416", "abs": "https://arxiv.org/abs/2512.03416", "authors": ["Ruiqi Lai", "Hongrui Liu", "Chengzhi Lu", "Zonghao Liu", "Siyu Cao", "Siyang Shao", "Yixin Zhang", "Luo Mai", "Dmitrii Ustiugov"], "title": "TokenScale: Timely and Accurate Autoscaling for Disaggregated LLM Serving with Token Velocity", "comment": null, "summary": "The architectural shift to prefill/decode (PD) disaggregation in LLM serving improves resource utilization but struggles with the bursty nature of modern workloads. Existing autoscaling policies, often retrofitted from monolithic systems like those in AIBrix and DistServe, rely on lagging indicators such as GPU utilization or coarse-grained request counts. This results in slow reactions to load spikes, leading to significant Time-to First-Token (TTFT) and Time-Per-Output-Token (TPOT) SLO violations and costly over-provisioning. We introduce TokenScale, an autoscaling framework that resolves this performance mismatch through two innovations. First, we propose Token Velocity, a novel metric that unifies the prefill, network, and decode stages by quantifying their rate of work. As a leading indicator of system backpressure, it enables proactive scaling. Second, Convertible Decoders allow decoder GPUs to dynamically execute prefill tasks during traffic spikes, creating a rapid-response buffer that absorbs bursts and eliminates the initialization latency of new prefillers. Our evaluation on a GPU cluster with production traces shows TokenScale improves SLO attainment from 50-88% to 80-96% and reduces costs by 4-14% over state-of-the-art systems, including DistServe, BlitzScale, and AIBrix. By uniting a predictive metric with a flexible system design, TokenScale significantly boosts the performance and efficiency of disaggregated LLM serving infrastructure.", "AI": {"tldr": "TokenScale is an autoscaling framework for prefill/decode (PD) disaggregated LLM serving that uses a predictive metric (Token Velocity) and Convertible Decoders (decoder GPUs that can run prefill tasks) to proactively absorb bursts. On production traces it raises SLO attainment from 50\u201388% to 80\u201396% and cuts costs 4\u201314% vs. DistServe, BlitzScale, and AIBrix.", "motivation": "PD disaggregation improves utilization but existing autoscalers use lagging signals (GPU util, coarse request counts), causing slow reaction to bursty traffic, high TTFT/TPOT SLO violations, and over-provisioning.", "method": "Introduce Token Velocity, a unified rate metric spanning prefill, network, and decode stages to predict backpressure and trigger proactive scaling. Add Convertible Decoders that can dynamically execute prefill work during spikes to provide a rapid-response buffer and avoid prefill initialization latency.", "result": "Evaluation on a GPU cluster with production traces shows substantial SLO improvements (80\u201396% attainment vs. prior 50\u201388%) and 4\u201314% cost reduction compared to state-of-the-art systems.", "conclusion": "Combining a predictive, leading indicator with a flexible runtime (Convertible Decoders) significantly improves responsiveness and cost-efficiency of disaggregated LLM serving under bursty workloads."}}
{"id": "2512.03644", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.03644", "abs": "https://arxiv.org/abs/2512.03644", "authors": ["Bohan Zhao", "Yuanhong Wang", "Chenglin Liu", "Jiagi Pan", "Guang Yang", "Ruitao Liu", "Tingrui Zhang", "Kai Luo", "Wei Xu"], "title": "FFTrainer: Fast Failover in Large-Language Model Training with Almost-Free State Management", "comment": null, "summary": "Recent developments in large language models (LLMs) have introduced new requirements for efficient and robust training. As LLM clusters scale, node failures, lengthy recoveries, and bulky checkpoints erode efficiency. Infrequent asynchronous checkpoints trigger costly rollbacks, yet higher frequencies add prohibitive overhead. To address these challenges, we propose FFTrainer, a system designed for robust LLM training. FFTrainer leverages surplus network capacity to quickly save and load states, thereby preventing rollbacks and accelerating recovery. Compared with prior checkpointing approaches, FFTrainer reduces recovery time by up to 98% and mitigates GPU utilization loss by up to 68% without hindering normal training.", "AI": {"tldr": "FFTrainer is a system for robust large language model training that uses spare network capacity to save and load model state quickly, enabling frequent lightweight checkpoints and rapid recovery. It claims up to 98% reduction in recovery time and up to 68% reduction in GPU utilization loss without disrupting normal training.", "motivation": "As LLM clusters scale, node failures, long recoveries, and bulky checkpoints reduce training efficiency. Infrequent checkpoints cause costly rollbacks while frequent checkpoints usually impose prohibitive overhead. A mechanism is needed to make checkpoints cheap and recovery fast to maintain high utilization.", "method": "FFTrainer leverages surplus network bandwidth to perform quick state save/load operations (lightweight, frequent checkpoints) so that training can resume rapidly after failures without heavy rollbacks. It integrates with normal training to avoid interfering with throughput while using network resources to offload checkpoint overhead.", "result": "Empirically, FFTrainer reduces recovery time by up to 98% and reduces GPU utilization loss by up to 68% compared with prior checkpointing approaches, while not hindering normal training performance.", "conclusion": "Using spare network capacity for fast, frequent state transfers is an effective engineering approach to improve robustness and utilization in large-scale LLM training. FFTrainer demonstrates major reductions in recovery time and utilization loss, though applicability depends on available network headroom and integration with cluster/storage architectures."}}
{"id": "2512.03927", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.03927", "abs": "https://arxiv.org/abs/2512.03927", "authors": ["Liujianfu Wang", "Yuyang Du", "Yuchen Pan", "Soung Chang Liew", "Jiacheng Liu", "Kexin Chen"], "title": "OD-MoE: On-Demand Expert Loading for Cacheless Edge-Distributed MoE Inference", "comment": null, "summary": "Mixture-of-Experts (MoE), while offering significant advantages as a Large Language Model (LLM) architecture, faces substantial challenges when deployed on low-cost edge devices with tight memory constraints. Expert offloading mitigates this issue by storing expert parameters in CPU memory and caching a subset of popular experts in GPU memory. Although this approach improves GPU memory utilization by caching only the likely-used experts, the GPU memory reserved for expert caching is underutilized compared with dense LLMs. This paper presents OD-MoE, a distributed MoE inference framework that obviates the need for expert caches via fully on-demand expert loading. OD-MoE is built upon two key mechanisms: 1) parallelizing expert loading and expert computation across distributed edge nodes, and 2) an ultra-accurate emulative predictor that forecasts expert activations multiple layers ahead while expert computation is ongoing. With these innovations, OD-MoE dynamically loads each target expert to one of the distributed nodes just-in-time before its activation and promptly evicts it afterward, freeing GPU memory for subsequent experts. We comprehensively benchmark OD-MoE against state-of-the-art MoE offloading systems on a ten-node testbed. Experimental results show that: 1) OD-MoE achieves 99.94% expert activation prediction accuracy, substantially surpassing all existing methods; and 2) OD-MoE delivers approximately 75% of the decoding speed of a fully GPU-cached MoE deployment while using only 1/3 of the GPU memory. More importantly, by eliminating the need for expert caches, OD-MoE enables MoE inference on edge nodes with less-than-1GB GPU memory, paving the way for practical MoE deployment of low-cost IoT devices at the edge in the LLM era.", "AI": {"tldr": "OD-MoE removes expert caches and implements fully on-demand expert loading across distributed edge nodes using parallelized loading/computation and an ultra-accurate predictor. On a 10-node testbed it attains 99.94% activation prediction accuracy and ~75% decoding speed of fully GPU-cached MoE while using 1/3 of GPU memory, enabling MoE inference on <1GB GPUs.", "motivation": "MoE architectures reduce model size but require large expert memory; existing expert-offloading with GPU caches still underutilizes reserved GPU memory and remains infeasible on tight-memory edge/IoT devices. A memory-efficient, low-latency inference method is needed for edge deployment.", "method": "Introduce OD-MoE: (1) distribute expert loading and computation across edge nodes to hide I/O and balance load; (2) use an emulative predictor that forecasts expert activations multiple layers ahead, allowing just-in-time loading of the target expert to one node and eviction immediately after use.", "result": "On a 10-node testbed, the predictor achieves 99.94% activation prediction accuracy. OD-MoE attains ~75% of decoding throughput of a fully GPU-cached MoE while using only ~1/3 GPU memory and enables operation on GPUs with <1GB memory.", "conclusion": "OD-MoE effectively eliminates the need for GPU expert caches and enables practical MoE inference on constrained edge devices. It trades off some throughput for a large reduction in required GPU memory, but introduces distributed system complexity and depends on predictor reliability and network characteristics."}}
{"id": "2512.03722", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.03722", "abs": "https://arxiv.org/abs/2512.03722", "authors": ["Lingyi Cai", "Wenjie Fu", "Yuxi Huang", "Ruichen Zhang", "Yinqiu Liu", "Jiawen Kang", "Zehui Xiong", "Tao Jiang", "Dusit Niyato", "Xianbin Wang", "Shiwen Mao", "Xuemin Shen"], "title": "Tutorial on Large Language Model-Enhanced Reinforcement Learning for Wireless Networks", "comment": "30 pages, 12 figures, survey paper", "summary": "Reinforcement Learning (RL) has shown remarkable success in enabling adaptive and data-driven optimization for various applications in wireless networks. However, classical RL suffers from limitations in generalization, learning feedback, interpretability, and sample efficiency in dynamic wireless environments. Large Language Models (LLMs) have emerged as a transformative Artificial Intelligence (AI) paradigm with exceptional capabilities in knowledge generalization, contextual reasoning, and interactive generation, which have demonstrated strong potential to enhance classical RL. This paper serves as a comprehensive tutorial on LLM-enhanced RL for wireless networks. We propose a taxonomy to categorize the roles of LLMs into four critical functions: state perceiver, reward designer, decision-maker, and generator. Then, we review existing studies exploring how each role of LLMs enhances different stages of the RL pipeline. Moreover, we provide a series of case studies to illustrate how to design and apply LLM-enhanced RL in low-altitude economy networking, vehicular networks, and space-air-ground integrated networks. Finally, we conclude with a discussion on potential future directions for LLM-enhanced RL and offer insights into its future development in wireless networks.", "AI": {"tldr": "This tutorial surveys how Large Language Models (LLMs) can augment Reinforcement Learning (RL) for wireless networks by serving as state perceivers, reward designers, decision-makers, and generators; it presents taxonomy, literature review, case studies (low-altitude, vehicular, space-air-ground networks), and future directions.", "motivation": "Classical RL in wireless networks struggles with generalization, interpretability, learning feedback, and sample efficiency in dynamic environments. LLMs offer strengths in contextual reasoning, knowledge generalization, and interactive generation that could address these shortcomings.", "method": "Propose a four-role taxonomy for LLMs (state perceiver, reward designer, decision-maker, generator); review existing works mapped to these roles across the RL pipeline; provide case studies applying LLM-enhanced RL to three wireless domains; discuss challenges and future research directions.", "result": "Synthesis of prior studies showing potential benefits of LLMs in improving generalization, reward shaping, decision guidance, and data/knowledge generation for RL in wireless settings. Illustrative case studies demonstrate design patterns and potential performance improvements (conceptual, not necessarily empirical).", "conclusion": "LLM-enhanced RL is a promising direction for wireless networks, but concrete empirical validation, standardized benchmarks, safety/latency analysis, and design guidelines remain open; the paper identifies future research avenues to make LLM+RL practical for real deployments."}}
