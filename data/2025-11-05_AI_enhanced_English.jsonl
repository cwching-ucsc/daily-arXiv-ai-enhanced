{"id": "2511.01861", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.01861", "abs": "https://arxiv.org/abs/2511.01861", "authors": ["Johan Messchendorp", "Mohammad Al-Turany", "Volker Friese", "Thorsten Kollegger", "Bastian Loeher", "Jochen Markert", "Andrew Mistry", "Thomas Neff", "Adrian Oeftiger", "Michael Papenbrock", "Stephane Pietri", "Shahab Sanjari", "Tobias Stockmanns"], "title": "Conceptual Design Report for FAIR Computing", "comment": "88 pages, Conceptual Design Report for FAIR Computing", "summary": "This Conceptual Design Report (CDR) presents the plans of the computing\ninfrastructure for research at FAIR, Darmstadt, Germany. It presents the\ncomputing requirements of the various research groups, the policies for the\ncomputing and storage infrastructure, the foreseen FAIR computing model\nincluding the open data, software and services policies and architecture for\nthe periods starting in 2028 with the \"first science (plus)\" phase to the\nmodularized start version of FAIR. The overall ambition is to create a\nfederated and centrally-orchestrated infrastructure serving the large diversity\nof the research lines present with sufficient scalability and flexibility to\ncope with future data challenges that will be present at FAIR.", "AI": {"tldr": "CDR outlines plans for FAIR computing infrastructure (2028 onward), proposing a federated, centrally-orchestrated, scalable and flexible model with open data/software/services policies to support diverse research lines through FAIR's modular start-up phases.", "motivation": "Provide a coherent computing and storage infrastructure plan to meet the diverse, growing data and compute needs of FAIR experiments starting with the \"first science (plus)\" phase through the modularized start version, ensuring openness and scalability.", "method": "Collect and present computing requirements from research groups, define policies (computing, storage, open data/software/services), and specify an architectural model: a federated yet centrally-orchestrated infrastructure with policy and service layers for orchestration and openness.", "result": "A proposed computing model and architecture, policy frameworks, and high-level plans for deployment and operation covering the period from 2028 onwards; emphasis on federation, central orchestration, scalability, and flexibility.", "conclusion": "The CDR advocates building a federated, centrally-orchestrated computing infrastructure for FAIR that adheres to open data and software principles and is scalable/flexible enough to handle future data challenges."}}
{"id": "2511.01866", "categories": ["cs.DC", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.01866", "abs": "https://arxiv.org/abs/2511.01866", "authors": ["Benjamin Kubwimana", "Qijing Huang"], "title": "EdgeReasoning: Characterizing Reasoning LLM Deployment on Edge GPUs", "comment": "Published in the Proceedings of the 2025 IEEE International Symposium\n  on Workload Characterization (IISWC 2025)", "summary": "Edge intelligence paradigm is increasingly demanded by the emerging\nautonomous systems, such as robotics. Beyond ensuring privacy-preserving\noperation and resilience in connectivity-limited environments, edge deployment\noffers significant energy and cost advantages over cloud-based solutions.\nHowever, deploying large language models (LLMs) for reasoning tasks on edge\nGPUs faces critical challenges from strict latency constraints and limited\ncomputational resources. To navigate these constraints, developers must balance\nmultiple design factors - choosing reasoning versus non-reasoning\narchitectures, selecting appropriate model sizes, allocating token budgets, and\napplying test-time scaling strategies - to meet target latency and optimize\naccuracy. Yet guidance on optimal combinations of these variables remains\nscarce. In this work, we present EdgeReasoning, a comprehensive study\ncharacterizing the deployment of reasoning LLMs on edge GPUs. We systematically\nquantify latency-accuracy tradeoffs across various LLM architectures and model\nsizes. We systematically evaluate prompt-based and model-tuning-based\ntechniques for reducing reasoning token length while maintaining performance\nquality. We further profile test-time scaling methods with varying degrees of\nparallelism to maximize accuracy under strict latency budgets. Through these\nanalyses, EdgeReasoning maps the Pareto frontier of achievable accuracy-latency\nconfigurations, offering systematic guidance for optimal edge deployment of\nreasoning LLMs.", "AI": {"tldr": "EdgeReasoning studies deployment of reasoning-capable LLMs on edge GPUs, measuring latency-accuracy tradeoffs and evaluating token-reduction and test-time scaling techniques to find Pareto-optimal configurations for edge reasoning.", "motivation": "Edge/autonomous systems require local, low-latency, energy-efficient reasoning while preserving privacy and operating with intermittent connectivity; existing guidance on how to pick model architecture, size, token budgets, and scaling strategies for edge GPUs is lacking.", "method": "Systematic empirical evaluation across LLM architectures and sizes on edge GPUs; comparison of prompt-based and model-tuning techniques to shorten reasoning token length; profiling test-time scaling methods with different parallelism levels to maximize accuracy under latency constraints; mapping accuracy-latency Pareto frontiers.", "result": "Quantified latency-accuracy tradeoffs for varied architectures and sizes; identified effective prompt- and tuning-based token-reduction techniques that retain performance; characterized how test-time scaling and parallelism impact achievable accuracy under strict latency budgets; produced Pareto frontiers guiding deployment choices.", "conclusion": "EdgeReasoning provides practical, systematic guidance for selecting model family, size, token budget, and runtime scaling to meet latency targets while optimizing reasoning accuracy on edge GPUs, enabling informed tradeoffs and efficient edge deployment of reasoning LLMs."}}
{"id": "2511.02501", "categories": ["cs.NI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.02501", "abs": "https://arxiv.org/abs/2511.02501", "authors": ["Mohan Liyanage", "Eldiyar Zhantileuov", "Ali Kadhum Idrees", "Rolf Schuster"], "title": "Lightweight Latency Prediction Scheme for Edge Applications: A Rational Modelling Approach", "comment": "Presented at the ICCS 2025 - 5th International Conference on Computer\n  Systems, Xian, China", "summary": "Accurately predicting end-to-end network latency is essential for enabling\nreliable task offloading in real-time edge computing applications. This paper\nintroduces a lightweight latency prediction scheme based on rational modelling\nthat uses features such as frame size, arrival rate, and link utilization,\neliminating the need for intrusive active probing. The model achieves\nstate-of-the-art prediction accuracy through extensive experiments and 5-fold\ncross-validation (MAE = 0.0115, R$^2$ = 0.9847) with competitive inference\ntime, offering a substantial trade-off between precision and efficiency\ncompared to traditional regressors and neural networks.", "AI": {"tldr": "Lightweight rational model predicts end-to-end network latency from passive features (frame size, arrival rate, link utilization) with high accuracy and low inference cost, avoiding active probing.", "motivation": "Enable reliable task offloading in real-time edge computing by providing accurate, low-overhead predictions of end-to-end latency to support scheduling and resource decisions.", "method": "Proposes a rational (parameterized analytic) modelling approach using passive network features (frame size, arrival rate, link utilization) for latency prediction; evaluated via extensive experiments and 5-fold cross-validation; compared against traditional regressors and neural networks.", "result": "Achieves strong predictive performance (MAE=0.0115, R^2=0.9847) with competitive inference time, claimed to provide a favorable precision-efficiency trade-off relative to other models.", "conclusion": "Rational modelling using passive features offers state-of-the-art latency prediction suitable for real-time edge computing, reducing the need for intrusive active probing while maintaining accuracy and efficiency."}}
{"id": "2511.02168", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02168", "abs": "https://arxiv.org/abs/2511.02168", "authors": ["Octavian Alexandru Trifan", "Karthik Sangaiah", "Muhammad Awad", "Muhammad Osama", "Sumanth Gudaparthi", "Alexandru Nicolau", "Alexander Veidenbaum", "Ganesh Dasika"], "title": "Eliminating Multi-GPU Performance Taxes: A Systems Approach to Efficient Distributed LLMs", "comment": null, "summary": "As large language models (LLMs) continue to scale, their workloads\nincreasingly rely on distributed execution across multiple GPUs. However, the\nconventional bulk synchronous parallel~(BSP) model used in such settings\nintroduces significant performance inefficiencies. To characterize these\nbottlenecks, we introduce the ''Three Taxes'' (Bulk Synchronous, Inter-Kernel\nData Locality, and Kernel Launch Overhead) as an analytical framework. We\npropose moving beyond the rigid BSP model to address key inefficiencies in\ndistributed GPU execution. By exploiting libraries like Iris for Triton, we\ngain access to in-kernel communication primitives that enable the design of\nnovel fine-grained programming patterns, offering greater flexibility and\nperformance than traditional BSP-based approaches. These patterns\nsystematically eliminate the three taxes by creating direct, tile-level\nproducer-consumer pipelines and replacing global barriers with fine-grained\ndataflow synchronization. Applying this methodology to critical kernels, from\nthe foundational All-Gather + general matrix multiplication operation to the\ncomplex Flash Decode algorithm, we observe a 10-20% speedup in end-to-end\nlatency over BSP-based approaches, establishing a more programmable and\nefficient paradigm for distributed LLM workloads.", "AI": {"tldr": "Paper identifies three sources of inefficiency when running distributed LLM kernels under BSP and proposes using in-kernel communication primitives (via Iris for Triton) to implement tile-level producer-consumer pipelines and fine-grained dataflow synchronization. Applied to All-Gather+GEMM and Flash Decode kernels, this approach reduces end-to-end latency by about 10\u201320% vs BSP implementations.", "motivation": "As LLMs scale, multi-GPU distributed execution is necessary but the dominant bulk synchronous parallel (BSP) model imposes global barriers, poor inter-kernel data locality, and frequent kernel launches, which together limit performance and hardware utilization.", "method": "Introduce an analytical framework called the \"Three Taxes\" (BSP, inter-kernel data locality, kernel launch overhead). Use Iris for Triton to access in-kernel communication primitives and design fine-grained programming patterns that create direct, tile-level producer-consumer pipelines replacing global barriers with dataflow synchronization. Apply these patterns to key kernels (All-Gather + GEMM and Flash Decode) and benchmark against BSP-based implementations.", "result": "Empirical evaluation shows systematic elimination of the three taxes and delivers roughly 10\u201320% improvement in end-to-end latency for distributed LLM workloads compared to BSP baselines.", "conclusion": "Fine-grained in-kernel communication and tile-level pipelining provide a more programmable and efficient execution model for distributed LLMs, yielding measurable latency improvements over traditional BSP approaches and enabling a new class of performance-optimized kernel designs."}}
{"id": "2511.02703", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.02703", "abs": "https://arxiv.org/abs/2511.02703", "authors": ["Mengyao Li", "Noah Ploch", "Sebastian Troia", "Carlo Spatocco", "Wolfgang Kellerer", "Guido Maier"], "title": "On the Optimization of Model Aggregation for Federated Learning at the Network Edge", "comment": "accepted by TNSM", "summary": "The rapid increase in connected devices has signifi- cantly intensified the\ncomputational and communication demands on modern telecommunication networks.\nTo address these chal- lenges, integrating advanced Machine Learning (ML)\ntechniques like Federated Learning (FL) with emerging paradigms such as\nMulti-access Edge Computing (MEC) and Software-Defined Wide Area Networks\n(SD-WANs) is crucial. This paper intro- duces online resource management\nstrategies specifically designed for FL model aggregation, utilizing\nintermediate aggregation at edge nodes. Our analysis highlights the benefits of\nincorporating edge aggregators to reduce network link congestion and maximize\nthe potential of edge computing nodes. However, the risk of network congestion\npersists. To mitigate this, we propose a novel aggregation approach that\ndeploys an aggregator overlay network. We present an Integer Linear Programming\n(ILP) model and a heuristic algorithm to optimize the routing within this\noverlay network. Our solution demonstrates improved adapt- ability to network\nresource utilization, significantly reducing FL training round failure rates by\nup to 15% while also alleviating cloud link congestion.", "AI": {"tldr": "Proposes using intermediate edge aggregators and an aggregator overlay network to manage federated learning model aggregation across MEC and SD-WAN infrastructures. Introduces an ILP and a practical heuristic to route aggregated updates, reducing FL training-round failures and cloud-link congestion.", "motivation": "Massive growth in connected devices stresses compute and network resources; centralized aggregation causes bottlenecks. Leveraging edge computing and programmable wide-area networks can reduce congestion and improve FL scalability and robustness.", "method": "Designs online resource-management strategies that perform intermediate aggregation at edge nodes and, to further mitigate congestion, deploys an aggregator overlay network. Formulates routing and allocation as an Integer Linear Program (ILP) and develops a heuristic for scalable, near-real-time routing decisions.", "result": "Simulation/experimental results claim improved resource utilization, reduced FL training-round failure rates by up to 15%, and decreased load on cloud links. The heuristic provides practical performance close to the ILP solution with lower computation overhead.", "conclusion": "Edge-level aggregation plus an overlay routing layer can substantially improve FL scalability and reliability; the ILP gives optimal benchmarks while the heuristic offers a deployable approach. Future work should address complexity, real-world deployment, privacy, and resilience to dynamics."}}
{"id": "2511.02248", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02248", "abs": "https://arxiv.org/abs/2511.02248", "authors": ["Xingqi Cui", "Chieh-Jan Mike Liang", "Jiarong Xing", "Haoran Qiu"], "title": "From Models to Operators: Rethinking Autoscaling Granularity for Large Generative Models", "comment": "16 pages, 13 figures", "summary": "Serving large generative models such as LLMs and multi- modal transformers\nrequires balancing user-facing SLOs (e.g., time-to-first-token,\ntime-between-tokens) with provider goals of efficiency and cost reduction.\nExisting solutions rely on static provisioning or model-level autoscaling, both\nof which treat the model as a monolith. This coarse-grained resource management\nleads to degraded performance or significant resource underutilization due to\npoor adaptability to dynamic inference traffic that is common online.\n  The root cause of this inefficiency lies in the internal structure of\ngenerative models: they are executed as graphs of interconnected operators.\nThrough detailed characterization and systematic analysis, we find that\noperators are heterogeneous in their compute and memory footprints and exhibit\ndiverse sensitivity to workload and resource factors such as batch size,\nsequence length, and traffic rate. This heterogeneity suggests that the\noperator, rather than the entire model, is the right granularity for scaling\ndecisions.\n  We propose an operator-level autoscaling framework, which allocates resources\nat finer (operator)-granularity, optimizing the scaling, batching, and\nplacement based on individual operator profiles. Evaluated on production-scale\ntraces, our approach preserves SLOs with up to 40% fewer GPUs and 35% less\nenergy, or under fixed resources achieves 1.6x higher throughput with 5% less\nenergy. These results show that the operator, rather than the model, is\nfundamentally a more effective unit for scaling large generative workloads.", "AI": {"tldr": "Operators inside generative models are heterogeneous; autoscaling at operator granularity (scaling, batching, placement per-operator) yields large efficiency gains over model-level scaling, preserving SLOs with fewer GPUs and lower energy or providing higher throughput under fixed resources.", "motivation": "Existing deployment schemes treat models as monoliths, leading to poor adaptation to dynamic online inference traffic and resulting in either missed SLOs or resource underutilization. The paper targets better resource efficiency while meeting user-facing SLOs.", "method": "Characterize operator-level compute/memory heterogeneity and sensitivity to batch size, sequence length, and traffic. Propose an operator-level autoscaling framework that decides scaling, batching, and placement per operator based on operator profiles.", "result": "On production-scale traces, the approach maintains SLOs with up to 40% fewer GPUs and 35% less energy, or achieves 1.6\u00d7 higher throughput with 5% less energy under fixed resources.", "conclusion": "Scaling at operator granularity is more effective than model-level or static provisioning for serving large generative workloads, yielding substantial efficiency and throughput improvements."}}
{"id": "2511.02748", "categories": ["cs.NI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02748", "abs": "https://arxiv.org/abs/2511.02748", "authors": ["Farhad Rezazadeh", "Hatim Chergui", "Merouane Debbah", "Houbing Song", "Dusit Niyato", "Lingjia Liu"], "title": "Agentic World Modeling for 6G: Near-Real-Time Generative State-Space Reasoning", "comment": "13 Pages, 3 Figures, 4 Tables", "summary": "We argue that sixth-generation (6G) intelligence is not fluent token\nprediction but the capacity to imagine and choose -- to simulate future\nscenarios, weigh trade-offs, and act with calibrated uncertainty. We reframe\nopen radio access network (O-RAN) near-real-time (Near-RT) control via\ncounterfactual dynamics and a world modeling (WM) paradigm that learns an\naction-conditioned generative state space. This enables quantitative \"what-if\"\nforecasting beyond large language models (LLMs) as the primary modeling\nprimitive. Actions such as physical resource blocks (PRBs) are treated as\nfirst-class control inputs in a causal world model, and both aleatoric and\nepistemic uncertainty are modeled for prediction and what-if analysis. An\nagentic, model predictive control (MPC)-based cross-entropy method (CEM)\nplanner operates over short horizons, using prior-mean rollouts within\ndata-driven PRB bounds to maximize a deterministic reward. The model couples\nmulti-scale structured state-space mixtures (MS3M) with a compact stochastic\nlatent to form WM-MS3M, summarizing key performance indicators (KPIs) histories\nand predicting next-step KPIs under hypothetical PRB sequences. On realistic\nO-RAN traces, WM-MS3M cuts mean absolute error (MAE) by 1.69% versus MS3M with\n32% fewer parameters and similar latency, and achieves 35-80% lower root mean\nsquared error (RMSE) than attention/hybrid baselines with 2.3-4.1x faster\ninference, enabling rare-event simulation and offline policy screening.", "AI": {"tldr": "Proposes WM-MS3M, an action-conditioned world model for near-RT O-RAN control that models aleatoric/epistemic uncertainty and enables counterfactual what-if forecasting. Uses MPC/CEM planning over PRB actions. Shows modest MAE improvement vs MS3M with fewer parameters and larger RMSE and latency gains vs attention/hybrid baselines on realistic traces.", "motivation": "Argues 6G intelligence should be able to imagine and choose via counterfactual simulation rather than rely on token-prediction primitives (LLMs). Applies this to O-RAN near-RT control where treating PRBs as causal control inputs and evaluating hypothetical sequences is valuable for KPI-driven decisions.", "method": "Introduces WM-MS3M: couples multi-scale structured state-space mixtures (MS3M) with a compact stochastic latent to produce an action-conditioned generative state space. Models both aleatoric and epistemic uncertainty. Uses an MPC-style planner with cross-entropy method (CEM) that rolls out prior-mean trajectories within data-driven PRB bounds to maximize deterministic reward over short horizons.", "result": "On realistic O-RAN traces, WM-MS3M reduces MAE by 1.69% versus MS3M while using 32% fewer parameters and similar latency. Achieves 35\u201380% lower RMSE than attention/hybrid baselines with 2.3\u20134.1\u00d7 faster inference. Enables rare-event simulation and offline policy screening.", "conclusion": "World-modeling with structured state-space mixtures and action conditioning is effective for near-RT O-RAN control: it improves accuracy, efficiency, and enables counterfactual planning. Promising direction for 6G intelligence; further work should test longer horizons, causal identification, real deployment, robustness, and multi-agent settings."}}
{"id": "2511.02293", "categories": ["cs.DC", "cs.CV", "C.2.4; I.2.10"], "pdf": "https://arxiv.org/pdf/2511.02293", "abs": "https://arxiv.org/abs/2511.02293", "authors": ["Taisuke Noguchi", "Takuya Azumi"], "title": "3D Point Cloud Object Detection on Edge Devices for Split Computing", "comment": "6 pages. This version includes minor lstlisting configuration\n  adjustments for successful compilation. No changes to content or layout.\n  Originally published at ACM/IEEE RAGE 2024", "summary": "The field of autonomous driving technology is rapidly advancing, with deep\nlearning being a key component. Particularly in the field of sensing, 3D point\ncloud data collected by LiDAR is utilized to run deep neural network models for\n3D object detection. However, these state-of-the-art models are complex,\nleading to longer processing times and increased power consumption on edge\ndevices. The objective of this study is to address these issues by leveraging\nSplit Computing, a distributed machine learning inference method. Split\nComputing aims to lessen the computational burden on edge devices, thereby\nreducing processing time and power consumption. Furthermore, it minimizes the\nrisk of data breaches by only transmitting intermediate data from the deep\nneural network model. Experimental results show that splitting after\nvoxelization reduces the inference time by 70.8% and the edge device execution\ntime by 90.0%. When splitting within the network, the inference time is reduced\nby up to 57.1%, and the edge device execution time is reduced by up to 69.5%.", "AI": {"tldr": "The paper applies Split Computing to LiDAR-based 3D object detection, showing large reductions in inference and edge execution time by splitting after voxelization or within the network.", "motivation": "State-of-the-art 3D LiDAR detection models are computationally heavy for edge devices, increasing latency and power consumption; split computing can offload work and reduce data exposure by transmitting intermediate representations.", "method": "Apply distributed inference (split computing) to a LiDAR 3D detection pipeline; evaluate splits after voxelization and at various internal network layers; measure inference time and edge-device execution time reductions.", "result": "Splitting after voxelization reduced total inference time by 70.8% and edge execution time by 90.0%. Splits within the network reduced inference time by up to 57.1% and edge execution time by up to 69.5%.", "conclusion": "Split computing can greatly reduce processing time and edge compute load for LiDAR 3D detection while reducing data transmitted; optimal split points (e.g., after voxelization) yield major gains."}}
{"id": "2511.02647", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02647", "abs": "https://arxiv.org/abs/2511.02647", "authors": ["Xiumei Deng", "Zehui Xiong", "Binbin Chen", "Dong In Kim", "Merouane Debbah", "H. Vincent Poor"], "title": "Federated Attention: A Distributed Paradigm for Collaborative LLM Inference over Edge Networks", "comment": null, "summary": "Large language models (LLMs) are proliferating rapidly at the edge,\ndelivering intelligent capabilities across diverse application scenarios.\nHowever, their practical deployment in collaborative scenarios confronts\nfundamental challenges: privacy vulnerabilities, communication overhead, and\ncomputational bottlenecks. To address these, we propose Federated Attention\n(FedAttn), which integrates the federated paradigm into the self-attention\nmechanism, creating a new distributed LLM inference framework that\nsimultaneously achieves privacy protection, communication efficiency, and\ncomputational efficiency. FedAttn enables participants to perform local\nself-attention over their own token representations while periodically\nexchanging and aggregating Key-Value (KV) matrices across multiple Transformer\nblocks, collaboratively generating LLM responses without exposing private\nprompts. Further, we identify a structural duality between contextual\nrepresentation refinement in FedAttn and parameter optimization in FL across\nprivate data, local computation, and global aggregation. This key insight\nprovides a principled foundation for systematically porting federated\noptimization techniques to collaborative LLM inference. Building on this\nframework, we theoretically analyze how local self-attention computation within\nparticipants and heterogeneous token relevance among participants shape error\npropagation dynamics across Transformer blocks. Moreover, we characterize the\nfundamental trade-off between response quality and communication/computation\nefficiency, which is governed by the synchronization interval and the number of\nparticipants. Experimental results validate our theoretical analysis, and\nreveal significant optimization opportunities through sparse attention and\nadaptive KV aggregation, highlighting FedAttn's potential to deliver\nscalability and efficiency in real-world edge deployments.", "AI": {"tldr": "FedAttn is a distributed LLM inference framework that embeds federated principles into self-attention: participants compute local attention over their tokens and periodically exchange/aggregate Key-Value (KV) matrices across Transformer blocks to collaboratively generate responses while keeping prompts local. The paper proves a duality with federated optimization, analyzes error propagation and trade-offs between quality and communication/computation, and validates gains experimentally using sparse attention and adaptive KV aggregation.", "motivation": "Edge and collaborative LLM deployments face privacy risks (private prompts), high communication costs, and computation constraints. The authors aim to enable collaborative inference that preserves privacy, reduces communication, and offloads computation across participants.", "method": "Introduce Federated Attention (FedAttn): local self-attention per participant; periodic exchange and aggregation of KV matrices across multiple Transformer blocks; map contextual refinement to federated optimization to adapt federated techniques; theoretical analysis of error propagation due to local computation and heterogeneous token relevance; derive trade-off governed by synchronization interval and participant count; propose sparse attention and adaptive KV aggregation as optimizations.", "result": "Theoretical bounds characterize how local attention and token heterogeneity affect error propagation across blocks and how synchronization/participant count affect quality vs. cost trade-offs. Experiments confirm theoretical predictions and show that sparse attention and adaptive KV aggregation substantially reduce communication/computation while retaining response quality.", "conclusion": "FedAttn provides a principled framework for privacy-aware, communication- and computation-efficient distributed LLM inference by treating attention aggregation as a federated process. It exposes tunable trade-offs (sync interval, participant number) and optimization opportunities (sparsity, adaptive aggregation) for scalable edge deployments."}}
