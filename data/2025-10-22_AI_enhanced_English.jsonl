{"id": "2510.17852", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17852", "abs": "https://arxiv.org/abs/2510.17852", "authors": ["Yuze Sun", "Wentao Luo", "Yanfei Xiang", "Jiancheng Pan", "Jiahao Li", "Quan Zhang", "Xiaomeng Huang"], "title": "Deploying Atmospheric and Oceanic AI Models on Chinese Hardware and Framework: Migration Strategies, Performance Optimization and Analysis", "comment": null, "summary": "With the growing role of artificial intelligence in climate and weather\nresearch, efficient model training and inference are in high demand. Current\nmodels like FourCastNet and AI-GOMS depend heavily on GPUs, limiting hardware\nindependence, especially for Chinese domestic hardware and frameworks. To\naddress this issue, we present a framework for migrating large-scale\natmospheric and oceanic models from PyTorch to MindSpore and optimizing for\nChinese chips, and evaluating their performance against GPUs. The framework\nfocuses on software-hardware adaptation, memory optimization, and parallelism.\nFurthermore, the model's performance is evaluated across multiple metrics,\nincluding training speed, inference speed, model accuracy, and energy\nefficiency, with comparisons against GPU-based implementations. Experimental\nresults demonstrate that the migration and optimization process preserves the\nmodels' original accuracy while significantly reducing system dependencies and\nimproving operational efficiency by leveraging Chinese chips as a viable\nalternative for scientific computing. This work provides valuable insights and\npractical guidance for leveraging Chinese domestic chips and frameworks in\natmospheric and oceanic AI model development, offering a pathway toward greater\ntechnological independence.", "AI": {"tldr": "The paper presents a framework to migrate large-scale atmospheric/oceanic AI models from PyTorch to MindSpore and optimize them for Chinese domestic chips, achieving similar accuracy while improving operational efficiency and reducing reliance on GPUs.", "motivation": "Existing weather/climate AI models are GPU- and PyTorch-centric, creating hardware and software dependency; the work aims to enable these models to run efficiently on Chinese chips and frameworks for greater independence.", "method": "Software-hardware adaptation pipeline including code migration to MindSpore, memory optimization, and parallelism strategies tailored to Chinese chips; evaluation across training/inference speed, accuracy, and energy efficiency compared to GPU-based implementations.", "result": "Migration preserves original model accuracy and demonstrates improved operational efficiency and reduced system dependencies when running on Chinese chips; experimental comparisons show competitive training/inference performance and better energy metrics.", "conclusion": "The framework offers practical guidance to leverage Chinese domestic chips and frameworks for atmospheric and oceanic AI, suggesting these chips are a viable alternative to GPUs for scientific computing while maintaining model fidelity."}}
{"id": "2510.18152", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.18152", "abs": "https://arxiv.org/abs/2510.18152", "authors": ["Zhuoyu Yao", "Yue Wang", "Songyang Zhang", "Yingshu Li", "Zhipeng Cai", "Zhi Tian"], "title": "Efficient Multi-Worker Selection based Distributed Swarm Learning via Analog Aggregation", "comment": "5 pages, 4 figures, conference", "summary": "Recent advances in distributed learning systems have introduced effective\nsolutions for implementing collaborative artificial intelligence techniques in\nwireless communication networks. Federated learning approaches provide a\nmodel-aggregation mechanism among edge devices to achieve collaborative\ntraining, while ensuring data security, communication efficiency, and sharing\ncomputational overheads. On the other hand, limited transmission resources and\ncomplex communication environments remain significant bottlenecks to the\nefficient collaborations among edge devices, particularly within large-scale\nnetworks. To address such issues, this paper proposes an over-the-air (OTA)\nanalog aggregation method designed for the distributed swarm learning (DSL),\ntermed DSL-OTA, aiming to enhance communication efficiency, enable effective\ncooperation, and ensure privacy preserving. Incorporating multi-worker\nselection strategy with over-the-air aggregation not only makes the standard\nDSL based on single best worker contributing to global model update to become\nmore federated, but also secures the aggregation from potential risks of data\nleakage. Our theoretical analyses verify the advantages of the proposed DSL-OTA\nalgorithm in terms of fast convergence rate and low communication costs.\nSimulation results reveal that our DSL-OTA outperforms the other existing\nmethods by achieving better learning performance under both homogeneous and\nheterogeneous dataset settings.", "AI": {"tldr": "This paper proposes DSL-OTA, an over-the-air analog aggregation scheme with multi-worker selection for distributed swarm learning (DSL). It aims to improve communication efficiency, cooperation, and privacy compared to single-best-worker DSL. The authors provide theoretical convergence and communication-cost analyses and show in simulations (homogeneous and heterogeneous data) that DSL-OTA outperforms baseline methods.", "motivation": "Large-scale edge networks face limited transmission resources, complex channels, and communication bottlenecks that hinder collaborative model training. Standard DSL relying on a single best worker is less federated and vulnerable to data leakage; an OTA aggregation with multi-worker participation can improve efficiency and privacy.", "method": "Introduce an analog over-the-air aggregation mechanism combined with a multi-worker selection strategy for DSL. Workers transmit analog-modulated model updates simultaneously so the server receives a superposed aggregate. The server uses this OTA-aggregated signal as the global update. The paper presents theoretical convergence analysis and communication cost models under channel/noise assumptions, and conducts simulations comparing DSL-OTA to existing methods under homogeneous and heterogeneous datasets.", "result": "Theoretical results claim faster convergence rates and reduced communication costs for DSL-OTA. Simulation results show improved learning performance relative to baselines across both homogeneous and heterogeneous dataset settings.", "conclusion": "DSL-OTA makes DSL more federated and communication-efficient while offering privacy-preserving aggregation. The approach is theoretically justified and empirically shown to beat existing methods in the considered scenarios."}}
{"id": "2510.18544", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.18544", "abs": "https://arxiv.org/abs/2510.18544", "authors": ["Pan Zhou", "Yiming Lei", "Ling Liu", "Xiaoqiong Xu", "Ying Cai", "Daji Ergu", "Hongfang Yu", "Yueyue Dai"], "title": "SLICE: SLO-Driven Scheduling for LLM Inference on Edge Computing Devices", "comment": null, "summary": "Large Language Models (LLMs), as the foundational architecture for\nnext-generation interactive AI applications, not only power intelligent\ndialogue systems but also drive the evolution of embodied intelligence on edge\ndevices, including humanoid robots, smart vehicles, and other scenarios. The\napplications running on these edge devices impose differentiated Service Level\nObjectives (SLO) requirements on LLM services, specifically manifested as\ndistinct constraints on Time to First Token (TTFT) and Time Per Output Token\n(TPOT) as well as end-to-end latency. Notably, edge devices typically handle\nreal-time tasks that are extremely sensitive to latency, such as machine\ncontrol and navigation planning. However, existing scheduling service systems\nstill prioritize maximizing output token throughput as the sole optimization\nobjective, failing to adequately address the diversity of SLO requirements.\nThis ultimately results in persistently high violation rates for end-to-end\nlatency or TPOT related SLOs.\n  This paper proposes SLICE, an innovative scheduling solution designed for\nedge computing scenarios with differentiated SLO requirements. By combining a\nutility-maximizing request scheduling algorithm with a dynamic iterative\ncontrol mechanism for generation rates, SLICE significantly improves LLM\ninference service SLO attainment. Experimental results demonstrate that\ncompared to state-of-the-art solutions Orca and FastServe, SLICE achieves up to\n35x higher SLO attainment and 3.4x advantage in task completion time than the\nother two solutions.", "AI": {"tldr": "SLICE is a scheduler for edge LLM inference that optimizes differentiated SLOs (TTFT, TPOT, end-to-end latency) using a utility-maximizing request scheduler combined with a dynamic iterative control for generation rate; experimentally it reports up to 35\u00d7 higher SLO attainment and 3.4\u00d7 faster task completion than Orca and FastServe.", "motivation": "Edge devices (humanoids, vehicles, etc.) run latency-sensitive LLM-powered tasks with heterogeneous SLOs (first-token latency, per-token generation latency, and end-to-end response time). Existing LLM serving schedulers focus on maximizing token throughput and therefore fail to meet diverse, stringent latency SLOs for real-time edge workloads.", "method": "SLICE integrates a utility-maximizing scheduling algorithm that prioritizes requests based on their SLO utility and a dynamic iterative control mechanism that adaptively adjusts generation rates to meet different SLO constraints. The scheduler balances throughput and latency objectives to reduce SLO violations in edge scenarios.", "result": "Compared to Orca and FastServe, SLICE reportedly achieves up to 35\u00d7 higher SLO attainment and a 3.4\u00d7 improvement in task completion time, demonstrating substantially reduced violation rates for latency- and TPOT-sensitive workloads.", "conclusion": "SLICE appears to be an effective approach to serving LLMs on edge devices with heterogeneous latency SLOs. The paper shows strong empirical benefits, though clarity is needed on the utility formulation, algorithmic complexity, overheads, robustness, and experimental setup to validate generality and reproducibility."}}
{"id": "2510.18058", "categories": ["cs.NI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.18058", "abs": "https://arxiv.org/abs/2510.18058", "authors": ["Hongbo Lu", "Junsung Hwang", "Bernard Tenreiro", "Nabila Jaman Tripti", "Darren Hamilton", "Yuefan Deng"], "title": "A New Broadcast Model for Several Network Topologies", "comment": "19 pages, 11 figures", "summary": "We present Broadcast by Balanced Saturation (BBS), a general broadcast\nalgorithm designed to optimize communication efficiency across diverse network\ntopologies. BBS maximizes node utilization, addressing challenges in broadcast\noperations such as topology constraints, bandwidth limitations, and\nsynchronization overhead, particularly in large-scale systems like\nsupercomputers. The algorithm ensures sustained activity with nodes throughout\nthe broadcast, thereby enhancing data propagation and significantly reducing\nlatency. Through a precise communication cycle, BBS provides a repeatable,\nstreamlined, stepwise broadcasting framework. Simulation results across various\ntopologies demonstrate that the BBS algorithm consistently outperforms common\ngeneral broadcast algorithms, often by a substantial margin. These findings\nsuggest that BBS is a versatile and robust framework with the potential to\nredefine broadcast strategies across network topologies.", "AI": {"tldr": "BBS is a general broadcast algorithm that maximizes node utilization via a repeatable, stepwise communication cycle to reduce latency and improve data propagation across diverse network topologies; simulation claims show it outperforms common general broadcast algorithms.", "motivation": "Reduce broadcast latency and inefficiency in large-scale systems (e.g., supercomputers) by addressing topology constraints, bandwidth limits, and synchronization overhead; keep nodes active throughout broadcast to improve throughput and utilization.", "method": "A \u2018\u2018Balanced Saturation\u2019\u2019 scheduling strategy that enforces a precise, repeatable communication cycle so nodes remain engaged and links are balanced. The algorithm provides a stepwise broadcast framework tailored to different topologies to maximize sustained activity and data propagation.", "result": "Simulations across multiple topologies indicate BBS consistently outperforms standard general broadcast algorithms, often by a substantial margin, exhibiting lower latency and higher node/link utilization.", "conclusion": "BBS is presented as a versatile, robust broadcast framework with potential to improve or redefine broadcast strategies across varied network topologies."}}
{"id": "2510.18586", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.18586", "abs": "https://arxiv.org/abs/2510.18586", "authors": ["Zhuohang Bian", "Feiyang Wu", "Teng Ma", "Youwei Zhuo"], "title": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based Multi-Agent Applications", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM.", "AI": {"tldr": "Tokencake is a KV-cache-focused serving framework for multi-agent LLM workloads with frequent external function calls. It uses Space Scheduler (dynamic memory partitioning to protect critical agents) and Time Scheduler (proactive offload and predictive upload during stalls) to reduce eviction and repurpose idle GPU memory. On benchmarks it reports >47.06% latency reduction and up to 16.9% better effective GPU memory utilization vs vLLM.", "motivation": "Multi-agent applications that perform external function calls create two KV-Cache problems: space contention (evicting critical agents' caches) and time underutilization (GPU memory occupied by agents stalled on long tool calls). These lead to higher latency and poor GPU efficiency.", "method": "Agent-aware co-optimization of scheduling and memory management. Space Scheduler dynamically partitions KV cache to shield critical agents from eviction. Time Scheduler proactively offloads KV cache for stalled agents and uses predictive upload to restore caches before agent resumption, reclaiming GPU memory during tool calls.", "result": "Evaluation on representative multi-agent benchmarks shows Tokencake reduces end-to-end latency by over 47.06% and improves effective GPU memory utilization by up to 16.9% compared to vLLM.", "conclusion": "By treating the KV cache as a first-class resource and combining dynamic partitioning with predictive offload/upload, Tokencake substantially improves latency and GPU memory utilization in multi-agent LLM-serving scenarios and is a promising approach for function-call-heavy workloads."}}
{"id": "2510.18550", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.18550", "abs": "https://arxiv.org/abs/2510.18550", "authors": ["Enhan Li", "Hongyang Du"], "title": "JAUNT: Joint Alignment of User Intent and Network State for QoE-centric LLM Tool Routing", "comment": null, "summary": "Large Language Models (LLMs) increasingly rely on emerging protocols such as\nthe Model Context Protocol (MCP) to invoke external tools and services.\nHowever, current tool routing mechanisms remain fragile because they only\nconsider functional matching between users' queries and tools. In practice,\nuser intent expressed through queries can be vague or underspecified, and the\nactual Quality of Experience (QoE) also depends on external factors such as\nlink latency and server availability that are not captured by semantics alone.\nTo address this challenge, we propose JAUNT, a framework for Joint Alignment of\nUser intent and Network state in QoE-centric Tool routing. JAUNT introduces a\ndual-view alignment strategy that interprets user intent while employing LLM\nagents to construct network profiles, mapping numerical performance indicators\ninto the semantic space to guide routing. We further design a benchmark that\nintegrates diverse user request patterns with heterogeneous network states,\nenabling systematic evaluation of QoE outcomes. Experimental results show that\nJAUNT significantly improves QoE compared with several baselines, demonstrating\nthe importance of aligning both intent and network state for scalable LLM\nservice orchestration.", "AI": {"tldr": "JAUNT is a framework that routes LLM tool calls by jointly aligning user intent and network state to optimize Quality of Experience (QoE). It uses a dual-view strategy where LLMs interpret intent and construct semantic network profiles from numerical metrics, then routes requests accordingly. A benchmark of varied user patterns and network conditions shows QoE gains over baselines.", "motivation": "Existing tool routing for LLMs focuses on semantic/functional matching and ignores network factors (latency, availability) and underspecified user intent, causing fragile QoE in practice. The paper aims to close this gap by incorporating network state into routing decisions and aligning it with interpreted user intent.", "method": "Introduce JAUNT: (1) a dual-view alignment\u2014intent interpretation by LLM agents and network profiling that maps numerical performance indicators to semantic descriptors; (2) a routing mechanism that uses the combined semantic space to select tools/services optimizing QoE; (3) a benchmark that mixes diverse user requests and heterogeneous network states for evaluation.", "result": "Empirical results on the proposed benchmark show JAUNT significantly improves QoE compared to several baselines that rely on semantic-only or naive routing, demonstrating benefits of joint intent-network alignment.", "conclusion": "Aligning user intent with network state in the semantic space enables more robust, QoE-centric tool routing for LLMs. JAUNT provides a practical framework and benchmark that yield improved end-user experience and can guide future LLM service orchestration work."}}
