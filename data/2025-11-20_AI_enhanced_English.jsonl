{"id": "2511.14921", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.14921", "abs": "https://arxiv.org/abs/2511.14921", "authors": ["Mohamed Rouili", "Yang Xiao", "Sihang Liu", "Raouf Boutaba"], "title": "RAID: In-Network RA Signaling Storm Detection for 5G Open RAN", "comment": null, "summary": "The disaggregation and virtualization of 5G Open RAN (O-RAN) introduces new vulnerabilities in the control plane that can greatly impact the quality of service (QoS) of latency-sensitive 5G applications and services. One critical issue is Random Access (RA) signaling storms where, a burst of illegitimate or misbehaving user equipments (UEs) send Radio Resource Control (RRC) connection requests that rapidly saturate a Central Unit's (CU) processing pipeline. Such storms trigger widespread connection failures within the short contention resolution window defined by 3GPP. Existing detection and mitigation approaches based on near-real-time RAN Intelligent Controller (n-RT RIC) applications cannot guarantee a timely reaction to such attacks as RIC control loops incur tens to hundreds of milliseconds of latency due to the non-deterministic nature of their general purpose processor (GPP) based architectures. This paper presents RAID, an in-network RA signaling storm detection and mitigation system that leverages P4-programmable switch ASICs to enable real-time protection from malicious attacks. RAID embeds a lightweight Random Forest (RF) classifier into a programmable Tofino switch, enabling line-rate flow classification with deterministic microsecond-scale inference delay. By performing ML-based detection directly in the data plane, RAID catches and filters malicious RA requests before they reach and overwhelm the RRC. RAID achieves above 94% detection accuracy with a fixed per-flow inference delay on the order of 3.4 microseconds, effectively meeting strict O-RAN control-plane deadlines. These improvements are sustained across multiple traffic loads, making RAID a fast and scalable solution for the detection and mitigation of signaling storms in 5G O-RAN.", "AI": {"tldr": "RAID implements an in-network Random Forest classifier directly on a P4-programmable Tofino switch to detect and filter Random Access signaling storms in 5G O-RAN at line rate. It reports >94% detection accuracy and a fixed per-flow inference delay of ~3.4 \u03bcs, enabling deterministic, real-time mitigation before RA requests reach the CU/RRC and outperforming slower n-RT RIC approaches.", "motivation": "Disaggregated O-RAN control planes are vulnerable to RA signaling storms where bursts of misbehaving UEs can saturate CU processing and cause mass connection failures. Existing n-RT RIC-based defenses are too slow due to GPP-based latencies. There is a need for deterministic, microsecond-scale mitigation in the data plane.", "method": "Embed a lightweight Random Forest ML model into a P4 program running on a Tofino ASIC so that per-flow RA requests are classified in the data plane at line rate. Use compact feature set and RF implementation adapted to match-action pipelines, then filter identified malicious RA flows before they reach the RRC/CU. Evaluate detection accuracy, per-flow inference latency, and behavior under multiple traffic loads.", "result": "RAID achieves over 94% detection accuracy and a fixed per-flow inference delay of approximately 3.4 \u03bcs. The system sustains these metrics across multiple traffic load conditions, meeting O-RAN control-plane timing requirements and preventing CU overloads.", "conclusion": "Data-plane ML on programmable switches is a viable, fast, and scalable approach to protecting O-RAN control plane from signaling storms. RAID demonstrates that lightweight classifiers can be implemented on ASICs to provide deterministic, line-rate defense, offering an effective complement or alternative to slower RIC-based mitigation."}}
{"id": "2511.14852", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14852", "abs": "https://arxiv.org/abs/2511.14852", "authors": ["Mingkun Yu", "Heming Zhong", "Dan Huang", "Yutong Lu", "Jiazhi Jiang"], "title": "PolyKAN: Efficient Fused GPU Operators for Polynomial Kolmogorov-Arnold Network Variants", "comment": null, "summary": "Kolmogorov-Arnold Networks (KANs) promise higher expressive capability and stronger interpretability than Multi-Layer Perceptron, particularly in the domain of AI for Science. However, practical adoption has been hindered by low GPU utilization of existing parallel implementations. To address this challenge, we present a GPU-accelerated operator library, named PolyKAN which is the first general open-source implementation of KAN and its variants. PolyKAN fuses the forward and backward passes of polynomial KAN layers into a concise set of optimized CUDA kernels. Four orthogonal techniques underpin the design: (i) \\emph{lookup-table} with linear interpolation that replaces runtime expensive math-library functions; (ii) \\emph{2D tiling} to expose thread-level parallelism with preserving memory locality; (iii) a \\emph{two-stage reduction} scheme converting scattered atomic updates into a single controllable merge step; and (iv) \\emph{coefficient-layout reordering} yielding unit-stride reads under the tiled schedule. Using a KAN variant, Chebyshev KAN, as a case-study, PolyKAN delivers $1.2$--$10\\times$ faster inference and $1.4$--$12\\times$ faster training than a Triton + cuBLAS baseline, with identical accuracy on speech, audio-enhancement, and tabular-regression workloads on both highend GPU and consumer-grade GPU.", "AI": {"tldr": "PolyKAN is an open-source GPU operator library that implements polynomial Kolmogorov\u2013Arnold Networks (KANs). It fuses forward/backward passes into optimized CUDA kernels and uses four techniques (lookup-table interpolation, 2D tiling, two-stage reduction, coefficient-layout reordering). On a Chebyshev-KAN case study it reports 1.2\u201310\u00d7 inference and 1.4\u201312\u00d7 training speedups vs a Triton+cuBLAS baseline, with identical accuracy on multiple workloads and GPUs.", "motivation": "KANs offer higher expressivity and interpretability than MLPs (useful in AI for Science) but have seen poor GPU utilization in existing parallel implementations, limiting practical adoption. The goal is to provide a high-performance, general, open-source implementation to make KANs usable at scale.", "method": "Design and implement fused forward/backward polynomial KAN kernels in CUDA. Key optimizations: (i) replace costly math-library calls with lookup tables plus linear interpolation; (ii) 2D tiling to increase thread-level parallelism while preserving locality; (iii) two-stage reduction to convert scattered atomics into a single merge step; (iv) reorder coefficient layout to achieve unit-stride reads under tiling. Evaluate using Chebyshev KAN across speech, audio-enhancement, and tabular-regression tasks on high-end and consumer GPUs.", "result": "PolyKAN achieves 1.2\u201310\u00d7 faster inference and 1.4\u201312\u00d7 faster training than a Triton+cuBLAS baseline with equivalent accuracy across tested workloads and hardware. It is presented as the first general open-source KAN implementation.", "conclusion": "The library substantially closes the GPU-utilization gap for KANs, enabling faster training and inference with no accuracy loss, and may accelerate adoption of KANs in AI-for-Science and other domains."}}
