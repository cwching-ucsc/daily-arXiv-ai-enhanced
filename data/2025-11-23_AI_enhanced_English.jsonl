{"id": "2511.15950", "categories": ["cs.DC", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.15950", "abs": "https://arxiv.org/abs/2511.15950", "authors": ["Michael V. DeBole", "Rathinakumar Appuswamy", "Neil McGlohon", "Brian Taba", "Steven K. Esser", "Filipp Akopyan", "John V. Arthur", "Arnon Amir", "Alexander Andreopoulos", "Peter J. Carlson", "Andrew S. Cassidy", "Pallab Datta", "Myron D. Flickner", "Rajamohan Gandhasri", "Guillaume J. Garreau", "Megumi Ito", "Jennifer L. Klamo", "Jeffrey A. Kusnitz", "Nathaniel J. McClatchey", "Jeffrey L. McKinstry", "Tapan K. Nayak", "Carlos Ortega Otero", "Hartmut Penner", "William P. Risk", "Jun Sawada", "Jay Sivagnaname", "Daniel F. Smith", "Rafael Sousa", "Ignacio Terrizzano", "Takanori Ueda", "Trent Gray-Donald", "David Cox", "Dharmendra S. Modha"], "title": "A Scalable NorthPole System with End-to-End Vertical Integration for Low-Latency and Energy-Efficient LLM Inference", "comment": null, "summary": "A vertically integrated, end-to-end, research prototype system combines 288 NorthPole neural inference accelerator cards, offline training algorithms, a high-performance runtime stack, and a containerized inference pipeline to deliver a scalable and efficient cloud inference service. The system delivers 115 peta-ops at 4-bit integer precision and 3.7 PB/s of memory bandwidth across 18 2U servers, while consuming only 30 kW of power and weighing 730 kg in a 0.67 m^2 42U rack footprint. The system can run 3 simultaneous instances of the 8-billion-parameter open-source IBM Granite-3.3-8b-instruct model at 2,048 context length with 28 simultaneous users and a per-user inter-token latency of 2.8 ms. The system is scalable, modular, and reconfigurable, supporting various model sizes and context lengths, and is ideal for deploying agentic workflows for enterprise AI applications in existing data center (cloud, on-prem) environments. For example, the system can support 18 instances of a 3-billion-parameter model or a single instance of a 70-billion-parameter model.", "AI": {"tldr": "A rack-scale inference prototype using 288 NorthPole accelerator cards across 18 2U servers delivers 115 peta-ops at 4\u2011bit integer precision and 3.7 PB/s memory bandwidth, using 30 kW and a 0.67 m^2, 42U footprint. It runs multiple instances of large open-source LMs (e.g., three instances of an 8B instruct model at 2,048 context with 2.8 ms inter-token latency per user) and can reconfigure to support other model sizes (e.g., 18\u00d73B or one 70B).", "motivation": "Provide a high-throughput, low-latency, energy- and space-efficient inference platform that can run large language models at production scale in existing data-center environments to enable agentic enterprise AI workflows.", "method": "Vertically integrated system design combining 288 NorthPole neural inference accelerator cards, offline training/quantization algorithms, a high-performance runtime stack, and a containerized inference pipeline. Uses 4\u2011bit integer inference, a specialized memory hierarchy delivering 3.7 PB/s, and a distributed runtime across 18 2U servers packed into a 42U rack.", "result": "System-level measurements: 115 peta-ops compute, 3.7 PB/s memory bandwidth, 30 kW power draw, 730 kg weight, 0.67 m^2 footprint. Achieved service-level: 3 concurrent instances of IBM Granite-3.3-8b-instruct at 2048 context, 28 simultaneous users, 2.8 ms per-user inter-token latency. Flexible scaling to other model sizes.", "conclusion": "The prototype demonstrates that dense, modular accelerator-based racks can deliver high-efficiency, low-latency inference for multi-instance LLM deployment in datacenter settings and is suitable for enterprise agentic applications when paired with appropriate runtime and container orchestration."}}
{"id": "2511.15977", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PF", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2511.15977", "abs": "https://arxiv.org/abs/2511.15977", "authors": ["Daniel Mas Montserrat", "Ray Verma", "M\u00edriam Barrab\u00e9s", "Francisco M. de la Vega", "Carlos D. Bustamante", "Alexander G. Ioannidis"], "title": "Efficient Chromosome Parallelization for Precision Medicine Genomic Workflows", "comment": "Accepted at AAAI 2026", "summary": "Large-scale genomic workflows used in precision medicine can process datasets spanning tens to hundreds of gigabytes per sample, leading to high memory spikes, intensive disk I/O, and task failures due to out-of-memory errors. Simple static resource allocation methods struggle to handle the variability in per-chromosome RAM demands, resulting in poor resource utilization and long runtimes. In this work, we propose multiple mechanisms for adaptive, RAM-efficient parallelization of chromosome-level bioinformatics workflows. First, we develop a symbolic regression model that estimates per-chromosome memory consumption for a given task and introduces an interpolating bias to conservatively minimize over-allocation. Second, we present a dynamic scheduler that adaptively predicts RAM usage with a polynomial regression model, treating task packing as a Knapsack problem to optimally batch jobs based on predicted memory requirements. Additionally, we present a static scheduler that optimizes chromosome processing order to minimize peak memory while preserving throughput. Our proposed methods, evaluated on simulations and real-world genomic pipelines, provide new mechanisms to reduce memory overruns and balance load across threads. We thereby achieve faster end-to-end execution, showcasing the potential to optimize large-scale genomic workflows.", "AI": {"tldr": "Adaptive, RAM-aware scheduling for chromosome-level genomic tasks using per-chromosome symbolic regression, a dynamic knapsack-based scheduler with polynomial RAM prediction, and a static order-optimizer reduces memory overruns and speeds end-to-end pipelines.", "motivation": "Large genomic workflows show high variability in per-chromosome RAM needs; static allocation either wastes memory or causes out-of-memory failures, hurting throughput and utilization on shared systems.", "method": "1) Learn a symbolic-regression model to predict per-chromosome memory usage with an interpolating conservative bias to avoid underestimation. 2) Dynamic scheduler: use polynomial regression to predict task RAM and pack tasks into batches by solving a Knapsack-style packing problem to maximize parallelism under RAM limits. 3) Static scheduler: compute an optimized chromosome processing order that minimizes peak memory while keeping throughput.", "result": "On simulations and real genomic pipelines the methods reduced memory overruns, improved load balancing across threads, and achieved faster end-to-end execution compared to simple static allocation. They demonstrate fewer task failures and better utilization of memory resources.", "conclusion": "Predictive, conservative per-chromosome memory models combined with packing-aware scheduling provide a practical approach to RAM-efficient parallelization of chromosome-level bioinformatics workflows; further work could extend models to I/O, time prediction, and online adaptation."}}
{"id": "2511.16193", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16193", "abs": "https://arxiv.org/abs/2511.16193", "authors": ["Rongxin Cheng", "Kai Zhou", "Xingda Wei", "Siyuan Liu", "Mingcong Han", "Mingjing Ai", "Yeju Zhou", "Baoquan Zhong", "Wencong Xiao", "Xin Liu", "Rong Chen", "Haibo Chen"], "title": "Fast LLM Post-training via Decoupled and Best-of-N Speculation", "comment": null, "summary": "Rollout dominates the training time in large language model (LLM) post-training, where the trained model is used to generate tokens given a batch of prompts. SpecActor achieves fast rollout with speculative decoding that deploys a fast path (e.g., a smaller model) to accelerate the unparallelizable generation, while the correctness is guaranteed by fast parallel verification of the outputs with the original model. SpecActor addresses two foundational challenges in speculative rollout by (1) a \\emph{dynamic decoupled speculation} execution method that maximizes the GPU computational efficiency to realize speedup for large-batch execution -- a configuration common in training but unfriendly to speculative execution and (2) a \\emph{dynamic Best-of-N speculation} method that selects and combines different drafting methods according to the rollout progress. It substantially improves the speculation accuracy even when the best drafting method is unknown a priori, meanwhile without requiring adding extra computation resources. {\\sys} is {1.3--1.7}\\,$\\times$ faster than common post-training baselines, and is {1.3--1.5}\\,$\\times$ faster compared to naively adopting speculative decoding for rollout.", "AI": {"tldr": "SpecActor speeds up LLM post-training rollout by applying speculative decoding tailored for large-batch training. It uses a dynamic decoupled speculation execution to improve GPU utilization and a dynamic Best-of-N drafting strategy to pick/merge drafting methods during generation, yielding 1.3\u20131.7\u00d7 speedups over baselines and 1.3\u20131.5\u00d7 over naive speculative decoding.", "motivation": "Rollout (model inference over many prompts) dominates post-training time for LLMs. Existing speculative decoding helps generation latency but is ill-suited to large-batch training scenarios and requires choosing a good drafting method beforehand. The work aims to make speculative rollout fast and accurate under large-batch conditions without extra compute.", "method": "(1) Dynamic decoupled speculation: re-architects execution so the fast path (drafting model) and verification with the original model are decoupled and scheduled to maximize GPU utilization in large-batch settings. (2) Dynamic Best-of-N speculation: adaptively selects and combines multiple drafting methods during rollout, improving drafting accuracy when the best drafter is unknown, with no added compute resource requirement.", "result": "{\\sys} achieves 1.3\u20131.7\u00d7 speedup over common post-training baselines and 1.3\u20131.5\u00d7 speedup relative to a straightforward application of speculative decoding for rollout. It also substantially improves speculation accuracy under unknown best-drafter conditions.", "conclusion": "By combining an execution design optimized for large-batch GPU efficiency and an adaptive drafting selection strategy, SpecActor provides practical, resource-efficient speculative rollout improvements, yielding meaningful end-to-end speedups without extra hardware."}}
{"id": "2511.16450", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.16450", "abs": "https://arxiv.org/abs/2511.16450", "authors": ["Ziyue Xu", "Zhihong Zhang", "Holger R. Roth", "Chester Chen", "Yan Cheng", "Andrew Feng"], "title": "Optimizing Federated Learning in the Era of LLMs: Message Quantization and Streaming", "comment": "FLLM 2025", "summary": "Federated Learning (FL) offers a promising solution for training machine learning models across distributed data sources while preserving data privacy. However, FL faces critical challenges related to communication overhead and local resource constraints, especially in the era of Large Language Models (LLMs) with billions of parameters. The sheer size of these models exacerbates both memory and communication constraints, making efficient transmission and processing essential for practical deployment. NVIDIA FLARE, an open-source SDK for federated learning, addresses these challenges by introducing advanced communication capabilities. Building upon existing solutions for large object streaming, we enhance FL workflows for LLMs through two key techniques: message quantization and container/file streaming. Quantization reduces message size, while streaming enables efficient memory management, improving scalability and integration with existing workflows. These advancements significantly enhance the robustness and efficiency of FL with LLMs, ensuring better performance in real-world federated learning scenarios.", "AI": {"tldr": "Proposes two practical enhancements to NVIDIA FLARE\u2014message quantization and container/file streaming\u2014to reduce communication and memory overhead when federated learning large language models, claiming improved scalability and robustness.", "motivation": "Federated Learning struggles with bandwidth and memory limits as Large Language Models grow to billions of parameters. Efficient transmission and memory management are essential for practical, privacy-preserving distributed training.", "method": "Extend NVIDIA FLARE\u2019s large-object streaming capabilities using (1) message quantization to compress messages before transmission, and (2) container/file streaming to transfer model artifacts and large payloads in a streamable, memory-efficient fashion\u2014integrating with existing FL workflows.", "result": "These techniques reduce message size and enable efficient memory handling, improving scalability and integration for FL with LLMs. The abstract claims significant improvements in robustness and efficiency but provides no quantitative metrics.", "conclusion": "Message quantization and streaming make federated learning of LLMs more practical by lowering communication and memory bottlenecks, though empirical validation and details are needed for assessing trade-offs like accuracy loss and security."}}
