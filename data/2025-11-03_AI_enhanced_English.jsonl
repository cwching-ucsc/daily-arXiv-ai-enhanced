{"id": "2510.26913", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.26913", "abs": "https://arxiv.org/abs/2510.26913", "authors": ["Junyi Shen", "Noppanat Wadlom", "Lingfeng Zhou", "Dequan Wang", "Xu Miao", "Lei Fang", "Yao Lu"], "title": "FlowMesh: A Service Fabric for Composable LLM Workflows", "comment": null, "summary": "AI deployment increasingly resembles a pipeline of data transformation,\nfine-tuning, and agent interactions rather than a monolithic LLM job; recent\nexamples include RLHF/RLAIF training and agentic workflows. To cope with this\nshift, we propose FlowMesh, a multi-tenant service fabric that executes and\noptimizes these workloads as one shared service instead of isolated pipelines.\nIt decomposes workflows into fine-grained operators with recorded lineage,\nenabling de-duplication of work across users and batching requests on the same\nhardware while preserving per-workflow provenance. A global control plane\nmaintains a cluster-wide pool of ready operators and uses a single utility\nfunction to pick both the batch and the worker, balancing throughput, cost, and\ndata locality on heterogeneous GPUs. The data plane is an elastic fleet of\nstateless workers backed by a content-addressable store, enabling rapid,\nautomatic scale-out, safe retry after preemption, and portability across\nmanaged clusters such as Kubernetes and geo-distributed GPU marketplaces such\nas Vast.ai. Compared with baseline solutions, FlowMesh achieves up to 3.8x cost\nreduction and 2.0x lower energy usage, provides a similar or better latency\nprofile, and remains efficient under dynamic and failure-prone conditions.", "AI": {"tldr": "FlowMesh is a multi-tenant execution fabric that decomposes agentic and pipelined AI workloads into fine-grained operators with lineage, enabling cross-user deduplication and batching. A global control plane schedules batches and workers using a single utility function balancing throughput, cost, and locality on heterogeneous GPUs. A stateless worker fleet backed by a content-addressable store provides elasticity, fault-recovery, and portability. Reported gains: up to 3.8x cost reduction, 2x lower energy, comparable or better latency, and robustness under dynamic/failure conditions.", "motivation": "Modern AI deployments are becoming pipelines of transformations, fine-tuning, and agent interactions (e.g., RLHF/RLAIF, agent workflows) rather than single monolithic LLM calls. Running these as isolated pipelines wastes work (duplicate computation), underutilizes hardware, and complicates scalability and fault recovery. A shared, optimized service fabric could reduce cost, improve throughput, and provide provenance.", "method": "Decompose workflows into fine-grained operators with recorded lineage to allow deduplication and provenance. Maintain a global control plane that keeps a cluster-wide pool of ready operators and selects batches + workers via a single utility function optimizing throughput, cost, and data locality across heterogeneous GPUs. Implement an elastic, stateless worker fleet backed by a content-addressable store for inputs/artifacts to enable rapid scale-out, safe retry after preemption, and portability across clusters and GPU marketplaces.", "result": "Compared to baseline isolated-pipeline solutions, FlowMesh achieves up to 3.8x cost reduction and 2.0x lower energy consumption, while providing similar or better latency and maintaining efficiency under dynamic load and failures.", "conclusion": "FlowMesh demonstrates that treating AI workloads as a shared fabric of fine-grained, provenance-tracked operators with centralized scheduling and stateless workers can significantly reduce cost and energy while preserving performance and reliability. Further detail on scheduling utility, operator semantics, and security/tenant isolation would aid adoption."}}
{"id": "2510.27257", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.27257", "abs": "https://arxiv.org/abs/2510.27257", "authors": ["Mengshi Qi", "Jiaxuan Peng", "Jie Zhang", "Juan Zhu", "Yong Li", "Huadong Ma"], "title": "Synergistic Tensor and Pipeline Parallelism", "comment": null, "summary": "In the machine learning system, the hybrid model parallelism combining tensor\nparallelism (TP) and pipeline parallelism (PP) has become the dominant solution\nfor distributed training of Large Language Models~(LLMs) and Multimodal LLMs\n(MLLMs). However, TP introduces significant collective communication overheads,\nwhile PP suffers from synchronization inefficiencies such as pipeline bubbles.\nExisting works primarily address these challenges from isolated perspectives,\nfocusing either on overlapping TP communication or on flexible PP scheduling to\nmitigate pipeline bubbles. In this paper, we propose a new synergistic tensor\nand pipeline parallelism schedule that simultaneously reduces both types of\nbubbles. Our proposed schedule decouples the forward and backward passes in PP\ninto fine-grained computation units, which are then braided to form a composite\ncomputation sequence. This compositional structure enables near-complete\nelimination of TP-related bubbles. Building upon this structure, we further\ndesign the PP schedule to minimize PP bubbles. Experimental results demonstrate\nthat our approach improves training throughput by up to 12% for LLMs and 16%\nfor MLLMs compared to existing scheduling methods. Our source code is avaiable\nat https://github.com/MICLAB-BUPT/STP.", "AI": {"tldr": "Paper proposes a synergistic schedule for combined tensor and pipeline parallelism that braids fine-grained forward/backward computation units to eliminate TP communication bubbles and reduce PP pipeline bubbles, yielding up to 12% (LLMs) and 16% (MLLMs) throughput gains over prior schedules.", "motivation": "Hybrid TP+PP is widely used for distributed training of large (multi-modal) LLMs, but TP causes heavy collective-communication overheads while PP creates synchronization inefficiencies (pipeline bubbles). Prior works treat these problems separately, leaving room for a joint scheduling approach that reduces both kinds of inefficiency simultaneously.", "method": "Decouple forward and backward passes in pipeline parallelism into fine-grained computation units. Compose these units into a braided (interleaved) computation sequence that hides or eliminates TP communication waits (TP bubbles). On top of that compositional structure, design a pipeline-parallel schedule that minimizes remaining PP bubbles.", "result": "Empirical evaluation shows improved training throughput up to 12% for LLMs and 16% for multimodal LLMs compared to state-of-the-art scheduling methods. The paper provides source code at the given GitHub link.", "conclusion": "A compositional, braided scheduling strategy that jointly optimizes tensor and pipeline parallelism can substantially reduce communication and synchronization inefficiencies, improving training throughput for large and multimodal LLMs."}}
{"id": "2510.27289", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.27289", "abs": "https://arxiv.org/abs/2510.27289", "authors": ["Zhengchang Hua", "Panagiotis Oikonomou", "Karim Djemame", "Nikos Tziritas", "Georgios Theodoropoulos"], "title": "A Digital Twin-based Multi-Agent Reinforcement Learning Framework for Vehicle-to-Grid Coordination", "comment": "16 pages, 8 figures. Accepted by the 25th International Conference on\n  Algorithms and Architectures for Parallel Processing (ICA3PP'25)", "summary": "The coordination of large-scale, decentralised systems, such as a fleet of\nElectric Vehicles (EVs) in a Vehicle-to-Grid (V2G) network, presents a\nsignificant challenge for modern control systems. While collaborative Digital\nTwins have been proposed as a solution to manage such systems without\ncompromising the privacy of individual agents, deriving globally optimal\ncontrol policies from the high-level information they share remains an open\nproblem. This paper introduces Digital Twin Assisted Multi-Agent Deep\nDeterministic Policy Gradient (DT-MADDPG) algorithm, a novel hybrid\narchitecture that integrates a multi-agent reinforcement learning framework\nwith a collaborative DT network. Our core contribution is a simulation-assisted\nlearning algorithm where the centralised critic is enhanced by a predictive\nglobal model that is collaboratively built from the privacy-preserving data\nshared by individual DTs. This approach removes the need for collecting\nsensitive raw data at a centralised entity, a requirement of traditional\nmulti-agent learning algorithms. Experimental results in a simulated V2G\nenvironment demonstrate that DT-MADDPG can achieve coordination performance\ncomparable to the standard MADDPG algorithm while offering significant\nadvantages in terms of data privacy and architectural decentralisation. This\nwork presents a practical and robust framework for deploying intelligent,\nlearning-based coordination in complex, real-world cyber-physical systems.", "AI": {"tldr": "Proposes DT-MADDPG: a hybrid multi-agent RL (MADDPG) that uses a collaborative Digital Twin network to build a predictive global model for the centralized critic, aiming to preserve agent privacy and decentralize architecture while maintaining coordination performance in a V2G simulation.", "motivation": "Large-scale decentralized systems (e.g., EVs in V2G) require coordinated control but sharing raw data to a central learner raises privacy and architectural concerns. Collaborative Digital Twins can share high-level, privacy-preserving information, but converting that into globally optimal control policies is unresolved.", "method": "Introduce DT-MADDPG: retain MADDPG\u2019s actor-critic structure but replace/augment the centralized critic with a predictive global model assembled from privacy-preserving summaries sent by individual DTs. Learning is simulation-assisted: the global model predicts aggregate dynamics to train the critic without raw data centralization.", "result": "In a simulated V2G scenario, DT-MADDPG matches coordination performance of standard MADDPG while improving privacy and decentralization. The paper claims comparable rewards and coordination metrics in experiments.", "conclusion": "DT-MADDPG offers a practical, privacy-aware, decentralized framework for deploying learning-based coordination in complex cyber-physical systems, enabling multi-agent coordination without central collection of sensitive raw data."}}
{"id": "2510.27656", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.27656", "abs": "https://arxiv.org/abs/2510.27656", "authors": ["Nandor Licker", "Kevin Hu", "Vladimir Zaytsev", "Lequn Chen"], "title": "RDMA Point-to-Point Communication for LLM Systems", "comment": null, "summary": "Emerging Large Language Model (LLM) system patterns, such as disaggregated\ninference, Mixture-of-Experts (MoE) routing, and asynchronous reinforcement\nfine-tuning, require flexible point-to-point communication beyond simple\ncollectives. Existing implementations are locked to specific Network Interface\nControllers (NICs), hindering integration into inference engines and\nportability across hardware providers. We present TransferEngine, which bridges\nthe functionality of common NICs to expose a uniform interface. TransferEngine\nexposes one-sided WriteImm operations with a ImmCounter primitive for\ncompletion notification, without ordering assumptions of network transport,\ntransparently managing multiple NICs per GPU. We demonstrate peak throughput of\n400 Gbps on both NVIDIA ConnectX-7 and AWS Elastic Fabric Adapter (EFA). We\nshowcase TransferEngine through three production systems: (1) KvCache transfer\nfor disaggregated inference with dynamic scaling, (2) RL weight updates\nachieving 1.3 seconds for trillion-parameter models, and (3) MoE\ndispatch/combine implementation exceeding DeepEP decode latency on ConnectX-7,\nwith the first viable latencies on EFA. We demonstrate that our portable\npoint-to-point communication complements collectives while avoiding lock-in.", "AI": {"tldr": "TransferEngine provides a portable NIC-agnostic layer that exposes one-sided WriteImm operations with an ImmCounter completion primitive to implement flexible point-to-point communication needed by modern LLM system patterns (disaggregated inference, MoE routing, async RL fine-tuning). It transparently manages multiple NICs per GPU, achieves 400 Gbps peak throughput on NVIDIA ConnectX-7 and AWS EFA, and is demonstrated in three production systems (KvCache transfer, RL weight updates, MoE dispatch) with improved latencies and throughput while avoiding vendor lock-in.", "motivation": "Emerging LLM system patterns require flexible point-to-point communication beyond collective primitives. Existing implementations are tied to specific NICs, hindering inference engine integration and portability across hardware providers. A uniform, portable interface is needed to support diverse NICs and the point-to-point patterns these systems demand.", "method": "Design and implement TransferEngine as a portability layer that bridges common NIC functionality. It exposes one-sided WriteImm operations and an ImmCounter primitive for completion notification, avoids assuming transport ordering, and manages multiple NICs per GPU transparently. The implementation is benchmarked on different NICs and integrated into production workloads to validate real-world benefits.", "result": "TransferEngine reaches peak throughput of 400 Gbps on both NVIDIA ConnectX-7 and AWS EFA. In production demos it enables: (1) KvCache transfer for disaggregated inference with dynamic scaling, (2) RL weight updates completing in 1.3s for trillion-parameter models, and (3) MoE dispatch/combine with latencies better than DeepEP on ConnectX-7 and first viable latencies on EFA.", "conclusion": "A portable point-to-point communication layer like TransferEngine complements collective primitives, enables vendor-agnostic deployment of advanced LLM system patterns, and avoids NIC lock-in while delivering high throughput and low-latency performance in production scenarios."}}
{"id": "2510.27664", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.27664", "abs": "https://arxiv.org/abs/2510.27664", "authors": ["Niloy Saha", "Noura Limam", "Yang Xiao", "Raouf Boutaba"], "title": "Rethinking Telemetry Design for Fine-Grained Anomaly Detection in 5G User Planes", "comment": null, "summary": "Detecting QoS anomalies in 5G user planes requires fine-grained per-flow\nvisibility, but existing telemetry approaches face a fundamental trade-off.\nCoarse per-class counters are lightweight but mask transient and per-flow\nanomalies, while per-packet telemetry postcards provide full visibility at\nprohibitive cost that grows linearly with line rate. Selective postcard schemes\nreduce overhead but miss anomalies that fall below configured thresholds or\noccur during brief intervals. We present Kestrel, a sketch-based telemetry\nsystem for 5G user planes that provides fine-grained visibility into key metric\ndistributions such as latency tails and inter-arrival times at a fraction of\nthe cost of per-packet postcards. Kestrel extends Count-Min Sketch with\nhistogram-augmented buckets and per-queue partitioning, which compress\nper-packet measurements into compact summaries while preserving\nanomaly-relevant signals. We develop formal detectability guarantees that\naccount for sketch collisions, yielding principled sizing rules and binning\nstrategies that maximize anomaly separability. Our evaluations on a 5G testbed\nwith Intel Tofino switches show that Kestrel achieves 10% better detection\naccuracy than existing selective postcard schemes while reducing export\nbandwidth by 10x.", "AI": {"tldr": "Kestrel is a sketch-based telemetry design for 5G user planes that records compact per-packet measurements (histogram-augmented Count-Min Sketch with per-queue partitioning) to detect QoS anomalies with similar or better accuracy than selective postcard schemes while using ~10x less export bandwidth.", "motivation": "Per-flow, fine-grained visibility is required to detect transient and tail QoS anomalies in 5G user planes, but existing options either aggregate too coarsely (per-class counters) or are too expensive (per-packet postcards). Selective postcards reduce cost but miss short-lived or sub-threshold anomalies.", "method": "Kestrel extends Count-Min Sketch by embedding histograms into sketch buckets and partitioning sketches per egress queue. It compresses per-packet measurements (e.g., latency, inter-arrival times) into compact summaries. The authors develop formal detectability guarantees that model sketch collisions, and derive sizing and binning rules to maximize anomaly separability.", "result": "On a 5G testbed with Intel Tofino switches, Kestrel achieves about 10% higher detection accuracy than selective-postcard baselines and reduces export bandwidth by approximately 10x compared to per-packet telemetry.", "conclusion": "Sketch-based, histogram-augmented telemetry can provide fine-grained, anomaly-relevant visibility in high-speed 5G user planes at drastically lower bandwidth costs than full postcards, provided sketches are sized and binned according to the derived guarantees."}}
