<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 3]
- [cs.NI](#cs.NI) [Total: 1]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [AsyncHZP: Hierarchical ZeRO Parallelism with Asynchronous Scheduling for Scalable LLM Training](https://arxiv.org/abs/2510.20111)
*Huawei Bai,Yifan Huang,Wenqi Shi,Ansheng You,Feifan Shao,Tengfei Han,Minghui Yu*

Main category: cs.DC

TL;DR: AsyncHZP is an asynchronous, hierarchical variant of ZeRO that adaptively reshards states across replica groups and uses multi-stream background communication to reduce overhead and overlap communication with computation, improving large-scale training performance and scalability for Dense and MoE models.


<details>
  <summary>Details</summary>
Motivation: Large-scale LM training suffers from communication and memory bottlenecks: fine-grained ZeRO sharding causes inefficient communication; ND parallelism is complex to manage. There is a need for a simple, memory-efficient method that reduces communication and scales well.

Method: Introduce Asynchronous Hierarchical Zero Parallelism (AsyncHZP): 1) adaptive resharing of parameters, gradients, and optimizer states across different replica groups (hierarchical sharding) to optimize device memory utilization and lower communication; 2) multi-stream asynchronous scheduling, running parameter all-gather and gradient reduce-scatter in dedicated background threads to overlap communication with compute with minimal memory fragmentation.

Result: Empirical results on Dense and Mixture-of-Experts models show robust stability at scale and consistent performance gains over classic ND parallelism. AsyncHZP achieves state-of-the-art wall-clock performance without complex tuning.

Conclusion: AsyncHZP simplifies large-scale model training by reducing communication overhead and improving memory utilization through adaptive hierarchical sharding and async background communication, making it a practical, high-performance alternative to existing parallelism schemes.

Abstract: The training efficiency and scalability of language models on massive
clusters currently remain a critical bottleneck. Mainstream approaches like ND
parallelism are often cumbersome and complex, while flexible alternatives such
as the Zero Redundancy Optimizer (ZeRO) are frequently hampered by
communication overhead. In this paper, we propose Asynchronous Hierarchical
Zero Parallelism (AsyncHZP), a novel asynchronous variant of ZeRO designed to
achieve superior performance while maintaining simplicity and memory
efficiency. Unlike traditional ZeRO, which employs over-fine-grained sharding
that can lead to inefficient communication, AsyncHZP adaptively reshards
parameters, gradients, and optimizer states across different replica groups.
This strategy optimizes device memory utilization and significantly reduces
communication overhead. In addition, we also design a multi-stream asynchronous
scheduling method that executes parameter all-gather and gradient
reduce-scatter operations in dedicated background threads, effectively
overlapping communication with computation while incurring negligible memory
fragmentation. Empirical evaluations on both Dense and Mixture-of-Experts (MoE)
models confirm that AsyncHZP maintains robust stability at scale. It
consistently outperforms classic ND parallelism, achieving state-of-the-art
performance without complex strategic tuning, thereby simplifying the path to
efficient large-scale training.

</details>


### [2] [Collective Communication for 100k+ GPUs](https://arxiv.org/abs/2510.20171)
*Min Si,Pavan Balaji,Yongzhou Chen,Ching-Hsiang Chu,Adi Gangidi,Saif Hasan,Subodh Iyengar,Dan Johnson,Bingzhe Liu,Jingliang Ren,Ashmitha Jeevaraj Shetty,Greg Steinbrecher,Xinfeng Xie,Yulun Wang,Bruce Wu,Jingyi Yang,Mingran Yang,Minlan Yu,Cen Zhao,Wes Bland,Denis Boyda,Suman Gumudavelli,Cristian Lumezanu,Rui Miao,Zhe Qu,Venkat Ramesh,Maxim Samoylov,Jan Seidel,Feng Tian,Qiye Tan,Shuqiang Zhang,Yimeng Zhao,Shengbao Zheng,Art Zhu,Hongyi Zeng*

Main category: cs.DC

TL;DR: Presents NCCLX, a collective communication framework from Meta that targets high-throughput and low-latency data exchange across clusters >100k GPUs, evaluated on Llama4 with reported large gains in communication efficiency.


<details>
  <summary>Details</summary>
Motivation: Training and serving very large LLMs demand communication frameworks that scale beyond existing solutions; current methods struggle with throughput and latency when workloads span tens to hundreds of thousands of GPUs.

Method: Design and engineering of the NCCLX framework to optimize collective communication for both synchronous large-scale training and low-latency inference across massive clusters, emphasizing reliability and efficiency.

Result: Empirical results on Llama4 show substantial improvements in communication efficiency, suggesting higher throughput and lower latency compared to previous approaches (details not provided in abstract).

Conclusion: NCCLX is positioned as a robust, scalable communication solution enabling next-generation LLMs to operate at unprecedented scale.

Abstract: The increasing scale of large language models (LLMs) necessitates highly
efficient collective communication frameworks, particularly as training
workloads extend to hundreds of thousands of GPUs. Traditional communication
methods face significant throughput and latency limitations at this scale,
hindering both the development and deployment of state-of-the-art models. This
paper presents the NCCLX collective communication framework, developed at Meta,
engineered to optimize performance across the full LLM lifecycle, from the
synchronous demands of large-scale training to the low-latency requirements of
inference. The framework is designed to support complex workloads on clusters
exceeding 100,000 GPUs, ensuring reliable, high-throughput, and low-latency
data exchange. Empirical evaluation on the Llama4 model demonstrates
substantial improvements in communication efficiency. This research contributes
a robust solution for enabling the next generation of LLMs to operate at
unprecedented scales.

</details>


### [3] [Accurate Performance Predictors for Edge Computing Applications](https://arxiv.org/abs/2510.20495)
*Panagiotis Giannakopoulos,Bart van Knippenberg,Kishor Chandra Joshi,Nicola Calabretta,George Exarchakos*

Main category: cs.DC

TL;DR: Automated methodology builds and evaluates server-specific performance predictors for edge environments (trained on correlated monitoring metrics) optimizing both accuracy and inference latency; achieves up to 90% accuracy and inference time <1% of RTT, validated on electron microscopy workflows under dynamic co-location.


<details>
  <summary>Details</summary>
Motivation: Achieve predictable application performance in resource-constrained, heterogeneous, co-located edge environments to enable effective scheduling and resource management—particularly for real-time, resource-diverse workloads like electron microscopy workflows.

Method: Automatically construct and assess multiple predictor models using historical monitoring metrics that are most correlated with application performance. Jointly optimize for prediction accuracy and inference latency to select server-specific models. Evaluate predictors across multiple servers and dynamic co-location scenarios, measuring accuracy and inference time relative to Round Trip Time (RTT).

Result: Predictors reach up to 90% accuracy while keeping inference latency below 1% of RTT. Models trained on top-correlated metrics generalize across dynamic co-location scenarios and improve predictability for EM workflows, enabling better resource utilization.

Conclusion: Selecting server-specific predictors by jointly optimizing accuracy and inference latency is necessary in dynamic co-location edge settings. Integrating such predictors into scheduling/resource management can yield predictable performance. Future directions include online adaptation, broader workload evaluation, and addressing model-selection overhead.

Abstract: Accurate prediction of application performance is critical for enabling
effective scheduling and resource management in resource-constrained dynamic
edge environments. However, achieving predictable performance in such
environments remains challenging due to the co-location of multiple
applications and the node heterogeneity. To address this, we propose a
methodology that automatically builds and assesses various performance
predictors. This approach prioritizes both accuracy and inference time to
identify the most efficient model. Our predictors achieve up to 90% accuracy
while maintaining an inference time of less than 1% of the Round Trip Time.
These predictors are trained on the historical state of the most correlated
monitoring metrics to application performance and evaluated across multiple
servers in dynamic co-location scenarios. As usecase we consider electron
microscopy (EM) workflows, which have stringent real-time demands and diverse
resource requirements. Our findings emphasize the need for a systematic
methodology that selects server-specific predictors by jointly optimizing
accuracy and inference latency in dynamic co-location scenarios. Integrating
such predictors into edge environments can improve resource utilization and
result in predictable performance.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [4] [A Tutorial on Cognitive Biases in Agentic AI-Driven 6G Autonomous Networks](https://arxiv.org/abs/2510.19973)
*Hatim Chergui,Farhad Rezazadeh,Merouane Debbah,Christos Verikoukis*

Main category: cs.NI

TL;DR: This paper argues that 6G network autonomy should move beyond KPI optimization toward LLM-powered agentic AI that perceives multimodal telemetry, reasons with memory, negotiates across domains, and acts via APIs. It catalogs cognitive biases that can impair such agents, gives mathematical formulations and telecom-specific emergence points, and proposes tailored mitigations. Two applied use-cases (inter-slice and cross-domain management) demonstrate techniques—anchor randomization, temporal decay, inflection bonus—yielding up to 5× lower latency and ≈40% higher energy savings.


<details>
  <summary>Details</summary>
Motivation: KPIs are insufficient proxies for the true goals of next-gen networks (seamless connectivity, fairness, adaptability, resilience). Agentic LLMs can deliver true autonomy but inherit cognitive biases that distort reasoning, negotiation and actuation in telecom contexts.

Method: Provide a tutorial-style taxonomy and formal definitions of common cognitive biases; map how each bias emerges in telecom agentic components (perception, memory, negotiation, tool use, actuation); propose mitigation strategies per bias. Validate with two practical use-cases applying specific mitigation techniques (anchor randomization, temporal decay, inflection bonus) and measure impact on latency and energy.

Result: Mitigations reduce bias effects in test scenarios: anchored/recency/confirmation biases were mitigated, leading to significantly better negotiation outcomes and resource allocations. Reported gains include 5× lower latency and ~40% energy savings in the second use-case.

Conclusion: Agentic AI is a promising path to 6G autonomy but must confront cognitive biases. The paper offers a taxonomy, mathematical treatments, telecom mappings, mitigation techniques and empirical evidence that targeted debiasing substantially improves performance and robustness.

Abstract: The path to higher network autonomy in 6G lies beyond the mere optimization
of key performance indicators (KPIs). While KPIs have enabled automation gains
under TM Forum Levels 1--3, they remain numerical abstractions that act only as
proxies for the real essence of communication networks: seamless connectivity,
fairness, adaptability, and resilience. True autonomy requires perceiving and
reasoning over the network environment as it is. Such progress can be achieved
through \emph{agentic AI}, where large language model (LLM)-powered agents
perceive multimodal telemetry, reason with memory, negotiate across domains,
and act via APIs to achieve multi-objective goals. However, deploying such
agents introduces the challenge of cognitive biases inherited from human
design, which can distort reasoning, negotiation, tool use, and actuation.
Between neuroscience and AI, this paper provides a tutorial on a selection of
well-known biases, including their taxonomy, definition, mathematical
formulation, emergence in telecom systems and the commonly impacted agentic
components. The tutorial also presents various mitigation strategies tailored
to each type of bias. The article finally provides two practical use-cases,
which tackle the emergence, impact and mitigation gain of some famous biases in
6G inter-slice and cross-domain management. In particular, anchor
randomization, temporal decay and inflection bonus techniques are introduced to
specifically address anchoring, temporal and confirmation biases. This avoids
that agents stick to the initial high resource allocation proposal or decisions
that are recent and/or confirming a prior hypothesis. By grounding decisions in
a richer and fairer set of past experiences, the quality and bravery of the
agentic agreements in the second use-case, for instance, are leading to $\times
5$ lower latency and around $40\%$ higher energy saving.

</details>
