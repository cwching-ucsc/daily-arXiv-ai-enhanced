<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Accelerating HDC-CNN Hybrid Models Using Custom Instructions on RISC-V GPUs](https://arxiv.org/abs/2511.05053)
*Wakuto Matsumi,Riaz-Ul-Haque Mian*

Main category: cs.DC

TL;DR: Paper proposes custom RISC-V GPU instructions for Hyperdimensional Computing (HDC) to accelerate hybrid HDC–CNN workloads, reporting up to 56.2× microbenchmark speedups.


<details>
  <summary>Details</summary>
Motivation: Neural networks are energy-hungry; HDC is a low-energy, highly parallel alternative but has lower accuracy on complex visual tasks. Hybrid HDC–CNN accelerators exist but face programmability and generalizability issues. RISC-V-based GPUs offer a flexible platform to implement domain-specific instructions for HDC.

Method: Design and implement four custom GPU instructions tailored to HDC primitives and integrate them into a RISC-V GPU pipeline. Use these instructions to accelerate hybrid HDC–CNN workloads and evaluate via microbenchmarks and end-to-end experiments.

Result: Microbenchmark experiments show performance improvements up to 56.2× with the custom HDC instructions. Results demonstrate feasibility and efficiency gains of implementing HDC primitives as GPU instructions on RISC-V-based platforms.

Conclusion: Custom HDC instructions on RISC-V GPUs can markedly accelerate HDC and hybrid HDC–CNN workloads, suggesting RISC-V GPUs are a promising, programmable, energy-efficient platform for domain-specific ML acceleration.

Abstract: Machine learning based on neural networks has advanced rapidly, but the high
energy consumption required for training and inference remains a major
challenge. Hyperdimensional Computing (HDC) offers a lightweight,
brain-inspired alternative that enables high parallelism but often suffers from
lower accuracy on complex visual tasks. To overcome this, hybrid accelerators
combining HDC and Convolutional Neural Networks (CNNs) have been proposed,
though their adoption is limited by poor generalizability and programmability.
The rise of open-source RISC-V architectures has created new opportunities for
domain-specific GPU design. Unlike traditional proprietary GPUs, emerging
RISC-V-based GPUs provide flexible, programmable platforms suitable for custom
computation models such as HDC. In this study, we design and implement custom
GPU instructions optimized for HDC operations, enabling efficient processing
for hybrid HDC-CNN workloads. Experimental results using four types of custom
HDC instructions show a performance improvement of up to 56.2 times in
microbenchmark tests, demonstrating the potential of RISC-V GPUs for
energy-efficient, high-performance computing.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [2] [EPFL-REMNet: Efficient Personalized Federated Digital Twin Towards 6G Heterogeneous Radio Environme](https://arxiv.org/abs/2511.05238)
*Peide Li,Liu Cao,Lyutianyang Zhang,Dongyu Wei,Ye Hu,Qipeng Xie*

Main category: cs.NI

TL;DR: EPFL-REMNet is a personalized federated learning framework for Radio Environment Map construction in 6G heterogeneous settings that transmits a compressed shared backbone and keeps lightweight personalized heads locally, achieving higher accuracy and lower uplink overhead than FedAvg and recent baselines under three Non‑IID scenarios across 90 clients.


<details>
  <summary>Details</summary>
Motivation: As REM moves from homogeneous 5G to heterogeneous B5G/6G environments, federated learning faces accuracy and communication-efficiency degradation under naturally Non‑IID client data; a personalized, communication‑efficient FL design is needed to build high‑fidelity digital twins.

Method: Use a 'shared backbone + lightweight personalized head' model: compress the shared backbone and transmit it between server and clients while keeping each client's personalized head local. Evaluate under three Non‑IID complexity levels (light/medium/heavy) with geographical partitioning across 90 clients, comparing to FedAvg and state‑of‑the‑art methods.

Result: EPFL-REMNet improves digital twin fidelity (accuracy) and reduces uplink communication overhead across all Non‑IID settings. It reduces performance disparity among clients and particularly improves accuracy for long‑tail clients.

Conclusion: Personalized model partitioning with compressed backbone updates is an effective strategy for REM construction in heterogeneous 6G scenarios, yielding communication savings and better per‑client performance than standard federated approaches, though further details and evaluations would strengthen the claim.

Abstract: Radio Environment Map (REM) is transitioning from 5G homogeneous environments
to B5G/6G heterogeneous landscapes. However, standard Federated Learning (FL),
a natural fit for this distributed task, struggles with performance degradation
in accuracy and communication efficiency under the non-independent and
identically distributed (Non-IID) data conditions inherent to these new
environments. This paper proposes EPFL-REMNet, an efficient personalized
federated framework for constructing a high-fidelity digital twin of the 6G
heterogeneous radio environment. The proposed EPFL-REMNet employs a"shared
backbone + lightweight personalized head" model, where only the compressed
shared backbone is transmitted between the server and clients, while each
client's personalized head is maintained locally. We tested EPFL-REMNet by
constructing three distinct Non-IID scenarios (light, medium, and heavy) based
on radio environment complexity, with data geographically partitioned across 90
clients. Experimental results demonstrate that EPFL-REMNet simultaneously
achieves higher digital twin fidelity (accuracy) and lower uplink overhead
across all Non-IID settings compared to standard FedAvg and recent
state-of-the-art methods. Particularly, it significantly reduces performance
disparities across datasets and improves local map accuracy for long-tail
clients, enhancing the overall integrity of digital twin.

</details>


### [3] [To Squelch or not to Squelch: Enabling Improved Message Dissemination on the XRP Ledger](https://arxiv.org/abs/2511.05362)
*Lucian Trestioreanu,Flaviene Scheidt,Wazen Shbair,Jerome Francois,Damien Magoni,Radu State*

Main category: cs.NI

TL;DR: This paper evaluates squelching — a duplication-reduction flooding optimization — for the XRP Ledger (an FBA-based blockchain) using a controllable testbed and the XRPL production network, and compares it to Named Data Networking and gossip-based dissemination alternatives.


<details>
  <summary>Details</summary>
Motivation: While consensus-validation blockchains like XRPL scale well in transaction throughput, their peer-to-peer flooding for consensus messages produces many duplicates that hinder network scaling. Prior work focused on PoW chains; squelching has been proposed but not quantitatively evaluated on the XRPL production network.

Method: Deploy a real-life controllable testbed interacting with the XRPL production network to measure the impact of squelching. Compare squelching to two alternative approaches — Named Data Networking (NDN)-based dissemination and a gossip-based method — using metrics that capture duplication, message latency, bandwidth use, and resilience.

Result: Quantitative measurements showing how squelching reduces duplicate messages and bandwidth use compared to baseline flooding. Comparative results indicate trade-offs: squelching lowers duplication with modest latency effects, while NDN and gossip approaches offer different bandwidth/latency/resilience profiles.

Conclusion: Squelching is a practical and effective mitigation for duplicate flooding in XRPL-like FBA networks and can improve scalability; however, alternative designs (NDN, gossip) present viable trade-offs and merit further exploration for larger-scale deployments.

Abstract: With the large increase in the adoption of blockchain technologies, their
underlying peer-to-peer networks must also scale with the demand. In this
context, previous works highlighted the importance of ensuring efficient and
resilient communication for the underlying consensus and replication
mechanisms. However, they were mainly focused on mainstream,
Proof-of-Work-based Distributed Ledger Technologies like Bitcoin or Ethereum.
  In this paper, the problem is investigated in the context of
consensus-validation based blockchains, like the XRP Ledger. The latter relies
on a Federated Byzantine Agreement (FBA) consensus mechanism which is proven to
have a good scalability in regards to transaction throughput. However, it is
known that significant increases in the size of the XRP Ledger network would be
challenging to achieve. The main reason is the flooding mechanism used to
disseminate the messages related to the consensus protocol, which creates many
duplicates in the network. Squelching is a recent solution proposed for
limiting this duplication, however, it was never evaluated quantitatively in
real-life scenarios involving the XRPL production network. In this paper, our
aim is to assess this mechanism using a real-life controllable testbed and the
XRPL production network, to assess its benefit and compare it to alternative
solutions relying on Named Data Networking and on a gossip-based approach.

</details>
