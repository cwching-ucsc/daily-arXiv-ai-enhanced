{"id": "2510.16144", "categories": ["cs.NI", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.16144", "abs": "https://arxiv.org/abs/2510.16144", "authors": ["Sukhdeep Singh", "Avinash Bhat", "Shweta M", "Subhash K Singh", "Moonki Hong", "Madhan Raj K", "Kandeepan Sithamparanathan", "Sunder A. Khowaja", "Kapal Dev"], "title": "Agentic AI for Ultra-Modern Networks: Multi-Agent Framework for RAN Autonomy and Assurance", "comment": null, "summary": "The increasing complexity of Beyond 5G and 6G networks necessitates new\nparadigms for autonomy and assur- ance. Traditional O-RAN control loops rely\nheavily on RIC- based orchestration, which centralizes intelligence and exposes\nthe system to risks such as policy conflicts, data drift, and unsafe actions\nunder unforeseen conditions. In this work, we argue that the future of\nautonomous networks lies in a multi-agentic architecture, where specialized\nagents collaborate to perform data collection, model training, prediction,\npolicy generation, verification, deployment, and assurance. By replacing\ntightly- coupled centralized RIC-based workflows with distributed agents, the\nframework achieves autonomy, resilience, explainability, and system-wide\nsafety. To substantiate this vision, we design and evaluate a traffic steering\nuse case under surge and drift conditions. Results across four KPIs: RRC\nconnected users, IP throughput, PRB utilization, and SINR, demonstrate that a\nnaive predictor-driven deployment improves local KPIs but destabilizes\nneighbors, whereas the agentic system blocks unsafe policies, preserving global\nnetwork health. This study highlights multi- agent architectures as a credible\nfoundation for trustworthy AI- driven autonomy in next-generation RANs.", "AI": {"tldr": "Proposes a multi-agent architecture for O-RAN autonomy that distributes data, learning, policy, verification, and deployment across cooperating agents. In a traffic-steering case with surge/drift, the agentic system prevents harmful policies and preserves global KPIs, unlike a naive predictor that boosts local metrics but destabilizes neighbors.", "motivation": "Centralized RIC-centric control loops concentrate intelligence and create risks: policy conflicts, model/data drift, and unsafe actions under unforeseen conditions. There is a need for autonomy with resilience, explainability, and system-wide safety in B5G/6G RANs.", "method": "Design a distributed, specialized multi-agent framework covering data collection, model training, prediction, policy generation, verification, deployment, and assurance. Implement a traffic-steering use case subjected to demand surges and data/model drift. Compare naive predictor-driven deployment versus the proposed agentic, safety-verifying system across multiple KPIs (RRC users, IP throughput, PRB utilization, SINR).", "result": "The naive predictor improves local KPIs but induces negative externalities, destabilizing neighboring cells. The agent-based system detects and blocks unsafe policies, preserving overall network health across the four KPIs, even under surge and drift.", "conclusion": "Multi-agent architectures can provide trustworthy, safe, and resilient AI-driven autonomy for next-generation RANs, outperforming tightly coupled, centralized RIC-based approaches by preventing unsafe actions and maintaining system-wide performance."}}
{"id": "2510.17147", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.17147", "abs": "https://arxiv.org/abs/2510.17147", "authors": ["Linhan Xia", "Mingzhan Yang", "Jingjing Wang", "Ziwei Yan", "Yakun Ren", "Guo Yu", "Kai Lei"], "title": "Mamba4Net: Distilled Hybrid Mamba Large Language Models For Networking", "comment": null, "summary": "Transformer-based large language models (LLMs) are increasingly being adopted\nin networking research to address domain-specific challenges. However, their\nquadratic time complexity and substantial model sizes often result in\nsignificant computational overhead and memory constraints, particularly in\nresource-constrained environments. Drawing inspiration from the efficiency and\nperformance of the Deepseek-R1 model within the knowledge distillation\nparadigm, this paper introduces Mamba4Net, a novel cross-architecture\ndistillation framework. Mamba4Net transfers networking-specific knowledge from\ntransformer-based LLMs to student models built on the Mamba architecture, which\nfeatures linear time complexity. This design substantially enhances\ncomputational efficiency compared to the quadratic complexity of\ntransformer-based models, while the reduced model size further minimizes\ncomputational demands, improving overall performance and resource utilization.\nTo evaluate its effectiveness, Mamba4Net was tested across three diverse\nnetworking tasks: viewport prediction, adaptive bitrate streaming, and cluster\njob scheduling. Compared to existing methods that do not leverage LLMs,\nMamba4Net demonstrates superior task performance. Furthermore, relative to\ndirect applications of transformer-based LLMs, it achieves significant\nefficiency gains, including a throughput 3.96 times higher and a storage\nfootprint of only 5.48% of that required by previous LLM-based approaches.\nThese results highlight Mamba4Net's potential to enable the cost-effective\napplication of LLM-derived knowledge in networking contexts. The source code is\nopenly available to support further research and development.", "AI": {"tldr": "Mamba4Net distills networking-specific knowledge from transformer LLMs into a linear-time Mamba student, delivering superior task performance to non-LLM baselines while achieving 3.96\u00d7 higher throughput and a 5.48% storage footprint versus prior LLM-based approaches.", "motivation": "Transformer LLMs help with networking problems but suffer from quadratic time and large memory footprints, limiting deployment in resource-constrained environments. The goal is to retain LLM-derived benefits while drastically reducing compute and storage costs.", "method": "A cross-architecture knowledge distillation framework: teacher models are transformer-based LLMs; student models use the Mamba architecture (state-space models) with linear time complexity. The framework transfers networking-domain knowledge and is evaluated on viewport prediction, adaptive bitrate streaming, and cluster job scheduling.", "result": "Across three tasks, Mamba4Net outperforms methods that do not use LLMs and, compared to directly applying transformer LLMs, yields major efficiency gains\u20143.96\u00d7 higher throughput and only 5.48% of the storage requirement\u2014while improving resource utilization.", "conclusion": "Mamba4Net enables cost-effective, efficient application of LLM-derived knowledge in networking by leveraging linear-time Mamba students distilled from transformer teachers; open-source code supports reproducibility and further research."}}
{"id": "2510.16415", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.16415", "abs": "https://arxiv.org/abs/2510.16415", "authors": ["Rizhen Hu", "Yutong He", "Ran Yan", "Mou Sun", "Binghang Yuan", "Kun Yuan"], "title": "MeCeFO: Enhancing LLM Training Robustness via Fault-Tolerant Optimization", "comment": "NeurIPS 2025 poster", "summary": "As distributed optimization scales to meet the demands of Large Language\nModel (LLM) training, hardware failures become increasingly non-negligible.\nExisting fault-tolerant training methods often introduce significant\ncomputational or memory overhead, demanding additional resources. To address\nthis challenge, we propose Memory- and Computation-efficient Fault-tolerant\nOptimization (MeCeFO), a novel algorithm that ensures robust training with\nminimal overhead. When a computing node fails, MeCeFO seamlessly transfers its\ntraining task to a neighboring node while employing memory- and\ncomputation-efficient algorithmic optimizations to minimize the extra workload\nimposed on the neighboring node handling both tasks. MeCeFO leverages three key\nalgorithmic designs: (i) Skip-connection, which drops the multi-head attention\n(MHA) module during backpropagation for memory- and computation-efficient\napproximation; (ii) Recomputation, which reduces activation memory in\nfeedforward networks (FFNs); and (iii) Low-rank gradient approximation,\nenabling efficient estimation of FFN weight matrix gradients. Theoretically,\nMeCeFO matches the convergence rate of conventional distributed training, with\na rate of $\\mathcal{O}(1/\\sqrt{nT})$, where n is the data parallelism size and\nT is the number of iterations. Empirically, MeCeFO maintains robust performance\nunder high failure rates, incurring only a 4.18% drop in throughput,\ndemonstrating 5.0$\\times$ to 6.7$\\times$ greater resilience than previous SOTA\napproaches. Codes are available at https://github.com/pkumelon/MeCeFO.", "AI": {"tldr": "MeCeFO is a fault-tolerant distributed training algorithm for LLMs that hands a failed node\u2019s work to a neighbor and uses three lightweight approximations (skip-connection for MHA in backprop, FFN activation recomputation, and low-rank gradient approximation) to keep the extra compute/memory small, while matching standard convergence and improving resilience with minimal throughput loss.", "motivation": "Large-scale LLM training increasingly encounters hardware failures, but existing fault-tolerant methods impose substantial compute/memory overhead or require extra resources. There is a need for a method that preserves convergence and model quality while minimizing added cost during failure recovery.", "method": "Upon node failure, the failed node\u2019s training shard is reassigned to a neighboring node. To control the doubled workload, MeCeFO applies: (i) a skip-connection scheme that drops the MHA module during backpropagation to reduce memory/compute; (ii) activation recomputation in feedforward networks to lower memory; and (iii) low-rank approximation of FFN gradients to cut compute/memory for gradient estimation. The algorithm is analyzed to retain the conventional O(1/sqrt(nT)) convergence rate (n: data-parallel size; T: iterations).", "result": "Theory: convergence rate matches standard distributed training. Empirics: robust under high failure rates with only a 4.18% throughput drop and 5.0\u00d7\u20136.7\u00d7 higher resilience than prior SOTA baselines. Code is publicly available.", "conclusion": "MeCeFO offers practical, compute- and memory-efficient fault tolerance for distributed LLM training. It maintains standard convergence guarantees and delivers strong robustness with minimal performance degradation, making it a promising approach for large-scale deployments where failures are common."}}
{"id": "2510.17410", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.17410", "abs": "https://arxiv.org/abs/2510.17410", "authors": ["Dmitry Bankov", "Artem Krasilov", "Artem Otmakhov", "Pavel Savlukovich", "Evgeny Khorov"], "title": "Is It Worth to Use Feedback Channel in 5G V2X Platoon Scenarios?", "comment": null, "summary": "5G Vehicle-to-Everything (V2X) is a new technology developed by 3GPP to\nsupport inter-vehicle communication. In contrast to 4G V2X which allows only\nbroadcast communication, 5G V2X enables groupcast and unicast communication.\nSuch types of communication are needed for new V2X scenarios: platooning,\nextended sensors, remote driving, etc. To improve the data transmission\nreliability and assist in the selection of the transmission parameters in these\nscenarios, 5G V2X introduces a feedback channel that allows receivers to send\nacknowledgments in response to data packets. However, some part of the overall\nresource shall be allocated for the feedback channel, which reduces the amount\nof channel resources available for data transmission. In this paper, we\nconsider a scenario with a platoon, which generates groupcast traffic, and\nsurrounding vehicles, which generate legacy broadcast traffic. Using extensive\nsimulations in NS-3, we analyze how the usage of the feedback channel\ninfluences the overall system capacity. Our results show that depending on the\nplatoon size, groupcast, and broadcast traffic intensities, and their quality\nof service requirements, the usage of the feedback channel can in some cases\nsignificantly increase the system capacity (up to 2x), while in other cases it\nalmost halves the system capacity. We explain the reasons for such effects and\ndiscuss how to adaptively select the feedback channel parameters.", "AI": {"tldr": "In 5G NR V2X, enabling a feedback (ACK/NACK) channel for groupcast/unicast can either double or nearly halve overall capacity in mixed platoon (groupcast) and surrounding broadcast traffic, depending on traffic mix, QoS targets, and platoon size; careful, adaptive feedback configuration is essential.", "motivation": "New V2X use cases (platooning, extended sensors, remote driving) require reliable groupcast/unicast beyond 4G\u2019s broadcast. 5G adds a feedback channel to improve reliability and guide transmission parameter selection, but it consumes radio resources. The paper seeks to quantify this trade-off and when feedback helps or hurts system capacity.", "method": "Extensive NS-3 simulations of a mixed scenario: a platoon producing groupcast traffic and nearby vehicles producing legacy broadcast traffic. The study varies platoon size, traffic intensities, and QoS requirements, comparing system capacity with and without the feedback channel and analyzing the impact of feedback resource allocation.", "result": "Depending on scenario parameters, feedback can substantially increase capacity (up to 2\u00d7) or almost halve it. The outcomes hinge on the balance between reliability gains (fewer retransmissions/better parameter tuning) and resource overhead introduced by feedback, plus interactions between groupcast and broadcast loads and QoS constraints.", "conclusion": "A feedback channel is not universally beneficial in NR V2X. Its parameters should be adaptively configured based on platoon size, traffic intensities, and QoS targets to maximize capacity; static allocation risks significant underperformance."}}
{"id": "2510.16418", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.16418", "abs": "https://arxiv.org/abs/2510.16418", "authors": ["Jian Ma", "Xinchen Lyu", "Jun Jiang", "Longhao Zou", "Chenshan Ren", "Qimei Cui", "Xiaofeng Tao"], "title": "FourierCompress: Layer-Aware Spectral Activation Compression for Efficient and Accurate Collaborative LLM Inference", "comment": null, "summary": "Collaborative large language model (LLM) inference enables real-time,\nprivacy-preserving AI services on resource-constrained edge devices by\npartitioning computational workloads between client devices and edge servers.\nHowever, this paradigm is severely hindered by communication bottlenecks caused\nby the transmission of high-dimensional intermediate activations, exacerbated\nby the autoregressive decoding structure of LLMs, where bandwidth consumption\nscales linearly with output length. Existing activation compression methods\nstruggle to simultaneously achieve high compression ratios, low reconstruction\nerror, and computational efficiency. This paper proposes FourierCompress, a\nnovel, layer-aware activation compression framework that exploits the\nfrequency-domain sparsity of LLM activations. We rigorously demonstrate that\nactivations from the first Transformer layer exhibit strong smoothness and\nenergy concentration in the low-frequency domain, making them highly amenable\nto near-lossless compression via the Fast Fourier Transform (FFT).\nFourierCompress transforms activations into the frequency domain, retains only\na compact block of low-frequency coefficients, and reconstructs the signal at\nthe server using conjugate symmetry, enabling seamless hardware acceleration on\nDSPs and FPGAs. Extensive experiments on Llama 3 and Qwen2.5 models across 10\ncommonsense reasoning datasets demonstrate that FourierCompress preserves\nperformance remarkably close to the uncompressed baseline, outperforming Top-k,\nQR, and SVD. FourierCompress bridges the gap between communication efficiency\n(an average 7.6x reduction in activation size), near-lossless inference (less\nthan 0.3% average accuracy loss), and significantly faster compression\n(achieving over 32x reduction in compression time compared to Top-k via\nhardware acceleration) for edge-device LLM inference.", "AI": {"tldr": "FourierCompress compresses first-layer LLM activations in the frequency domain, keeping only low-frequency FFT coefficients to cut bandwidth ~7.6x with <0.3% accuracy loss and >32x faster compression (with HW accel), enabling efficient edge\u2013server collaborative inference.", "motivation": "Collaborative LLM inference on edge devices is bottlenecked by repeatedly transmitting high-dimensional activations during autoregressive decoding, where bandwidth scales with output length. Existing activation compression methods cannot jointly deliver high compression, low error, and low compute cost.", "method": "Leverage frequency-domain sparsity of early Transformer activations: prove/verify first-layer activations are smooth with energy concentrated in low frequencies; apply FFT, retain a compact low-frequency block, reconstruct via conjugate symmetry. Use a layer-aware design and exploit DSP/FPGA acceleration. Compare against Top-k, QR, and SVD baselines.", "result": "On Llama 3 and Qwen2.5 across 10 commonsense datasets: average 7.6x activation-size reduction with <0.3% average accuracy loss; outperforms Top-k/QR/SVD; compression time >32x faster than Top-k when using hardware acceleration.", "conclusion": "A hardware-friendly, near-lossless activation compression approach that bridges communication efficiency, model accuracy, and compute speed for edge-device LLM inference, making collaborative deployment more practical."}}
{"id": "2510.16497", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16497", "abs": "https://arxiv.org/abs/2510.16497", "authors": ["Pacome Simon Mbonimpa", "Diane Tuyizere", "Azizuddin Ahmed Biyabani", "Ozan K. Tonguz"], "title": "Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages", "comment": null, "summary": "This paper presents a novel framework for speech transcription and synthesis,\nleveraging edge-cloud parallelism to enhance processing speed and accessibility\nfor Kinyarwanda and Swahili speakers. It addresses the scarcity of powerful\nlanguage processing tools for these widely spoken languages in East African\ncountries with limited technological infrastructure. The framework utilizes the\nWhisper and SpeechT5 pre-trained models to enable speech-to-text (STT) and\ntext-to-speech (TTS) translation. The architecture uses a cascading mechanism\nthat distributes the model inference workload between the edge device and the\ncloud, thereby reducing latency and resource usage, benefiting both ends. On\nthe edge device, our approach achieves a memory usage compression of 9.5% for\nthe SpeechT5 model and 14% for the Whisper model, with a maximum memory usage\nof 149 MB. Experimental results indicate that on a 1.7 GHz CPU edge device with\na 1 MB/s network bandwidth, the system can process a 270-character text in less\nthan a minute for both speech-to-text and text-to-speech transcription. Using\nreal-world survey data from Kenya, it is shown that the cascaded edge-cloud\narchitecture proposed could easily serve as an excellent platform for STT and\nTTS transcription with good accuracy and response time.", "AI": {"tldr": "Proposes a cascaded edge\u2013cloud framework using Whisper (STT) and SpeechT5 (TTS) to deliver low-latency, low-resource speech transcription and synthesis for Kinyarwanda and Swahili, achieving reduced memory usage on edge devices and sub-minute processing for a short utterance under constrained CPU and bandwidth.", "motivation": "Many East African users lack access to powerful speech technologies due to limited infrastructure and under-resourced language support; enabling practical STT/TTS for Kinyarwanda and Swahili on constrained devices would improve accessibility.", "method": "Use pre-trained Whisper and SpeechT5 with a cascading edge\u2013cloud architecture that splits inference between edge and cloud to reduce latency and resource usage. Evaluate memory footprint, latency under a 1.7 GHz CPU and 1 MB/s bandwidth, and validate on real survey data.", "result": "Memory usage compressed by 9.5% for SpeechT5 and 14% for Whisper with a maximum of 149 MB on the edge; processing a 270-character text in under one minute for both STT and TTS on the specified hardware/network; demonstrates good accuracy and responsiveness on Kenyan survey data.", "conclusion": "The edge\u2013cloud cascade is a practical, accessible platform for STT/TTS in under-resourced languages, offering favorable memory and latency characteristics suitable for deployment."}}
{"id": "2510.16890", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.16890", "abs": "https://arxiv.org/abs/2510.16890", "authors": ["Ji\u0159\u00ed Klepl", "Martin Kruli\u0161", "Maty\u00e1\u0161 Brabec"], "title": "Layout-Agnostic MPI Abstraction for Distributed Computing in Modern C++", "comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Recent Advances in the Message Passing Interface (EuroMPI 2025),\n  and is available online at https://doi.org/10.1007/978-3-032-07194-1_3", "summary": "Message Passing Interface (MPI) has been a well-established technology in the\ndomain of distributed high-performance computing for several decades. However,\none of its greatest drawbacks is a rather ancient pure-C interface. It lacks\nmany useful features of modern languages (namely C++), like basic type-checking\nor support for generic code design. In this paper, we propose a novel\nabstraction for MPI, which we implemented as an extension of the C++ Noarr\nlibrary. It follows Noarr paradigms (first-class layout and traversal\nabstraction) and offers layout-agnostic design of MPI applications. We also\nimplemented a layout-agnostic distributed GEMM kernel as a case study to\ndemonstrate the usability and syntax of the proposed abstraction. We show that\nthe abstraction achieves performance comparable to the state-of-the-art MPI C++\nbindings while allowing for a more flexible design of distributed applications.", "AI": {"tldr": "They introduce a C++ abstraction layer for MPI, implemented as an extension to the Noarr library, enabling layout-agnostic, type-safe, generic MPI programming with performance comparable to state-of-the-art C++ MPI bindings. A distributed GEMM serves as a case study.", "motivation": "MPI\u2019s legacy C interface lacks modern language features such as strong type-checking and generic programming, making it awkward to build flexible, layout-agnostic distributed applications. The authors aim to bring modern C++ abstractions to MPI while preserving performance.", "method": "Extend the C++ Noarr library with MPI capabilities that follow Noarr\u2019s first-class layout and traversal abstractions. Provide an API that decouples data layout from communication and computation. Validate by implementing a layout-agnostic distributed GEMM kernel and comparing against contemporary C++ MPI bindings.", "result": "The proposed abstraction shows performance on par with state-of-the-art C++ MPI bindings in the GEMM case study while demonstrating cleaner, more flexible design and syntax.", "conclusion": "A Noarr-based, layout-agnostic MPI abstraction in C++ can retain high performance while improving type safety, genericity, and design flexibility for distributed applications."}}
{"id": "2510.16933", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16933", "abs": "https://arxiv.org/abs/2510.16933", "authors": ["Maty\u00e1\u0161 Brabec", "Ji\u0159\u00ed Klepl", "Michal T\u00f6pfer", "Martin Kruli\u0161"], "title": "Tutoring LLM into a Better CUDA Optimizer", "comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Euro-Par 2025: Parallel Processing, Part II, and is available\n  online at https://doi.org/10.1007/978-3-031-99857-7_18", "summary": "Recent leaps in large language models (LLMs) caused a revolution in\nprogramming tools (like GitHub Copilot) that can help with code generation,\ndebugging, and even performance optimization. In this paper, we focus on the\ncapabilities of the most recent reasoning models to generate optimized CUDA\ncode for predefined, well-known tasks. Our objective is to determine which\ntypes of code optimizations and parallel patterns the LLMs can perform by\nthemselves and whether they can be improved by tutoring (providing more\ndetailed hints and guidelines in the prompt). The generated solutions were\nevaluated both automatically (for correctness and speedup) and manually (code\nreviews) to provide a more detailed perspective. We also tried an interactive\napproach where the LLM can fix its previous mistakes within a session. The\nresults indicate that LLMs are quite skilled coders; however, they require\ntutoring to reach optimized solutions provided by parallel computing experts.", "AI": {"tldr": "The paper evaluates whether recent reasoning LLMs can autonomously generate and optimize CUDA kernels for standard tasks, finding they can produce correct, reasonably fast code but typically need targeted tutoring/prompts and interactive iteration to reach expert-level optimization.", "motivation": "To understand the extent to which modern LLMs can perform GPU-oriented performance engineering\u2014specifically, which optimization techniques and parallel patterns they can apply unaided\u2014and whether structured guidance (tutoring) meaningfully improves outcomes.", "method": "Generate CUDA implementations for predefined, well-known kernels using state-of-the-art reasoning LLMs. Compare base prompts vs tutoring prompts with detailed hints/guidelines. Evaluate automatically for correctness and speedups; conduct manual code reviews; and test interactive, within-session self-correction to fix prior mistakes.", "result": "LLMs reliably produce working CUDA code and some optimizations. However, without tutoring they often miss advanced parallel patterns and performance-critical details. Tutoring and interactive refinement substantially improve quality and speed, in some cases approaching or matching expert-optimized implementations.", "conclusion": "LLMs are competent CUDA coders but generally require structured guidance to achieve expert-level optimization. Prompt design and interactive iteration are key levers; fully autonomous expert-grade optimization remains inconsistent without tutoring."}}
