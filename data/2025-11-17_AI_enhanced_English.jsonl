{"id": "2511.10763", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.10763", "abs": "https://arxiv.org/abs/2511.10763", "authors": ["Abdul Saboor", "Evgenii Vinogradov"], "title": "Millimeter-Wave UAV Channel Model with Height-Dependent Path Loss and Shadowing in Urban Scenarios", "comment": "Submitted to the International Journal of Microwave and Wireless Technologies", "summary": "Uncrewed Aerial Vehicles (UAVs) serving as Aerial Base Stations (ABSs) are expected to extend 6G millimeter-Wave (mmWave) coverage and improve link reliability in urban areas. However, UAV-based Air-to-Ground (A2G) channels are highly dependent on height and urban geometry. This paper proposes an ABS height-dependent mmWave channel model and investigates whether urban geometry, beyond the standard built-up parameters, significantly affects LoS probability (PLoS) and Large-Scale Fading (LSF). Using MATLAB ray tracing at 26 GHz, we simulate approximately 10K city realizations for four urban layouts that share identical built-up parameters but differ in their spatial organization. We extract elevation-based PLoS using a sigmoid model and derive height-dependent Path-Loss Exponents (PLEs) and shadow-fading trends using exponential fits. Results show that PLE for Non-Line-of-Sight (NLoS) decreases toward 2.5-3 at high altitudes, Line-of-Sight (LoS) PLE remains near 2, and shadow fading reduces with height. We also find that geometric layout introduces a modest but consistent change in PLE (+/- 0.2), even when built-up parameters are fixed. The proposed unified model aligns well with ray-tracing statistics and offers a practical, height-dependent LSF model suitable for ABS planning in complex urban scenarios.", "AI": {"tldr": "The paper proposes a height-dependent mmWave A2G channel model for ABSs based on MATLAB ray-tracing at 26 GHz across \u224810k city realizations. It fits a sigmoid to elevation-based LoS probability and exponential laws to derive height-varying PLEs and shadow-fading; finds LoS PLE \u22482, NLoS PLE decreases to \u22482.5\u20133 at high altitudes, shadow fading reduces with height, and spatial layout causes a modest \u00b10.2 shift in PLE even with fixed built-up parameters.", "motivation": "ABS mmWave links depend strongly on UAV height and fine-grained urban geometry. Existing models often use only built-up parameters; the paper asks whether spatial layout (arrangement) materially affects LoS probability and large-scale fading for ABS planning.", "method": "MATLAB ray-tracing at 26 GHz on four urban layouts (same built-up parameters, different spatial organization). Simulated \u224810k independent city realizations. Extracted elevation-dependent LoS probability and fitted a sigmoid; derived height-dependent PLEs and shadow-fading trends using exponential fits. Compared unified model against ray-tracing statistics.", "result": "LoS PLE stays near 2. NLoS PLE tends toward 2.5\u20133 at high UAV altitudes. Shadow-fading magnitude decreases with height. Spatial layout yields a consistent but modest change in PLE (\u00b10.2) despite fixed built-up parameters. The proposed unified height-dependent LSF model matches ray-tracing stats and is practical for ABS planning.", "conclusion": "Urban spatial organization (beyond standard built-up parameters) has a measurable, modest impact on A2G large-scale fading; the provided height-dependent LoS/PLE/shadow-fading model captures these effects and is suited for ABS deployment studies in complex urban environments."}}
{"id": "2511.10753", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.10753", "abs": "https://arxiv.org/abs/2511.10753", "authors": ["Jiamin Li", "Lei Qu", "Tao Zhang", "Grigory Chirkov", "Shuotao Xu", "Peng Cheng", "Lidong Zhou"], "title": "FengHuang: Next-Generation Memory Orchestration for AI Inferencing", "comment": null, "summary": "This document presents a vision for a novel AI infrastructure design that has been initially validated through inference simulations on state-of-the-art large language models. Advancements in deep learning and specialized hardware have driven the rapid growth of large language models (LLMs) and generative AI systems. However, traditional GPU-centric architectures face scalability challenges for inference workloads due to limitations in memory capacity, bandwidth, and interconnect scaling. To address these issues, the FengHuang Platform, a disaggregated AI infrastructure platform, is proposed to overcome memory and communication scaling limits for AI inference. FengHuang features a multi-tier shared-memory architecture combining high-speed local memory with centralized disaggregated remote memory, enhanced by active tensor paging and near-memory compute for tensor operations. Simulations demonstrate that FengHuang achieves up to 93% local memory capacity reduction, 50% GPU compute savings, and 16x to 70x faster inter-GPU communication compared to conventional GPU scaling. Across workloads such as GPT-3, Grok-1, and QWEN3-235B, FengHuang enables up to 50% GPU reductions while maintaining end-user performance, offering a scalable, flexible, and cost-effective solution for AI inference infrastructure. FengHuang provides an optimal balance as a rack-level AI infrastructure scale-up solution. Its open, heterogeneous design eliminates vendor lock-in and enhances supply chain flexibility, enabling significant infrastructure and power cost reductions.", "AI": {"tldr": "FengHuang is a disaggregated, multi\u2011tier shared\u2011memory AI inference infrastructure that uses active tensor paging and near\u2011memory compute to reduce local memory needs, GPU count, and inter\u2011GPU communication, validated in simulations showing large efficiency and performance gains across major LLM workloads.", "motivation": "GPU\u2011centric architectures for LLM inference are hitting scalability limits in memory capacity, bandwidth, and interconnect scaling; a new infrastructure is needed to serve large models cost\u2011effectively at rack scale while avoiding vendor lock\u2011in.", "method": "Design a disaggregated platform (FengHuang) combining high\u2011speed local memory with centralized remote memory, active tensor paging to move tensors between tiers, and near\u2011memory compute to perform tensor operations closer to memory. Use inference simulations on state\u2011of\u2011the\u2011art LLMs (e.g., GPT\u20113, Grok\u20111, QWEN3\u2011235B) to quantify benefits.", "result": "Simulations report up to 93% reduction in required local memory capacity, ~50% GPU compute savings, 16\u00d7\u201370\u00d7 faster inter\u2011GPU communication versus conventional GPU scaling, and up to 50% fewer GPUs needed while maintaining end\u2011user performance across tested workloads.", "conclusion": "FengHuang is presented as a rack\u2011level scale\u2011up solution that balances scalability, flexibility and cost, removes vendor lock\u2011in through an open heterogeneous design, and can significantly lower infrastructure and power costs for AI inference."}}
{"id": "2511.10860", "categories": ["cs.DC", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.10860", "abs": "https://arxiv.org/abs/2511.10860", "authors": ["Rabimba Karanjai", "Lei Xu", "Weidong Shi"], "title": "HPCAgentTester: A Multi-Agent LLM Approach for Enhanced HPC Unit Test Generation", "comment": "Accepted in AIWare 2025", "summary": "Unit testing in High-Performance Computing (HPC) is critical but challenged by parallelism, complex algorithms, and diverse hardware. Traditional methods often fail to address non-deterministic behavior and synchronization issues in HPC applications. This paper introduces HPCAgentTester, a novel multi-agent Large Language Model (LLM) framework designed to automate and enhance unit test generation for HPC software utilizing OpenMP and MPI. HPCAgentTester employs a unique collaborative workflow where specialized LLM agents (Recipe Agent and Test Agent) iteratively generate and refine test cases through a critique loop. This architecture enables the generation of context-aware unit tests that specifically target parallel execution constructs, complex communication patterns, and hierarchical parallelism. We demonstrate HPCAgentTester's ability to produce compilable and functionally correct tests for OpenMP and MPI primitives, effectively identifying subtle bugs that are often missed by conventional techniques. Our evaluation shows that HPCAgentTester significantly improves test compilation rates and correctness compared to standalone LLMs, offering a more robust and scalable solution for ensuring the reliability of parallel software systems.", "AI": {"tldr": "HPCAgentTester is a multi-agent LLM framework that automates unit-test generation for OpenMP/MPI HPC code by having specialized agents iteratively produce and critique tests, yielding higher compilation and correctness rates and finding subtle parallel bugs compared with standalone LLMs.", "motivation": "Unit testing in HPC is hard because parallelism (race conditions, synchronization), complex communication patterns, and heterogeneous hardware lead to non-deterministic behavior that traditional test-generation methods struggle to cover; automated, context-aware test generation could increase reliability and developer productivity.", "method": "Introduce HPCAgentTester: a multi-agent architecture where a Recipe Agent generates test plans targeting parallel constructs and communication patterns and a Test Agent implements and refines concrete test cases. Agents iterate in a critique loop, incorporating compilation feedback and runtime results to produce context-aware tests for OpenMP and MPI primitives.", "result": "HPCAgentTester produces compilable, functionally correct tests that target parallel execution and inter-process communication; it finds subtle bugs missed by conventional techniques and shows improved compilation rates and correctness versus standalone LLMs in the authors' evaluation.", "conclusion": "The collaborative multi-agent LLM approach yields more robust, scalable automated unit tests for parallel HPC software, improving test quality and bug detection over single-model baselines."}}
{"id": "2511.11332", "categories": ["cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.11332", "abs": "https://arxiv.org/abs/2511.11332", "authors": ["Chaoyun Zhang", "Liqun Li", "He Huang", "Chiming Ni", "Bo Qiao", "Si Qin", "Yu Kang", "Minghua Ma", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "title": "UFO$^3$: Weaving the Digital Agent Galaxy", "comment": "We developed UFO$^3$ as a fully engineered system with over 73K lines of code, encompassing agent implementations and integrations for Windows, Linux, and Android mobile devices. The entire project is open-sourced at https://github.com/microsoft/UFO/, accompanied by detailed documentation and tutorials at https://microsoft.github.io/UFO/", "summary": "Large language model (LLM)-powered agents are transforming digital devices from passive tools into proactive intelligent collaborators. However, most existing frameworks remain confined to a single OS or device, making cross-device workflows brittle and largely manual. We present UFO$^3$, a system that unifies heterogeneous endpoints, desktops, servers, mobile devices, and edge, into a single orchestration fabric. UFO$^3$ models each user request as a mutable TaskConstellation: a distributed DAG of atomic subtasks (TaskStars) with explicit control and data dependencies (TaskStarLines). The TaskConstellation continuously evolves as results stream in from distributed devices, enabling asynchronous execution, adaptive recovery, and dynamic optimization. A Constellation Orchestrator} executes tasks safely and asynchronously while applying dynamic DAG updates, and the Agent Interaction Protocol (AIP) provides persistent, low-latency channels for reliable task dispatch and result streaming. These designs dissolve the traditional boundaries between devices and platforms, allowing agents to collaborate seamlessly and amplify their collective intelligence.\n  We evaluate UFO$^3$ on NebulaBench, a benchmark of 55 cross-device tasks across 5 machines and 10 categories. UFO$^3$ achieves 83.3% subtask completion, 70.9% task success, exposes parallelism with an average width of 1.72, and reduces end-to-end latency by 31% relative to a sequential baseline. Fault-injection experiments demonstrate graceful degradation and recovery under transient and permanent agent failures. These results show that UFO$^3$ achieves accurate, efficient, and resilient task orchestration across heterogeneous devices, uniting isolated agents into a coherent, adaptive computing fabric that extends across the landscape of ubiquitous computing.", "AI": {"tldr": "UFO^3 is a system for cross-device LLM agent orchestration that models user requests as evolving distributed DAGs (TaskConstellations) of atomic subtasks, executed by a Constellation Orchestrator over a persistent Agent Interaction Protocol. It shows improved completion, parallelism, latency, and fault recovery on a 5-machine benchmark.", "motivation": "Existing LLM-agent frameworks are usually confined to single OS/device and fail to support robust cross-device workflows. The authors aim to dissolve device/platform boundaries so agents across heterogeneous endpoints can collaborate asynchronously and adaptively.", "method": "Introduce TaskConstellation (mutable distributed DAG), TaskStars (atomic subtasks), TaskStarLines (control/data edges). A Constellation Orchestrator executes tasks, applies dynamic DAG updates, and recovers adaptively. An Agent Interaction Protocol (AIP) offers persistent low-latency channels for dispatch and result streaming. System supports streaming results and asynchronous execution across desktops, servers, mobile, and edge devices.", "result": "On NebulaBench (55 cross-device tasks, 5 machines, 10 categories), UFO^3 achieved 83.3% subtask completion, 70.9% task success, average parallel width 1.72, and 31% latency reduction vs sequential baseline; fault-injection shows graceful degradation and recovery.", "conclusion": "UFO^3 enables accurate, efficient, and resilient orchestration of LLM-powered agents across heterogeneous devices by modeling requests as evolving distributed DAGs and using persistent channels and an orchestrator to execute and adapt tasks."}}
