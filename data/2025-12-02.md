<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 8]
- [cs.DC](#cs.DC) [Total: 6]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Multi-Server FL with Overlapping Clients: A Latency-Aware Relay Framework](https://arxiv.org/abs/2512.00025)
*Yun Ji,Zeyu Chen,Xiaoxiong Zhong,Yanan Ma,Sheng Zhang,Yuguang Fang*

Main category: cs.NI

TL;DR: Paper proposes a cloud-free multi-server federated learning framework that uses overlapping clients as relays to exchange models among edge servers, derives a convergence bound for non-convex non-IID settings that captures propagation depth, and optimizes routing/scheduling with a conflict-graph-based local search to maximize model dissemination under latency constraints.


<details>
  <summary>Details</summary>
Motivation: Single-server FL faces communication bottlenecks; multi-server FL with overlapping edge-server coverage allows clients in overlap zones to access multiple ES models. The paper leverages these overlapping clients to relay models across ESs without adding infrastructure, aiming to improve inter-server model dissemination and scalability under latency limits.

Method: Introduce a cloud-free multi-server FL architecture using Overlapping Clients (OCs) as relays for inter-server model exchange. Derive a convergence upper bound for non-convex objectives with non-IID data and arbitrary number of cells that quantifies the effect of inter-server propagation depth. Formulate an optimization problem to maximize dissemination range of each ES model under latency constraints. Propose a conflict-graph-based local search algorithm to determine routing and transmission scheduling among ESs.

Result: Theoretical result: a convergence upper bound explicitly depending on propagation depth, showing trade-offs between dissemination hops and convergence error. Algorithmic result: a practical routing/scheduling algorithm that increases per-model transmission coverage within latency limits. Empirical result: extensive experiments report significant performance gains over existing methods.

Conclusion: Leveraging overlapping clients as relays is a feasible and effective approach to enhance model dissemination in multi-server FL without adding links; the paper provides theoretical justification and an algorithmic solution, with experiments supporting improved performance.

Abstract: Multi-server Federated Learning (FL) has emerged as a promising solution to mitigate communication bottlenecks of single-server FL. In a typical multi-server FL architecture, the regions covered by different edge servers (ESs) may overlap. Under this architecture, clients located in the overlapping areas can access edge models from multiple ESs. Building on this observation, we propose a cloud-free multi-server FL framework that leverages Overlapping Clients (OCs) as relays for inter-server model exchange while uploading the local updated model to ESs. This enables ES models to be relayed across multiple hops through neighboring ESs by OCs without introducing new communication links. We derive a new convergence upper bound for non-convex objectives under non-IID data and an arbitrary number of cells, which explicitly quantifies the impact of inter-server propagation depth on convergence error. Guided by this theoretical result, we formulate an optimization problem that aims to maximize dissemination range of each ES model among all ESs within a limited latency. To solve this problem, we develop a conflict-graph-based local search algorithm optimizing the routing strategy and scheduling the transmission times of individual ESs to its neighboring ESs. This enables ES models to be relayed across multiple hops through neighboring ESs by OCs, achieving the widest possible transmission coverage for each model without introducing new communication links. Extensive experimental results show remarkable performance gains of our scheme compared to existing methods.

</details>


### [2] [LM4Opt-RA: A Multi-Candidate LLM Framework with Structured Ranking for Automating Network Resource Allocation](https://arxiv.org/abs/2512.00039)
*Tasnim Ahmed,Siana Rizwan,Naveed Ejaz,Salimur Choudhury*

Main category: cs.NI

TL;DR: Paper introduces NL4RA, a 50-problem LP/ILP/MILP dataset for resource allocation, evaluates open-source LLMs, proposes LM4Opt-RA multi-candidate prompting + ranking, and LAME automated mathematical-evaluation metric; Llama-3.1-70B scores best (LAME=0.8007) but still below human experts.


<details>
  <summary>Details</summary>
Motivation: LLMs are improving at complex reasoning but existing benchmarks do not capture dynamic, interdependent, heterogeneous resource-allocation optimization problems; there is a need for datasets and automated metrics to evaluate correctness of mathematical formulations and models' outputs.

Method: Create NL4RA (50 optimization problems) with ground-truth LP/ILP/MILP formulations. Evaluate multiple open-source LLMs with prompts (direct, few-shot, chain-of-thought). Propose LM4Opt-RA: generate multiple candidate solutions per prompt style and apply a structured ranking mechanism to select the best. Introduce LAME: an automated metric tailored to compare mathematical formulations and quantify differences to ground truth.

Result: LM4Opt-RA improves over baselines; Llama-3.1-70B achieved LAME=0.8007, Llama-3.1-8B close behind. Existing automated metrics (ROUGE/BLEU/BERT) diverge from human judgment; human evaluation is costly. Baselines promising but behind human performance.

Conclusion: NL4RA and LAME provide new resources for evaluating LLMs on optimization formulation tasks; LM4Opt-RA shows gains but LLMs still do not match human experts; further work needed on evaluation fidelity and model improvements.

Abstract: Building on advancements in Large Language Models (LLMs), we can tackle complex analytical and mathematical reasoning tasks requiring nuanced contextual understanding. A prime example of such complex tasks is modelling resource allocation optimization in networks, which extends beyond translating natural language inputs into mathematical equations or Linear Programming (LP), Integer Linear Programming (ILP), and Mixed-Integer Linear Programming (MILP) models. However, existing benchmarks and datasets cannot address the complexities of such problems with dynamic environments, interdependent variables, and heterogeneous constraints. To address this gap, we introduce NL4RA, a curated dataset comprising 50 resource allocation optimization problems formulated as LP, ILP, and MILP. We then evaluate the performance of well-known open-source LLMs with varying parameter counts. To enhance existing LLM based methods, we introduce LM4Opt RA, a multi candidate framework that applies diverse prompting strategies such as direct, few shot, and chain of thought, combined with a structured ranking mechanism to improve accuracy. We identified discrepancies between human judgments and automated scoring such as ROUGE, BLEU, or BERT scores. However, human evaluation is time-consuming and requires specialized expertise, making it impractical for a fully automated end-to-end framework. To quantify the difference between LLM-generated responses and ground truth, we introduce LLM-Assisted Mathematical Evaluation (LAME), an automated metric designed for mathematical formulations. Using LM4Opt-RA, Llama-3.1-70B achieved a LAME score of 0.8007, outperforming other models by a significant margin, followed closely by Llama-3.1-8B. While baseline LLMs demonstrate considerable promise, they still lag behind human expertise; our proposed method surpasses these baselines regarding LAME and other metrics.

</details>


### [3] [Constrained Network Slice Assignment via Large Language Models](https://arxiv.org/abs/2512.00040)
*Sagar Sudhakara,Pankaj Rajak*

Main category: cs.NI

TL;DR: LLMs can meaningfully assist 5G network slicing allocation: zero-shot prompting yields reasonable but imperfect slice assignments; using LLM-derived semantic similarities as inputs to an integer programming solver produces improved allocations competitive with numeric-data methods, reducing search space and aiding efficiency.


<details>
  <summary>Details</summary>
Motivation: Network slicing requires mapping heterogeneous service requests to constrained virtual resources; conventional optimization is complex and data-heavy. The paper investigates whether LLMs’ semantic understanding of textual service descriptions can simplify or guide allocation, especially when numerical details are minimal or unavailable.

Method: Two-pronged approach: (1) zero-shot LLM prompting to directly assign user requests to slices from textual descriptions; (2) building an integer programming model that uses LLM-estimated semantic similarity/grouping of requests to form input clusters, then solving the constrained allocation problem with an exact solver.

Result: Zero-shot LLM outputs provide plausible initial allocations but may violate capacity/latency constraints. Integrating LLM-derived groupings into an optimization solver yields allocations that match or approach performance of traditional, numeric-data-driven methods in metrics like resource utilization and slice isolation. The LLM significantly reduces search space and helps the solver converge to good solutions faster.

Conclusion: LLMs are valuable for semantic grouping and as preprocessing to exact solvers in network slicing tasks, particularly when numeric detail is sparse. They are not yet reliable standalone optimizers but serve as efficient guides to improve solver performance and practicality.

Abstract: Modern networks support network slicing, which partitions physical infrastructure into virtual slices tailored to different service requirements (for example, high bandwidth or low latency). Optimally allocating users to slices is a constrained optimization problem that traditionally requires complex algorithms. In this paper, we explore the use of Large Language Models (LLMs) to tackle radio resource allocation for network slicing. We focus on two approaches: (1) using an LLM in a zero-shot setting to directly assign user service requests to slices, and (2) formulating an integer programming model where the LLM provides semantic insight by estimating similarity between requests. Our experiments show that an LLM, even with zero-shot prompting, can produce a reasonable first draft of slice assignments, although it may violate some capacity or latency constraints. We then incorporate the LLM's understanding of service requirements into an optimization solver to generate an improved allocation. The results demonstrate that LLM-guided grouping of requests, based on minimal textual input, achieves performance comparable to traditional methods that use detailed numerical data, in terms of resource utilization and slice isolation. While the LLM alone does not perfectly satisfy all constraints, it significantly reduces the search space and, when combined with exact solvers, provides a promising approach for efficient 5G network slicing resource allocation.

</details>


### [4] [On the Prediction of Wi-Fi Performance through Deep Learning](https://arxiv.org/abs/2512.00211)
*Gabriele Formis,Amanda Ericson,Stefan Forsstrom,Kyi Thar,Gianluca Cena,Stefano Scanzio*

Main category: cs.NI

TL;DR: The paper predicts Frame Delivery Ratio (FDR) from binary success/failure time sequences using CNN and LSTM. Both models predict FDR well from a single sequence; CNN offers much lower inference latency with small accuracy loss versus LSTM.


<details>
  <summary>Details</summary>
Motivation: Industrial Wi‑Fi systems need reliable, low‑latency communications; predicting short‑term changes in wireless channel quality (FDR) allows adaptive control to improve robustness.

Method: Train two deep models — a 1D Convolutional Neural Network and a Long Short‑Term Memory network — on time sequences of binary transmission outcomes collected from a real scenario, then compare prediction accuracy and computational complexity (inference latency) to assess suitability for constrained systems.

Result: Both CNN and LSTM achieve good FDR prediction accuracy using minimal input (single binary sequence). CNN yields substantially lower inference latency while suffering only marginal accuracy degradation compared to LSTM.

Conclusion: CNN is preferable for resource‑limited deployments due to low latency and near‑equal accuracy; LSTM may be chosen when maximum accuracy is required. Further validation and deployment tests are recommended.

Abstract: Ensuring reliable and predictable communications is one of the main goals in modern industrial systems that rely on Wi-Fi networks, especially in scenarios where continuity of operation and low latency are required. In these contexts, the ability to predict changes in wireless channel quality can enable adaptive strategies and significantly improve system robustness. This contribution focuses on the prediction of the Frame Delivery Ratio (FDR), a key metric that represents the percentage of successful transmissions, starting from time sequences of binary outcomes (success/failure) collected in a real scenario. The analysis focuses on two models of deep learning: a Convolutional Neural Network (CNN) and a Long Short-Term Memory network (LSTM), both selected for their ability to predict the outcome of time sequences. Models are compared in terms of prediction accuracy and computational complexity, with the aim of evaluating their applicability to systems with limited resources. Preliminary results show that both models are able to predict the evolution of the FDR with good accuracy, even from minimal information (a single binary sequence). In particular, CNN shows a significantly lower inference latency, with a marginal loss in accuracy compared to LSTM.

</details>


### [5] [Design and Evaluation of a Multi-Agent Perception System for Autonomous Flying Networks](https://arxiv.org/abs/2512.00259)
*Diogo Ferreira,Pedro Ribeiro,André Coelho,Rui Campos*

Main category: cs.NI

TL;DR: MAPS is a modular multi-agent perception system using multimodal LLMs and agentic AI on UAV-collected audio/visual data to produce Service Level Specifications (SLSs) for autonomous Flying Networks. On a synthetic emergency dataset it achieves >70% user detection accuracy and generates SLSs within 130s in 90% of trials; audio+visual fusion improves detection.


<details>
  <summary>Details</summary>
Motivation: Enable zero-touch autonomous Flying Networks by filling the missing perception capability for detecting users and their service demands from UAV sensor data, beyond placement/routing/resource management.

Method: Design a modular, scalable system (MAPS) combining multi-modal large language models and agentic AI agents that ingest visual and audio UAV data and output SLSs describing user count, spatial distribution, and traffic demand. Evaluated on a synthetic multimodal emergency dataset.

Result: On the synthetic dataset, MAPS achieved user detection accuracies above 70% and produced SLSs within 130 seconds in 90% of cases. Multimodal fusion (audio+visual) yielded improved user detection compared to single modalities.

Conclusion: MAPS demonstrates a practical perception layer for autonomous, zero-touch Flying Networks; multimodal LLM-driven agentic architectures can generate actionable SLSs from UAV sensor data, though validation is limited to synthetic scenarios.

Abstract: Autonomous Flying Networks (FNs) are emerging as a key enabler of on-demand connectivity in dynamic and infrastructure-limited environments. However, current approaches mainly focus on UAV placement, routing, and resource management, neglecting the autonomous perception of users and their service demands - a critical capability for zero-touch network operation.
  This paper presents the Multi-Agent Perception System (MAPS), a modular and scalable system that leverages multi-modal large language models (MM-LLMs) and agentic Artificial Intelligence (AI) to interpret visual and audio data collected by UAVs and generate Service Level Specifications (SLSs) describing user count, spatial distribution, and traffic demand. MAPS is evaluated using a synthetic multimodal emergency dataset, achieving user detection accuracies above 70% and SLS generation under 130 seconds in 90% of cases. Results demonstrate that combining audio and visual modalities enhances user detection and show that MAPS provides the perception layer required for autonomous, zero-touch FNs.

</details>


### [6] [Smart-TCP: An Agentic AI-based Autonomous and Adaptive TCP Protocol](https://arxiv.org/abs/2512.00491)
*Yule Han,Kezhi Wang,Kun Yang*

Main category: cs.NI

TL;DR: Smart-TCP reframes TCP control logic as an autonomous agent using LLMs and an ALU tool; it aggregates context, uses LLM reasoning, and reports 93.33% success in end-to-end session tasks, demonstrating feasibility.


<details>
  <summary>Details</summary>
Motivation: Traditional TCP relies on deterministic state machines which may not adapt well to intelligent/autonomous network architectures. The authors aim to explore an agentic AI paradigm (context perception, autonomous reasoning, tool use) to provide more flexible, adaptive protocol control.

Method: They build Smart-TCP: a context aggregation module, a Large Language Model for autonomous logical reasoning, and an Arithmetic Logic Unit invoked as a computation tool. They also design a dual-agent interaction framework to implement TCP protocol interactions and run experiments on static prediction and error detection tasks.

Result: Smart-TCP achieves strong performance in static prediction and error detection, with a reported 93.33% success rate in end-to-end sessions, indicating technical feasibility of using an agentic AI-based approach for TCP.

Conclusion: The results validate that reimagining TCP control as an autonomous LLM-driven agent is technically feasible and can perform protocol tasks effectively, suggesting a promising direction for intelligent network protocols.

Abstract: The Transmission Control Protocol (TCP) relies on a state machine and deterministic arithmetic to ensure reliable connections. However, traditional protocol logic driven by hard-coded state machines struggles to meet the demands of intelligent and autonomous network architectures. Here, we adopt the agentic AI-based paradigm, driven by Large Language Models (LLMs), characterized by context perception, autonomous reasoning, and tool use. Based on this, we propose Smart-TCP, which re-imagines TCP's core control logic as an autonomous agent. Specifically, the proposed architecture employs a context aggregation mechanism to synthesize the protocol context, utilizes the LLM for autonomous logical reasoning, and invokes an Arithmetic Logic Unit (ALU) as a tool for computation. Furthermore, we establish a dual-agent interaction framework based on this architecture and implement TCP protocol interactions. Experiments demonstrate that the Smart-TCP agent excels in static prediction and error detection, achieving a 93.33% success rate in end-to-end sessions. These results strongly validate the technical feasibility of an agentic AI-based TCP protocol.

</details>


### [7] [Velocity-Adaptive Access Scheme for Semantic-Aware Vehicular Networks: Joint Fairness and AoI Optimization](https://arxiv.org/abs/2512.01571)
*Xiao Xu,Qiong Wu,Pingyi Fan,Kezhi Wang,Nan Cheng,Wen Chen,Khaled B. Letaief*

Main category: cs.NI

TL;DR: The paper formulates a multi-objective optimization to jointly improve fair access and Age of Information (AoI) in 5G NR V2X Mode 2 by tuning selection windows, using semantic image communication, analyzing AoI with a Stochastic Hybrid System, convexifying via Sequential Convex Approximation, and validating with simulations and an LLM-based algorithm.


<details>
  <summary>Details</summary>
Motivation: Vehicles in V2X Mode 2 have heterogeneous communication durations (due to differing speeds), causing unequal data exchange with the RSU and potential safety risks; the paper aims to ensure fairness in access while keeping AoI low, accounting for 5G NR re-evaluation mechanisms and latency constraints.

Method: Define a fairness index via configurable selection windows; adopt an image semantic communication system to reduce latency; analyze AoI dynamics using Stochastic Hybrid Systems (SHS); formulate a multi-objective non-convex optimization for fairness and AoI; apply Sequential Convex Approximation (SCA) to convexify the problem and solve via convex optimization; propose an LLM-based algorithm; validate through numerical simulations.

Result: The proposed scheme (selection-window tuning + semantic communication + SCA-based optimization and LLM algorithm) is reported to achieve improved fairness and acceptable AoI trade-offs; numerical simulations demonstrate effectiveness under the modeled scenarios.

Conclusion: A joint design that trades off selection-window-based fairness and AoI—modeled via SHS and solved with SCA—can mitigate unequal data exchange in V2X Mode 2; semantic communication and an LLM-assisted algorithm further help reduce latency and improve performance, as supported by simulations.

Abstract: In this paper, we address the problem of fair access and Age of Information (AoI) optimization in 5G New Radio (NR) Vehicle to Everything (V2X) Mode 2. Specifically, vehicles need to exchange information with the road side unit (RSU). However, due to the varying vehicle speeds leading to different communication durations, the amount of data exchanged between different vehicles and the RSU may vary. This may poses significant safety risks in high-speed environments. To address this, we define a fairness index through tuning the selection window of different vehicles and consider the image semantic communication system to reduce latency. However, adjusting the selection window may affect the communication time, thereby impacting the AoI. Moreover, considering the re-evaluation mechanism in 5G NR, which helps reduce resource collisions, it may lead to an increase in AoI. We analyze the AoI using Stochastic Hybrid System (SHS) and construct a multi-objective optimization problem to achieve fair access and AoI optimization. Sequential Convex Approximation (SCA) is employed to transform the non-convex problem into a convex one, and solve it using convex optimization. We also provide a large language model (LLM) based algorithm. The scheme's effectiveness is validated through numerical simulations.

</details>


### [8] [HERMES: Heterogeneous Application-Enabled Routing Middleware for Edge-IoT Systems](https://arxiv.org/abs/2512.01824)
*Jéssica Consciência,António Grilo*

Main category: cs.NI

TL;DR: Framework that adds application-aware routing flexibility for heterogeneous multi-hop Wi‑Fi IoT devices (ESP8266, ESP32, Raspberry Pi), using a proactive routing layer and a middleware with three strategies to influence paths or topology; validated via distributed neural-network inference experiments showing heterogeneity and topology strongly affect performance.


<details>
  <summary>Details</summary>
Motivation: Traditional routing protocols are inflexible for IoT edge scenarios because they assume homogeneous devices and are agnostic to application-layer metrics and policies. With computation moving to the edge, there is a need to let applications influence routing decisions (for latency, energy, privacy, etc.).

Method: Implemented a proactive, fault-tolerant multi-hop Wi‑Fi routing layer on a physical testbed of heterogeneous devices (ESP8266, ESP32, Raspberry Pi 3B). Built a middleware exposing three strategies: two that adapt message paths en route to destination and one that lets applications modify network topology. The framework exposes a flexible interface and was tested with distributed neural-network inference workloads and offloading to a powerful node.

Result: Experiments on the physical testbed showed device heterogeneity significantly affects network performance. Analysis of throughput and inference duration demonstrated how the middleware strategies influence application behaviour and that topology critically impacts decentralized inference performance. The framework proved suitable for complex edge-intelligence tasks.

Conclusion: Application-aware routing and a middleware that enables path/topology control improve flexibility and can support distributed inference across heterogeneous IoT edge devices, but performance is strongly dependent on device heterogeneity and network topology.

Abstract: The growth of the Internet of Things has enabled a new generation of applications, pushing computation and intelligence toward the network edge. This trend, however, exposes challenges, as the heterogeneity of devices and the complex requirements of applications are often misaligned with the assumptions of traditional routing protocols, which lack the flexibility to accommodate application-layer metrics and policies. This work addresses this gap by proposing a software framework that enhances routing flexibility by dynamically incorporating application-aware decisions. The core of the work establishes a multi-hop Wi-Fi network of heterogeneous devices, specifically ESP8266, ESP32, and Raspberry Pi 3B. The routing layer follows a proactive approach, while the network is fault-tolerant, maintaining operation despite both node loss and message loss. On top of this, a middleware layer introduces three strategies for influencing routing behavior: two adapt the path a message traverses until arriving at the destination, while the third allows applications to shape the network topology. This layer offers a flexible interface for diverse applications. The framework was validated on a physical testbed through edge intelligence use cases, including distributing neural network inference computations across multiple devices and offloading the entire workload to the most capable node. Distributed inference is useful in scenarios requiring low latency, energy efficiency, privacy, and autonomy. Experimental results indicated that device heterogeneity significantly impacts network performance. Throughput and inference duration analysis showed the influence of the strategies on application behaviour, revealed that topology critically affects decentralized performance, and demonstrated the suitability of the framework for complex tasks.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [9] [IslandRun: Privacy-Aware Multi-Objective Orchestration for Distributed AI Inference](https://arxiv.org/abs/2512.00595)
*Bala Siva Sai Akhil Malepati*

Main category: cs.DC

TL;DR: IslandRun is a multi-objective orchestration system that treats compute resources as autonomous "islands" across personal devices, private edge, and public cloud. It uses agent-based routing, tiered trust groups, policy-constrained optimization, data-localized compute routing, and typed placeholder sanitization (reversible anonymization) to balance privacy, cost, latency, and trust for heterogeneous inference workloads.


<details>
  <summary>Details</summary>
Motivation: Existing orchestration frameworks optimize single dimensions (latency, privacy, cost) and fail under real-world heterogeneity where requests demand different trade-offs. The paper motivates a system that can handle request-level heterogeneity and respect trust boundaries while minimizing unnecessary data movement.

Method: IslandRun models resources as islands grouped by trust tiers and runs agent-based routing that applies policy-constrained multi-objective optimization to route requests. It emphasizes routing computation to where relevant data resides (data locality) and introduces typed placeholder sanitization to allow reversible anonymization while preserving semantic context across trust boundaries. The system combines tiered island groups, differential trust policies, and reversible anonymization to enable decentralized privacy-aware inference.

Result: The abstract claims IslandRun establishes a new paradigm that enables privacy-aware, decentralized inference orchestration across heterogeneous personal computing ecosystems. It implies improvements in aligning privacy, cost, and latency trade-offs and reducing data movement, though no quantitative results are provided in the abstract.

Conclusion: IslandRun proposes a practical architecture and techniques (agent routing, trust tiers, reversible sanitization) to reconcile competing objectives in distributed inference. It points to a research direction combining policy-driven optimization and data-local compute routing to support diverse, trust-sensitive inference workloads.

Abstract: Modern AI inference faces an irreducible tension: no single computational resource simultaneously maximizes performance, preserves privacy, minimizes cost, and maintains trust. Existing orchestration frameworks optimize single dimensions (Kubernetes prioritizes latency, federated learning preserves privacy, edge computing reduces network distance), creating solutions that struggle under real-world heterogeneity. We present IslandRun, a multi-objective orchestration system that treats computational resources as autonomous "islands" spanning personal devices, private edge servers, and public cloud. Our key insights: (1) request-level heterogeneity demands policy-constrained multi-objective optimization, (2) data locality enables routing compute to data rather than data to compute, and (3) typed placeholder sanitization preserves context semantics across trust boundaries. IslandRun introduces agent-based routing, tiered island groups with differential trust, and reversible anonymization. This establishes a new paradigm for privacy-aware, decentralized inference orchestration across heterogeneous personal computing ecosystems.

</details>


### [10] [SIMPLE: Disaggregating Sampling from GPU Inference into a Decision Plane for Faster Distributed LLM Serving](https://arxiv.org/abs/2512.00719)
*Bohan Zhao,Zane Cao,Yongchao He*

Main category: cs.DC

TL;DR: SIMPLE disaggregates LLM sampling to a CPU-side, sequence-parallel, overlappable decision plane that removes vocabulary collectives, uses linear-time single-pass CPU kernels with column-wise penalties and truncation-first filtering, and introduces speculative hot-vocab sampling (SHVS); it reduces the sampling bottleneck and yields up to 96% throughput gain and 20–65% P95 latency reduction without user code changes.


<details>
  <summary>Details</summary>
Motivation: As LLM training/inference scales with tensor and pipeline parallelism and GPU-optimized data-plane (attention/GEMM, KV cache) accelerates, sampling (the decision plane that converts logits to tokens) becomes a growing bottleneck because it neither shards with TP nor balances across PP stages; its relative cost increases with faster GPUs and limits pipeline frequency at the final stage.

Method: (1) Sequence-parallel sampling: shard sampling along batch dimension to avoid vocabulary-axis collectives. (2) CPU-based algorithm: implement column-wise penalties and truncation-first filtering to enable single-pass, linear-time kernels on CPUs. (3) Speculative Hot-Vocab Sampling (SHVS): sample from a small high-probability vocabulary subset with rejection-correctness and a sizing model that maximizes throughput.

Result: End-to-end throughput improvements up to 96% and P95 latency reductions of 20–65%; no user code changes required; composes with existing data-plane optimizations and maintains benefits as GPU performance scales.

Conclusion: SIMPLE effectively removes the decision-plane scaling bottleneck by moving sampling to an overlappable CPU service with sequence-parallelization and SHVS, restoring sampling to a minor runtime role and enabling further compounded scaling gains across future hardware generations.

Abstract: As large language models (LLMs) scale out with tensor parallelism (TP) and pipeline parallelism (PP) and production stacks have aggressively optimized the data plane (attention/GEMM and KV cache), sampling, the decision plane that turns logits into tokens, becomes a new bottleneck. This creates a structural holdout: sampling neither expands with TP nor balances across PP stages, so its share of iteration time grows as GPUs get faster and it caps pipeline frequency at the last stage. We present SIMPLE, a stage-agnostic, sequence-parallel, overlappable decision plane that disaggregates sampling into a CPU-side service and shrinks its runtime footprint back to a minor, hidden role. SIMPLE combines: (1) sequence-parallel sampling, which shards work along the batch dimension and removes vocabulary-axis collectives; (2) a CPU-based algorithm with column-wise penalties and truncation-first filtering to realize single-pass, linear-time kernels; and (3) speculative hot-vocab sampling (SHVS), which samples on a small hot set with rejection-correctness and uses a simple sizing model to choose the hot-vocab size that maximizes throughput. In evaluation, SIMPLE improves end-to-end throughput by up to 96% and reduces P95 latency by 20-65%. Crucially, SIMPLE requires no user-side code changes and composes with existing data-plane optimizations, unlocking scaling benefits that compound with future GPU generations.

</details>


### [11] [Elastic Mixture of Rank-Wise Experts for Knowledge Reuse in Federated Fine-Tuning](https://arxiv.org/abs/2512.00902)
*Yebo Wu,Jingguang Li,Zhijiang Guo,Li Li*

Main category: cs.DC

TL;DR: SmartFed is a federated fine-tuning framework that reuses pre-existing LoRA adapters to avoid training from scratch, using a Mixture of Rank-Wise Experts (MoRE) that decomposes LoRA modules into rank-level experts selected per-input and per-resource budget, and an Elastic Expert Quota Allocation (EEQA) that adaptively assigns capacity to matrices based on contribution. The system claims better model performance and training efficiency across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Federated fine-tuning adapts large language models to downstream tasks while preserving data privacy, but is costly in compute and communication for resource-limited devices. SmartFed aims to reduce these costs by reusing previously trained adapter modules (LoRA) and selectively activating small, relevant expert subcomponents, rather than training full adapters from scratch.

Method: SmartFed reuses knowledge embedded in existing LoRA modules. It introduces MoRE, which decomposes LoRA adapters into fine-grained rank-level experts; these experts are selectively activated and combined according to input semantics and device resource budgets. To focus computation on the most important parameters, SmartFed uses EEQA to adaptively allocate expert quotas across parameter matrices based on each matrix's contribution to performance.

Result: The abstract reports that SmartFed significantly outperforms existing federated fine-tuning methods on multiple benchmarks in both model performance and training efficiency. No numeric results are provided in the abstract.

Conclusion: SmartFed presents a promising, resource-efficient pathway for federated adaptation of LLMs by recycling LoRA modules and using fine-grained, budget-aware expert selection and quota allocation. The concept is strong, but the paper needs detailed empirical results, ablations (e.g., MoRE vs. full LoRA reuse), analysis of communication cost and convergence, and discussion of security/privacy/reporting of possible trade-offs.

Abstract: Federated fine-tuning offers a promising solution for adapting Large Language Models (LLMs) to downstream tasks while safeguarding data privacy. However, its high computational and communication demands hinder its deployment on resource-constrained devices. In this paper, we propose SmartFed, a resource-efficient federated fine-tuning framework. SmartFed intelligently reuses knowledge embedded in existing LoRA modules, eliminating the need for expensive training from scratch when adapting LLMs to new tasks. To effectively exploit this knowledge and ensure scalability, we introduce the Mixture of Rank-Wise Experts (MoRE). MoRE decomposes LoRA modules into fine-grained rank-level experts. These experts are selectively activated and combined based on input semantics and resource budgets. Moreover, to optimize resource utilization, we present the Elastic Expert Quota Allocation (EEQA). EEQA adaptively allocates expert capacity across parameter matrices based on their contribution to model performance, focusing computing resources on the critical experts. Extensive evaluations across multiple benchmarks demonstrate that SmartFed significantly outperforms existing methods in model performance and training efficiency.

</details>


### [12] [Joint Partitioning and Placement of Foundation Models for Real-Time Edge AI](https://arxiv.org/abs/2512.01039)
*Aladin Djuhera,Fernando Koch,Alecio Binotto*

Main category: cs.DC

TL;DR: Proposes a runtime-reconfigurable orchestration framework for partitioning and placing foundation models across heterogeneous edge resources, reacting to changing latency, utilization, and privacy conditions via dynamic graph re-partitioning and model-aware profiling.


<details>
  <summary>Details</summary>
Motivation: Static layer partitioning for large models assumes stable compute/network resources, which contradicts volatile edge environments. There is a need for an orchestration substrate that adapts spatial placement and internal segmentation of models at runtime to meet latency, utilization, and privacy constraints.

Method: Formalizes orchestration as a constrained optimization over layer-wise assignments with evolving constraints. Implements reactive inference composition combining model-aware capacity profiling, dynamic graph (re-)partitioning, and reallocation to respond to infrastructure changes. Describes architectural and algorithmic components and demonstrates a use case in 6G multi-access edge computing.

Result: Introduces a framework and algorithms enabling runtime-resolved placement and segmentation for foundation models; claims improved adaptability to fluctuating edge conditions (latency, utilization, privacy) and presents a representative MEC/6G use case to validate feasibility.

Conclusion: Runtime-resolved orchestration for model partitioning and placement better aligns inference deployments to volatile edge environments; the proposed framework and algorithms offer a path to more resilient, privacy-aware, and latency-sensitive inference in multi-access edge computing.

Abstract: Inference over large-scale foundation models within heterogeneous edge environments necessitates a fundamentally reconfigurable orchestration substrate. Static partitioning of model layers presumes temporal stability across compute and network resources, which is misaligned with the volatility of real-world deployments. We introduce a framework in which both the spatial placement and internal segmentation of foundation models are elevated to runtime-resolved constructs. The orchestration problem is formalized as a constrained optimization over layer-wise assignments, subject to evolving latency, utilization, and privacy gradients. The framework implements reactive inference composition responsive to infrastructural fluctuations by integrating model-aware capacity profiling with dynamic graph re-partitioning and reallocation. We introduce architectural and algorithmic components, along with a representative use case in 6G multi-access edge computing.

</details>


### [13] [Tangram: Accelerating Serverless LLM Loading through GPU Memory Reuse and Affinity](https://arxiv.org/abs/2512.01357)
*Wenbin Zhu,Zhaoyan Shen,Zili Shao,Hongjun Dai,Feng Chen*

Main category: cs.DC

TL;DR: Tangram is a system that reduces cold-start latency for serverless LLMs by reusing idle GPU memory to retain model parameters and by employing tensor-level sharing, on-demand KV cache allocation, and GPU-affinity-aware scheduling. It yields up to 6.2x faster model loading and lowers Time-To-First-Token by 23–55% compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Cold-start latency in serverless LLM deployments is dominated by model loading time, which grows with model size and undermines pay-as-you-go GPU sharing. Existing platforms inefficiently utilize GPU memory and re-transfer parameters frequently, making large LLMs impractical for serverless settings.

Method: Tangram introduces a unified GPU memory pool to store tensors for parameter sharing across models, an on-demand KV cache allocation scheme to avoid reserving large contiguous memory for every model instance, and GPU-affinity-aware scheduling to colocate workloads with retained parameters and maximize memory reuse. The system intercepts loading paths to keep parameters resident and manages dynamic allocation and scheduling to minimize transfers.

Result: A prototype of Tangram demonstrates up to 6.2x faster model loading and reduces Time-To-First-Token during cold-start by 23–55% relative to state-of-the-art baselines. Experiments show improved resource utilization and reduced transfer overheads across varying model sizes and workloads.

Conclusion: By exploiting idle GPU memory and combining tensor-level sharing with dynamic cache allocation and scheduling awareness, Tangram effectively mitigates the cold-start bottleneck in serverless LLM platforms, enabling more practical and responsive deployment of large models under pay-as-you-go GPU sharing.

Abstract: Serverless Large Language Models (LLMs) have emerged as a cost-effective solution for deploying AI services by enabling a 'pay-as-you-go' pricing model through GPU resource sharing. However, cold-start latency, especially the model loading phase, has become a critical performance bottleneck, as it scales linearly with model size and severely limits the practical deployment of large-scale LLM services. This paper presents Tangram, a novel system that accelerates Serverless LLM loading through efficient GPU memory reuse. By leveraging the unused GPU memory to retain model parameters, Tangram significantly reduces model transfer time and cold-start latency. Its design includes three key components: unified GPU memory pool for tensor-level parameter sharing across models, on-demand KV cache allocation for dynamic memory management, and GPU-affinity-aware scheduling for maximizing resource utilization. These techniques collectively address the critical challenges of inefficient memory usage and the cold-start problem in Serverless LLM platforms. We have implemented a fully functional prototype, and experiments show that Tangram achieves up to 6.2 times faster loading and reduces Time-To-First-Token (TTFT) during cold-start by 23--55% over state-of-the-art methods.

</details>


### [14] [Delta Sum Learning: an approach for fast and global convergence in Gossip Learning](https://arxiv.org/abs/2512.01549)
*Tom Goethals,Merlijn Sebrechts,Stijn De Schrijver,Filip De Turck,Bruno Volckaert*

Main category: cs.DC

TL;DR: The paper introduces Delta Sum Learning, an improved aggregation for Gossip (fully decentralized) Federated Learning, and packages it into an Open Application Model-based decentralized orchestration framework for edge deployment. Experiments show parity with alternatives at 10 nodes and a 58% smaller accuracy drop at 50 nodes, with logarithmic rather than linear accuracy degradation as topology size grows under limited connectivity.


<details>
  <summary>Details</summary>
Motivation: Existing averaging-based aggregation in Federated and Gossip Learning reduces model accuracy and harms global convergence, especially as the number of edge nodes and limited connectivity grow. Also, deploying learning workloads at the edge lacks a declarative, intent-driven approach (e.g., Kubernetes manifests) that supports dynamic node discovery and multi-workload apps.

Method: Propose Delta Sum Learning: a modified aggregation operator for Gossip Learning that operates on model deltas rather than simple averaging, designed to better preserve global convergence in decentralized peer-to-peer updates. Implement the approach within a decentralized orchestration framework using the Open Application Model to enable intent-driven, dynamic deployment and discovery of edge nodes and multi-workload applications.

Result: On small topologies (10 nodes), Delta Sum performs on par with alternative integration methods. On larger topologies (50 nodes), Delta Sum yields a 58% lower global accuracy drop relative to alternatives. The method demonstrates stronger global convergence and a logarithmic loss of accuracy with increasing topology size compared to linear loss for alternatives in limited connectivity settings.

Conclusion: Delta Sum Learning is an effective aggregation alternative to averaging in Gossip Learning, improving scalability and robustness of global accuracy. Coupling it with an OAM-based decentralized orchestration framework enables practical, declarative edge deployment. The method shows particular advantage as topology sizes increase under connectivity constraints.

Abstract: Federated Learning is a popular approach for distributed learning due to its security and computational benefits. With the advent of powerful devices in the network edge, Gossip Learning further decentralizes Federated Learning by removing centralized integration and relying fully on peer to peer updates. However, the averaging methods generally used in both Federated and Gossip Learning are not ideal for model accuracy and global convergence. Additionally, there are few options to deploy Learning workloads in the edge as part of a larger application using a declarative approach such as Kubernetes manifests. This paper proposes Delta Sum Learning as a method to improve the basic aggregation operation in Gossip Learning, and implements it in a decentralized orchestration framework based on Open Application Model, which allows for dynamic node discovery and intent-driven deployment of multi-workload applications. Evaluation results show that Delta Sum performance is on par with alternative integration methods for 10 node topologies, but results in a 58% lower global accuracy drop when scaling to 50 nodes. Overall, it shows strong global convergence and a logarithmic loss of accuracy with increasing topology size compared to a linear loss for alternatives under limited connectivity.

</details>
