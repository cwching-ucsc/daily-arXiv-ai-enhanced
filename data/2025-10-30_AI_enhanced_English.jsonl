{"id": "2510.25258", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.25258", "abs": "https://arxiv.org/abs/2510.25258", "authors": ["Xinru Tang", "Jingxiang Hou", "Dingcheng Jiang", "Taiquan Wei", "Jiaxin Liu", "Jinyi Deng", "Huizheng Wang", "Qize Yang", "Haoran Shang", "Chao Li", "Yang Hu", "Shouyi Yin"], "title": "MoEntwine: Unleashing the Potential of Wafer-scale Chips for Large-scale Expert Parallel Inference", "comment": null, "summary": "As large language models (LLMs) continue to scale up, mixture-of-experts\n(MoE) has become a common technology in SOTA models. MoE models rely on expert\nparallelism (EP) to alleviate memory bottleneck, which introduces all-to-all\ncommunication to dispatch and combine tokens across devices. However, in\nwidely-adopted GPU clusters, high-overhead cross-node communication makes\nall-to-all expensive, hindering the adoption of EP. Recently, wafer-scale chips\n(WSCs) have emerged as a platform integrating numerous devices on a wafer-sized\ninterposer. WSCs provide a unified high-performance network connecting all\ndevices, presenting a promising potential for hosting MoE models. Yet, their\nnetwork is restricted to a mesh topology, causing imbalanced communication\npressure and performance loss. Moreover, the lack of on-wafer disk leads to\nhigh-overhead expert migration on the critical path.\n  To fully unleash this potential, we first propose Entwined Ring Mapping\n(ER-Mapping), which co-designs the mapping of attention and MoE layers to\nbalance communication pressure and achieve better performance. We find that\nunder ER-Mapping, the distribution of cold and hot links in the attention and\nMoE layers is complementary. Therefore, to hide the migration overhead, we\npropose the Non-invasive Balancer (NI-Balancer), which splits a complete expert\nmigration into multiple steps and alternately utilizes the cold links of both\nlayers. Evaluation shows ER-Mapping achieves communication reduction up to 62%.\nNI-Balancer further delivers 54% and 22% improvements in MoE computation and\ncommunication, respectively. Compared with the SOTA NVL72 supernode, the WSC\nplatform delivers an average 39% higher per-device MoE performance owing to its\nscalability to larger EP.", "AI": {"tldr": "Paper studies running mixture-of-experts (MoE) LLMs on wafer-scale chips (WSCs). It proposes ER-Mapping to co-map attention and MoE layers to balance mesh-network communication and NI-Balancer to split and interleave expert migration using complementary \"cold\" links, hiding migration overhead. Evaluations show up to 62% communication reduction, NI-Balancer improves MoE computation by 54% and communication by 22%, and WSCs outperform an NVL72 supernode by ~39% per-device MoE performance.", "motivation": "MoE models need expert parallelism (EP) which requires costly all-to-all communication. Traditional GPU clusters suffer high cross-node overhead, limiting EP. WSCs offer a unified high-performance on-wafer network but with mesh topology and no on-wafer disk, causing imbalanced link pressure and costly expert migration. The paper aims to unlock WSCs for MoE by addressing these constraints.", "method": "Proposes Entwined Ring Mapping (ER-Mapping) that co-designs mapping of attention and MoE layers so their communication patterns' hot/cold links complement each other, reducing contention. Proposes Non-invasive Balancer (NI-Balancer) that decomposes expert migration into multiple steps and alternately uses the cold links of attention and MoE layers to hide migration overhead.", "result": "ER-Mapping reduces communication up to 62%. NI-Balancer yields 54% and 22% improvements in MoE computation and communication respectively. Overall, WSC platform achieves ~39% higher per-device MoE performance vs a state-of-the-art NVL72 supernode due to better scalability of EP.", "conclusion": "By co-designing layer mapping and migration scheduling (ER-Mapping + NI-Balancer), the paper mitigates mesh-network imbalance and migration overhead on WSCs, enabling more efficient and scalable MoE execution compared to current supernode GPU setups."}}
{"id": "2510.25145", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.25145", "abs": "https://arxiv.org/abs/2510.25145", "authors": ["Giancarlo Maldonado Cardenas", "Diana C. Gonzalez", "Judy C. Guevara", "Carlos A. Astudillo", "Nelson L. S. da Fonseca"], "title": "ML-Based Preamble Collision Detection in the Random Access Procedure of Cellular IoT Networks", "comment": null, "summary": "Preamble collision in the random access channel (RACH) is a major bottleneck\nin massive machine-type communication (mMTC) scenarios, typical of cellular IoT\n(CIoT) deployments. This work proposes a machine learning-based mechanism for\nearly collision detection during the random access (RA) procedure. A labeled\ndataset was generated using the RA procedure messages exchanged between the\nusers and the base station under realistic channel conditions, simulated in\nMATLAB. We evaluate nine classic classifiers -- including tree ensembles,\nsupport vector machines, and neural networks -- across four communication\nscenarios, varying both channel characteristics (e.g., Doppler spread,\nmultipath) and the cell coverage radius, to emulate realistic propagation,\nmobility, and spatial conditions. The neural network outperformed all other\nmodels, achieving over 98\\% balanced accuracy in the in-distribution evaluation\n(train and test drawn from the same dataset) and sustaining 95\\% under\nout-of-distribution evaluation (train/test from different datasets). To enable\ndeployment on typical base station hardware, we apply post-training\nquantization. Full integer quantization reduced inference time from 2500 ms to\nas low as 0.3 ms with negligible accuracy loss. The proposed solution combines\nhigh detection accuracy with low-latency inference, making it suitable for\nscalable, real-time CIoT applications found in real networks.", "AI": {"tldr": "ML-based early collision detection for RA in mMTC: a simulated dataset was used to train nine classifiers; a neural network achieved >98% balanced accuracy in-distribution and ~95% out-of-distribution. Post-training full integer quantization cut inference time from 2500 ms to 0.3 ms with negligible accuracy loss, enabling low-latency deployment on base-station hardware.", "motivation": "Preamble collisions in RACH limit scalability of CIoT/mMTC. Early detection of collisions allows faster resolution, reducing access delays and improving network efficiency in dense IoT deployments.", "method": "Generate labeled RA-message dataset under realistic channel models in MATLAB across four scenarios (varying Doppler, multipath, cell radius). Train and evaluate nine classic classifiers (tree ensembles, SVMs, NNs) in both in-distribution and out-of-distribution splits. Apply post-training full integer quantization to the best model for deployment and measure latency and accuracy.", "result": "A neural network outperformed other models: >98% balanced accuracy in-distribution, ~95% in out-of-distribution tests. Quantization reduced inference latency from 2500 ms to 0.3 ms while keeping accuracy nearly unchanged.", "conclusion": "The approach yields high early-collision detection accuracy and very low inference latency after quantization, making it practically suitable for scalable, real-time CIoT deployments on commodity base-station hardware."}}
{"id": "2510.25277", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.25277", "abs": "https://arxiv.org/abs/2510.25277", "authors": ["Simon S\u00fcwer", "Mai Khanh Mai", "Christoph Klein", "Nicola G\u00f6tzenberger", "Denis Dali\u0107", "Andreas Maier", "Jan Baumbach"], "title": "A Privacy-Preserving Ecosystem for Developing Machine Learning Algorithms Using Patient Data: Insights from the TUM.ai Makeathon", "comment": null, "summary": "The integration of clinical data offers significant potential for the\ndevelopment of personalized medicine. However, its use is severely restricted\nby the General Data Protection Regulation (GDPR), especially for small cohorts\nwith rare diseases. High-quality, structured data is essential for the\ndevelopment of predictive medical AI. In this case study, we propose a novel,\nmulti-stage approach to secure AI training: (1) The model is designed on a\nsimulated clinical knowledge graph (cKG). This graph is used exclusively to\nrepresent the structural characteristics of the real cKG without revealing any\nsensitive content. (2) The model is then integrated into the FeatureCloud (FC)\nfederated learning framework, where it is prepared in a single-client\nconfiguration within a protected execution environment. (3) Training then takes\nplace within the hospital environment on the real cKG, either under the direct\nsupervision of hospital staff or via a fully automated pipeline controlled by\nthe hospital. (4) Finally, verified evaluation scripts are executed, which only\nreturn aggregated performance metrics. This enables immediate performance\nfeedback without sensitive patient data or individual predictions, leaving the\nclinic. A fundamental element of this approach involves the incorporation of a\ncKG, which serves to organize multi-omics and patient data within the context\nof real-world hospital environments. This approach was successfully validated\nduring the TUM.ai Makeathon 2024 (TUMaiM24) challenge set by the Dr. von Hauner\nChildren's Hospital (HCH-LMU): 50 students developed models for patient\nclassification and diagnosis without access to real data. Deploying secure\nalgorithms via federated frameworks, such as the FC framework, could be a\npractical way of achieving privacy-preserving AI in healthcare.", "AI": {"tldr": "A multi-stage privacy-preserving workflow uses a simulated clinical knowledge graph and a federated learning framework (FeatureCloud) to design and deploy models trained inside hospital environments, validated in a Makeathon where 50 students developed models without accessing real patient data.", "motivation": "To enable development of predictive medical AI while complying with GDPR and protecting sensitive patient data, especially for small cohorts and rare diseases where data sharing is restricted; high-quality structured data and privacy-preserving training are needed.", "method": "(1) Model architecture and pipeline designed using a simulated clinical knowledge graph (cKG) that mirrors real graph structure without real data. (2) Model packaged into the FeatureCloud federated learning framework in a protected single-client setup. (3) Training executed inside hospitals on the real cKG under staff supervision or via an automated hospital-controlled pipeline. (4) Verified evaluation scripts run locally and return only aggregated performance metrics to prevent leakage of individual data. The cKG organizes multi-omics and patient data for realistic hospital deployment.", "result": "Validated during TUM.ai Makeathon 2024 (TUMaiM24) with Dr. von Hauner Children's Hospital challenge: 50 students built patient classification/diagnosis models without access to real data; framework provided secure algorithm deployment and feedback via aggregated metrics.", "conclusion": "The proposed approach demonstrates a practical pathway for privacy-preserving AI in healthcare by combining simulated knowledge graphs and federated frameworks, enabling model development without exposing sensitive patient-level data."}}
{"id": "2510.25757", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.25757", "abs": "https://arxiv.org/abs/2510.25757", "authors": ["Jonas Spenger", "Kolya Krafeld", "Ruben van Gemeren", "Philipp Haller", "Paris Carbone"], "title": "Holon Streaming: Global Aggregations with Windowed CRDTs", "comment": "10 pages, 9 figures, 2 tables, 2 listings, 2 algorithms", "summary": "Scaling global aggregations is a challenge for exactly-once stream processing\nsystems. Current systems implement these either by computing the aggregation in\na single task instance, or by static aggregation trees, which limits\nscalability and may become a bottleneck. Moreover, the end-to-end latency is\ndetermined by the slowest path in the tree, and failures and reconfiguration\ncause large latency spikes due to the centralized coordination. Towards these\nissues, we present Holon Streaming, an exactly-once stream processing system\nfor global aggregations. Its deterministic programming model uses windowed\nconflict-free replicated data types (Windowed CRDTs), a novel abstraction for\nshared replicated state. Windowed CRDTs make computing global aggregations\nscalable. Furthermore, their guarantees such as determinism and convergence\nenable the design of efficient failure recovery algorithms by decentralized\ncoordination. Our evaluation shows a 5x lower latency and 2x higher throughput\nthan an existing stream processing system on global aggregation workloads, with\nan 11x latency reduction under failure scenarios. The paper demonstrates the\neffectiveness of decentralized coordination with determinism, and the utility\nof Windowed CRDTs for global aggregations.", "AI": {"tldr": "Holon Streaming introduces Windowed CRDTs \u2014 a deterministic, decentralized approach for scalable, exactly-once global aggregations in stream processing \u2014 yielding substantially lower latency and higher throughput than an existing system, especially during failures.", "motivation": "Global aggregations are hard to scale in exactly-once stream processors: single-task aggregation or static aggregation trees create bottlenecks, increase end-to-end latency (bounded by slowest tree path), and centralized coordination worsens latency spikes under failures and reconfiguration.", "method": "Design Holon Streaming with a deterministic programming model based on Windowed CRDTs (conflict-free replicated data types scoped to windows) to represent shared replicated state. Use determinism and CRDT convergence guarantees to enable decentralized coordination and efficient failure-recovery algorithms, avoiding centralized bottlenecks.", "result": "Compared to an existing stream processing system on global aggregation workloads, Holon Streaming achieves ~5x lower latency and ~2x higher throughput in normal operation, and an ~11x latency reduction during failure scenarios.", "conclusion": "Deterministic, CRDT-based abstractions (Windowed CRDTs) plus decentralized coordination materially improve scalability, latency, and failure behavior for global aggregations in exactly-once stream processing."}}
