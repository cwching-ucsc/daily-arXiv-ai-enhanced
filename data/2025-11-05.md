<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 6]
- [cs.NI](#cs.NI) [Total: 3]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Conceptual Design Report for FAIR Computing](https://arxiv.org/abs/2511.01861)
*Johan Messchendorp,Mohammad Al-Turany,Volker Friese,Thorsten Kollegger,Bastian Loeher,Jochen Markert,Andrew Mistry,Thomas Neff,Adrian Oeftiger,Michael Papenbrock,Stephane Pietri,Shahab Sanjari,Tobias Stockmanns*

Main category: cs.DC

TL;DR: CDR outlines plans for FAIR computing infrastructure (2028 onward), proposing a federated, centrally-orchestrated, scalable and flexible model with open data/software/services policies to support diverse research lines through FAIR's modular start-up phases.


<details>
  <summary>Details</summary>
Motivation: Provide a coherent computing and storage infrastructure plan to meet the diverse, growing data and compute needs of FAIR experiments starting with the "first science (plus)" phase through the modularized start version, ensuring openness and scalability.

Method: Collect and present computing requirements from research groups, define policies (computing, storage, open data/software/services), and specify an architectural model: a federated yet centrally-orchestrated infrastructure with policy and service layers for orchestration and openness.

Result: A proposed computing model and architecture, policy frameworks, and high-level plans for deployment and operation covering the period from 2028 onwards; emphasis on federation, central orchestration, scalability, and flexibility.

Conclusion: The CDR advocates building a federated, centrally-orchestrated computing infrastructure for FAIR that adheres to open data and software principles and is scalable/flexible enough to handle future data challenges.

Abstract: This Conceptual Design Report (CDR) presents the plans of the computing
infrastructure for research at FAIR, Darmstadt, Germany. It presents the
computing requirements of the various research groups, the policies for the
computing and storage infrastructure, the foreseen FAIR computing model
including the open data, software and services policies and architecture for
the periods starting in 2028 with the "first science (plus)" phase to the
modularized start version of FAIR. The overall ambition is to create a
federated and centrally-orchestrated infrastructure serving the large diversity
of the research lines present with sufficient scalability and flexibility to
cope with future data challenges that will be present at FAIR.

</details>


### [2] [EdgeReasoning: Characterizing Reasoning LLM Deployment on Edge GPUs](https://arxiv.org/abs/2511.01866)
*Benjamin Kubwimana,Qijing Huang*

Main category: cs.DC

TL;DR: EdgeReasoning studies deployment of reasoning-capable LLMs on edge GPUs, measuring latency-accuracy tradeoffs and evaluating token-reduction and test-time scaling techniques to find Pareto-optimal configurations for edge reasoning.


<details>
  <summary>Details</summary>
Motivation: Edge/autonomous systems require local, low-latency, energy-efficient reasoning while preserving privacy and operating with intermittent connectivity; existing guidance on how to pick model architecture, size, token budgets, and scaling strategies for edge GPUs is lacking.

Method: Systematic empirical evaluation across LLM architectures and sizes on edge GPUs; comparison of prompt-based and model-tuning techniques to shorten reasoning token length; profiling test-time scaling methods with different parallelism levels to maximize accuracy under latency constraints; mapping accuracy-latency Pareto frontiers.

Result: Quantified latency-accuracy tradeoffs for varied architectures and sizes; identified effective prompt- and tuning-based token-reduction techniques that retain performance; characterized how test-time scaling and parallelism impact achievable accuracy under strict latency budgets; produced Pareto frontiers guiding deployment choices.

Conclusion: EdgeReasoning provides practical, systematic guidance for selecting model family, size, token budget, and runtime scaling to meet latency targets while optimizing reasoning accuracy on edge GPUs, enabling informed tradeoffs and efficient edge deployment of reasoning LLMs.

Abstract: Edge intelligence paradigm is increasingly demanded by the emerging
autonomous systems, such as robotics. Beyond ensuring privacy-preserving
operation and resilience in connectivity-limited environments, edge deployment
offers significant energy and cost advantages over cloud-based solutions.
However, deploying large language models (LLMs) for reasoning tasks on edge
GPUs faces critical challenges from strict latency constraints and limited
computational resources. To navigate these constraints, developers must balance
multiple design factors - choosing reasoning versus non-reasoning
architectures, selecting appropriate model sizes, allocating token budgets, and
applying test-time scaling strategies - to meet target latency and optimize
accuracy. Yet guidance on optimal combinations of these variables remains
scarce. In this work, we present EdgeReasoning, a comprehensive study
characterizing the deployment of reasoning LLMs on edge GPUs. We systematically
quantify latency-accuracy tradeoffs across various LLM architectures and model
sizes. We systematically evaluate prompt-based and model-tuning-based
techniques for reducing reasoning token length while maintaining performance
quality. We further profile test-time scaling methods with varying degrees of
parallelism to maximize accuracy under strict latency budgets. Through these
analyses, EdgeReasoning maps the Pareto frontier of achievable accuracy-latency
configurations, offering systematic guidance for optimal edge deployment of
reasoning LLMs.

</details>


### [3] [Eliminating Multi-GPU Performance Taxes: A Systems Approach to Efficient Distributed LLMs](https://arxiv.org/abs/2511.02168)
*Octavian Alexandru Trifan,Karthik Sangaiah,Muhammad Awad,Muhammad Osama,Sumanth Gudaparthi,Alexandru Nicolau,Alexander Veidenbaum,Ganesh Dasika*

Main category: cs.DC

TL;DR: Paper identifies three sources of inefficiency when running distributed LLM kernels under BSP and proposes using in-kernel communication primitives (via Iris for Triton) to implement tile-level producer-consumer pipelines and fine-grained dataflow synchronization. Applied to All-Gather+GEMM and Flash Decode kernels, this approach reduces end-to-end latency by about 10–20% vs BSP implementations.


<details>
  <summary>Details</summary>
Motivation: As LLMs scale, multi-GPU distributed execution is necessary but the dominant bulk synchronous parallel (BSP) model imposes global barriers, poor inter-kernel data locality, and frequent kernel launches, which together limit performance and hardware utilization.

Method: Introduce an analytical framework called the "Three Taxes" (BSP, inter-kernel data locality, kernel launch overhead). Use Iris for Triton to access in-kernel communication primitives and design fine-grained programming patterns that create direct, tile-level producer-consumer pipelines replacing global barriers with dataflow synchronization. Apply these patterns to key kernels (All-Gather + GEMM and Flash Decode) and benchmark against BSP-based implementations.

Result: Empirical evaluation shows systematic elimination of the three taxes and delivers roughly 10–20% improvement in end-to-end latency for distributed LLM workloads compared to BSP baselines.

Conclusion: Fine-grained in-kernel communication and tile-level pipelining provide a more programmable and efficient execution model for distributed LLMs, yielding measurable latency improvements over traditional BSP approaches and enabling a new class of performance-optimized kernel designs.

Abstract: As large language models (LLMs) continue to scale, their workloads
increasingly rely on distributed execution across multiple GPUs. However, the
conventional bulk synchronous parallel~(BSP) model used in such settings
introduces significant performance inefficiencies. To characterize these
bottlenecks, we introduce the ''Three Taxes'' (Bulk Synchronous, Inter-Kernel
Data Locality, and Kernel Launch Overhead) as an analytical framework. We
propose moving beyond the rigid BSP model to address key inefficiencies in
distributed GPU execution. By exploiting libraries like Iris for Triton, we
gain access to in-kernel communication primitives that enable the design of
novel fine-grained programming patterns, offering greater flexibility and
performance than traditional BSP-based approaches. These patterns
systematically eliminate the three taxes by creating direct, tile-level
producer-consumer pipelines and replacing global barriers with fine-grained
dataflow synchronization. Applying this methodology to critical kernels, from
the foundational All-Gather + general matrix multiplication operation to the
complex Flash Decode algorithm, we observe a 10-20% speedup in end-to-end
latency over BSP-based approaches, establishing a more programmable and
efficient paradigm for distributed LLM workloads.

</details>


### [4] [From Models to Operators: Rethinking Autoscaling Granularity for Large Generative Models](https://arxiv.org/abs/2511.02248)
*Xingqi Cui,Chieh-Jan Mike Liang,Jiarong Xing,Haoran Qiu*

Main category: cs.DC

TL;DR: Operators inside generative models are heterogeneous; autoscaling at operator granularity (scaling, batching, placement per-operator) yields large efficiency gains over model-level scaling, preserving SLOs with fewer GPUs and lower energy or providing higher throughput under fixed resources.


<details>
  <summary>Details</summary>
Motivation: Existing deployment schemes treat models as monoliths, leading to poor adaptation to dynamic online inference traffic and resulting in either missed SLOs or resource underutilization. The paper targets better resource efficiency while meeting user-facing SLOs.

Method: Characterize operator-level compute/memory heterogeneity and sensitivity to batch size, sequence length, and traffic. Propose an operator-level autoscaling framework that decides scaling, batching, and placement per operator based on operator profiles.

Result: On production-scale traces, the approach maintains SLOs with up to 40% fewer GPUs and 35% less energy, or achieves 1.6× higher throughput with 5% less energy under fixed resources.

Conclusion: Scaling at operator granularity is more effective than model-level or static provisioning for serving large generative workloads, yielding substantial efficiency and throughput improvements.

Abstract: Serving large generative models such as LLMs and multi- modal transformers
requires balancing user-facing SLOs (e.g., time-to-first-token,
time-between-tokens) with provider goals of efficiency and cost reduction.
Existing solutions rely on static provisioning or model-level autoscaling, both
of which treat the model as a monolith. This coarse-grained resource management
leads to degraded performance or significant resource underutilization due to
poor adaptability to dynamic inference traffic that is common online.
  The root cause of this inefficiency lies in the internal structure of
generative models: they are executed as graphs of interconnected operators.
Through detailed characterization and systematic analysis, we find that
operators are heterogeneous in their compute and memory footprints and exhibit
diverse sensitivity to workload and resource factors such as batch size,
sequence length, and traffic rate. This heterogeneity suggests that the
operator, rather than the entire model, is the right granularity for scaling
decisions.
  We propose an operator-level autoscaling framework, which allocates resources
at finer (operator)-granularity, optimizing the scaling, batching, and
placement based on individual operator profiles. Evaluated on production-scale
traces, our approach preserves SLOs with up to 40% fewer GPUs and 35% less
energy, or under fixed resources achieves 1.6x higher throughput with 5% less
energy. These results show that the operator, rather than the model, is
fundamentally a more effective unit for scaling large generative workloads.

</details>


### [5] [3D Point Cloud Object Detection on Edge Devices for Split Computing](https://arxiv.org/abs/2511.02293)
*Taisuke Noguchi,Takuya Azumi*

Main category: cs.DC

TL;DR: The paper applies Split Computing to LiDAR-based 3D object detection, showing large reductions in inference and edge execution time by splitting after voxelization or within the network.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art 3D LiDAR detection models are computationally heavy for edge devices, increasing latency and power consumption; split computing can offload work and reduce data exposure by transmitting intermediate representations.

Method: Apply distributed inference (split computing) to a LiDAR 3D detection pipeline; evaluate splits after voxelization and at various internal network layers; measure inference time and edge-device execution time reductions.

Result: Splitting after voxelization reduced total inference time by 70.8% and edge execution time by 90.0%. Splits within the network reduced inference time by up to 57.1% and edge execution time by up to 69.5%.

Conclusion: Split computing can greatly reduce processing time and edge compute load for LiDAR 3D detection while reducing data transmitted; optimal split points (e.g., after voxelization) yield major gains.

Abstract: The field of autonomous driving technology is rapidly advancing, with deep
learning being a key component. Particularly in the field of sensing, 3D point
cloud data collected by LiDAR is utilized to run deep neural network models for
3D object detection. However, these state-of-the-art models are complex,
leading to longer processing times and increased power consumption on edge
devices. The objective of this study is to address these issues by leveraging
Split Computing, a distributed machine learning inference method. Split
Computing aims to lessen the computational burden on edge devices, thereby
reducing processing time and power consumption. Furthermore, it minimizes the
risk of data breaches by only transmitting intermediate data from the deep
neural network model. Experimental results show that splitting after
voxelization reduces the inference time by 70.8% and the edge device execution
time by 90.0%. When splitting within the network, the inference time is reduced
by up to 57.1%, and the edge device execution time is reduced by up to 69.5%.

</details>


### [6] [Federated Attention: A Distributed Paradigm for Collaborative LLM Inference over Edge Networks](https://arxiv.org/abs/2511.02647)
*Xiumei Deng,Zehui Xiong,Binbin Chen,Dong In Kim,Merouane Debbah,H. Vincent Poor*

Main category: cs.DC

TL;DR: FedAttn is a distributed LLM inference framework that embeds federated principles into self-attention: participants compute local attention over their tokens and periodically exchange/aggregate Key-Value (KV) matrices across Transformer blocks to collaboratively generate responses while keeping prompts local. The paper proves a duality with federated optimization, analyzes error propagation and trade-offs between quality and communication/computation, and validates gains experimentally using sparse attention and adaptive KV aggregation.


<details>
  <summary>Details</summary>
Motivation: Edge and collaborative LLM deployments face privacy risks (private prompts), high communication costs, and computation constraints. The authors aim to enable collaborative inference that preserves privacy, reduces communication, and offloads computation across participants.

Method: Introduce Federated Attention (FedAttn): local self-attention per participant; periodic exchange and aggregation of KV matrices across multiple Transformer blocks; map contextual refinement to federated optimization to adapt federated techniques; theoretical analysis of error propagation due to local computation and heterogeneous token relevance; derive trade-off governed by synchronization interval and participant count; propose sparse attention and adaptive KV aggregation as optimizations.

Result: Theoretical bounds characterize how local attention and token heterogeneity affect error propagation across blocks and how synchronization/participant count affect quality vs. cost trade-offs. Experiments confirm theoretical predictions and show that sparse attention and adaptive KV aggregation substantially reduce communication/computation while retaining response quality.

Conclusion: FedAttn provides a principled framework for privacy-aware, communication- and computation-efficient distributed LLM inference by treating attention aggregation as a federated process. It exposes tunable trade-offs (sync interval, participant number) and optimization opportunities (sparsity, adaptive aggregation) for scalable edge deployments.

Abstract: Large language models (LLMs) are proliferating rapidly at the edge,
delivering intelligent capabilities across diverse application scenarios.
However, their practical deployment in collaborative scenarios confronts
fundamental challenges: privacy vulnerabilities, communication overhead, and
computational bottlenecks. To address these, we propose Federated Attention
(FedAttn), which integrates the federated paradigm into the self-attention
mechanism, creating a new distributed LLM inference framework that
simultaneously achieves privacy protection, communication efficiency, and
computational efficiency. FedAttn enables participants to perform local
self-attention over their own token representations while periodically
exchanging and aggregating Key-Value (KV) matrices across multiple Transformer
blocks, collaboratively generating LLM responses without exposing private
prompts. Further, we identify a structural duality between contextual
representation refinement in FedAttn and parameter optimization in FL across
private data, local computation, and global aggregation. This key insight
provides a principled foundation for systematically porting federated
optimization techniques to collaborative LLM inference. Building on this
framework, we theoretically analyze how local self-attention computation within
participants and heterogeneous token relevance among participants shape error
propagation dynamics across Transformer blocks. Moreover, we characterize the
fundamental trade-off between response quality and communication/computation
efficiency, which is governed by the synchronization interval and the number of
participants. Experimental results validate our theoretical analysis, and
reveal significant optimization opportunities through sparse attention and
adaptive KV aggregation, highlighting FedAttn's potential to deliver
scalability and efficiency in real-world edge deployments.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [7] [Lightweight Latency Prediction Scheme for Edge Applications: A Rational Modelling Approach](https://arxiv.org/abs/2511.02501)
*Mohan Liyanage,Eldiyar Zhantileuov,Ali Kadhum Idrees,Rolf Schuster*

Main category: cs.NI

TL;DR: Lightweight rational model predicts end-to-end network latency from passive features (frame size, arrival rate, link utilization) with high accuracy and low inference cost, avoiding active probing.


<details>
  <summary>Details</summary>
Motivation: Enable reliable task offloading in real-time edge computing by providing accurate, low-overhead predictions of end-to-end latency to support scheduling and resource decisions.

Method: Proposes a rational (parameterized analytic) modelling approach using passive network features (frame size, arrival rate, link utilization) for latency prediction; evaluated via extensive experiments and 5-fold cross-validation; compared against traditional regressors and neural networks.

Result: Achieves strong predictive performance (MAE=0.0115, R^2=0.9847) with competitive inference time, claimed to provide a favorable precision-efficiency trade-off relative to other models.

Conclusion: Rational modelling using passive features offers state-of-the-art latency prediction suitable for real-time edge computing, reducing the need for intrusive active probing while maintaining accuracy and efficiency.

Abstract: Accurately predicting end-to-end network latency is essential for enabling
reliable task offloading in real-time edge computing applications. This paper
introduces a lightweight latency prediction scheme based on rational modelling
that uses features such as frame size, arrival rate, and link utilization,
eliminating the need for intrusive active probing. The model achieves
state-of-the-art prediction accuracy through extensive experiments and 5-fold
cross-validation (MAE = 0.0115, R$^2$ = 0.9847) with competitive inference
time, offering a substantial trade-off between precision and efficiency
compared to traditional regressors and neural networks.

</details>


### [8] [On the Optimization of Model Aggregation for Federated Learning at the Network Edge](https://arxiv.org/abs/2511.02703)
*Mengyao Li,Noah Ploch,Sebastian Troia,Carlo Spatocco,Wolfgang Kellerer,Guido Maier*

Main category: cs.NI

TL;DR: Proposes using intermediate edge aggregators and an aggregator overlay network to manage federated learning model aggregation across MEC and SD-WAN infrastructures. Introduces an ILP and a practical heuristic to route aggregated updates, reducing FL training-round failures and cloud-link congestion.


<details>
  <summary>Details</summary>
Motivation: Massive growth in connected devices stresses compute and network resources; centralized aggregation causes bottlenecks. Leveraging edge computing and programmable wide-area networks can reduce congestion and improve FL scalability and robustness.

Method: Designs online resource-management strategies that perform intermediate aggregation at edge nodes and, to further mitigate congestion, deploys an aggregator overlay network. Formulates routing and allocation as an Integer Linear Program (ILP) and develops a heuristic for scalable, near-real-time routing decisions.

Result: Simulation/experimental results claim improved resource utilization, reduced FL training-round failure rates by up to 15%, and decreased load on cloud links. The heuristic provides practical performance close to the ILP solution with lower computation overhead.

Conclusion: Edge-level aggregation plus an overlay routing layer can substantially improve FL scalability and reliability; the ILP gives optimal benchmarks while the heuristic offers a deployable approach. Future work should address complexity, real-world deployment, privacy, and resilience to dynamics.

Abstract: The rapid increase in connected devices has signifi- cantly intensified the
computational and communication demands on modern telecommunication networks.
To address these chal- lenges, integrating advanced Machine Learning (ML)
techniques like Federated Learning (FL) with emerging paradigms such as
Multi-access Edge Computing (MEC) and Software-Defined Wide Area Networks
(SD-WANs) is crucial. This paper intro- duces online resource management
strategies specifically designed for FL model aggregation, utilizing
intermediate aggregation at edge nodes. Our analysis highlights the benefits of
incorporating edge aggregators to reduce network link congestion and maximize
the potential of edge computing nodes. However, the risk of network congestion
persists. To mitigate this, we propose a novel aggregation approach that
deploys an aggregator overlay network. We present an Integer Linear Programming
(ILP) model and a heuristic algorithm to optimize the routing within this
overlay network. Our solution demonstrates improved adapt- ability to network
resource utilization, significantly reducing FL training round failure rates by
up to 15% while also alleviating cloud link congestion.

</details>


### [9] [Agentic World Modeling for 6G: Near-Real-Time Generative State-Space Reasoning](https://arxiv.org/abs/2511.02748)
*Farhad Rezazadeh,Hatim Chergui,Merouane Debbah,Houbing Song,Dusit Niyato,Lingjia Liu*

Main category: cs.NI

TL;DR: Proposes WM-MS3M, an action-conditioned world model for near-RT O-RAN control that models aleatoric/epistemic uncertainty and enables counterfactual what-if forecasting. Uses MPC/CEM planning over PRB actions. Shows modest MAE improvement vs MS3M with fewer parameters and larger RMSE and latency gains vs attention/hybrid baselines on realistic traces.


<details>
  <summary>Details</summary>
Motivation: Argues 6G intelligence should be able to imagine and choose via counterfactual simulation rather than rely on token-prediction primitives (LLMs). Applies this to O-RAN near-RT control where treating PRBs as causal control inputs and evaluating hypothetical sequences is valuable for KPI-driven decisions.

Method: Introduces WM-MS3M: couples multi-scale structured state-space mixtures (MS3M) with a compact stochastic latent to produce an action-conditioned generative state space. Models both aleatoric and epistemic uncertainty. Uses an MPC-style planner with cross-entropy method (CEM) that rolls out prior-mean trajectories within data-driven PRB bounds to maximize deterministic reward over short horizons.

Result: On realistic O-RAN traces, WM-MS3M reduces MAE by 1.69% versus MS3M while using 32% fewer parameters and similar latency. Achieves 35–80% lower RMSE than attention/hybrid baselines with 2.3–4.1× faster inference. Enables rare-event simulation and offline policy screening.

Conclusion: World-modeling with structured state-space mixtures and action conditioning is effective for near-RT O-RAN control: it improves accuracy, efficiency, and enables counterfactual planning. Promising direction for 6G intelligence; further work should test longer horizons, causal identification, real deployment, robustness, and multi-agent settings.

Abstract: We argue that sixth-generation (6G) intelligence is not fluent token
prediction but the capacity to imagine and choose -- to simulate future
scenarios, weigh trade-offs, and act with calibrated uncertainty. We reframe
open radio access network (O-RAN) near-real-time (Near-RT) control via
counterfactual dynamics and a world modeling (WM) paradigm that learns an
action-conditioned generative state space. This enables quantitative "what-if"
forecasting beyond large language models (LLMs) as the primary modeling
primitive. Actions such as physical resource blocks (PRBs) are treated as
first-class control inputs in a causal world model, and both aleatoric and
epistemic uncertainty are modeled for prediction and what-if analysis. An
agentic, model predictive control (MPC)-based cross-entropy method (CEM)
planner operates over short horizons, using prior-mean rollouts within
data-driven PRB bounds to maximize a deterministic reward. The model couples
multi-scale structured state-space mixtures (MS3M) with a compact stochastic
latent to form WM-MS3M, summarizing key performance indicators (KPIs) histories
and predicting next-step KPIs under hypothetical PRB sequences. On realistic
O-RAN traces, WM-MS3M cuts mean absolute error (MAE) by 1.69% versus MS3M with
32% fewer parameters and similar latency, and achieves 35-80% lower root mean
squared error (RMSE) than attention/hybrid baselines with 2.3-4.1x faster
inference, enabling rare-event simulation and offline policy screening.

</details>
