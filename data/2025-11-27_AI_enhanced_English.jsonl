{"id": "2511.20961", "categories": ["cs.NI", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.20961", "abs": "https://arxiv.org/abs/2511.20961", "authors": ["Kasidis Arunruangsirilert", "Bo Wei", "Hang Song", "Jiro Katto"], "title": "Performance Evaluation of Low-Latency Live Streaming of MPEG-DASH UHD video over Commercial 5G NSA/SA Network", "comment": "2022 International Conference on Computer Communications and Networks (ICCCN), 25-28 July 2022, Honolulu, HI, USA", "summary": "5G Standalone (SA) is the goal of the 5G evolution, which aims to provide higher throughput and lower latency than the existing LTE network. One of the main applications of 5G is the real-time distribution of Ultra High-Definition (UHD) content with a resolution of 4K or 8K. In Q2/2021, Advanced Info Service (AIS), the biggest operator in Thailand, launched 5G SA, providing both 5G SA/NSA service nationwide in addition to the existing LTE network. While many parts of the world are still in process of rolling out the first phase of 5G in Non-Standalone (NSA) mode, 5G SA in Thailand already covers more than 76% of the population.\n  In this paper, UHD video will be a real-time live streaming via MPEG-DASH over different mobile network technologies with minimal buffer size to provide the lowest latency. Then, performance such as the number of dropped segments, MAC throughput, and latency are evaluated in various situations such as stationary, moving in the urban area, moving at high speed, and also an ideal condition with maximum SINR. It has been found that 5G SA can deliver more than 95% of the UHD video segment successfully within the required time window in all situations, while 5G NSA produced mixed results depending on the condition of the LTE network. The result also reveals that the LTE network failed to deliver more than 20% of the video segment within the deadline, which shows that 5G SA is absolutely necessary for low-latency UHD video streaming and 5G NSA may not be good enough for such task as it relies on the legacy control signal.", "AI": {"tldr": "This paper evaluates real-time UHD (4K/8K) MPEG\u2011DASH streaming with minimal client buffer over 5G Standalone (SA), 5G Non\u2011Standalone (NSA), and LTE using live measurements in stationary, urban mobility, high\u2011speed and ideal SINR scenarios. Results show 5G SA delivers >95% of segments within deadline in all tests, 5G NSA is inconsistent, and LTE misses >20% of segments\u2014leading the authors to conclude 5G SA is necessary for low\u2011latency UHD streaming.", "motivation": "To determine whether 5G SA\u2014recently rolled out nationwide by a major Thai operator\u2014can meet the strict low\u2011latency, high\u2011throughput demands of real\u2011time UHD live streaming, compared with 5G NSA and legacy LTE.", "method": "Live MPEG\u2011DASH streaming with minimal buffer to prioritize latency. Measured metrics: number of dropped/late segments, MAC throughput, and latency. Test scenarios: stationary, urban mobility, high\u2011speed mobility, and ideal maximum SINR. Comparative analysis across 5G SA, 5G NSA and LTE.", "result": "5G SA consistently delivered >95% of UHD segments within the required deadline across all scenarios. 5G NSA showed mixed performance dependent on LTE control-plane conditions. LTE failed to deliver >20% of segments on time. MAC throughput and latency measurements corroborated these delivery outcomes.", "conclusion": "5G SA is substantially better suited for low\u2011latency UHD live streaming than 5G NSA or LTE. Reliance of NSA on the legacy LTE control plane can prevent it from meeting strict real\u2011time delivery requirements."}}
{"id": "2511.21084", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.21084", "abs": "https://arxiv.org/abs/2511.21084", "authors": ["Ahmadreza Majlesara", "Ali Majlesi", "Ali Mamaghani", "Alireza Shokrani", "Babak Hossein Khalaj"], "title": "5G Network Automation Using Local Large Language Models and Retrieval-Augmented Generation", "comment": "4 pages, 1 figure, demonstrated on 6G Summit Abu Dhabi", "summary": "This demonstration showcases the integration of a lightweight, locally deployed Large Language Model (LLaMA-3 8b Q-4b) empowered by retrieval augmented generation (RAG) to automate 5G network management, with a strong emphasis on privacy. By running the LLM on local or edge devices ,we eliminate the need for external APIs, ensuring that sensitive data remains secure and is not transmitted over the internet. Although lightweight models may not match the performance of more complex models like GPT-4, we enhance their efficiency and accuracy through RAG. RAG retrieves relevant information from a comprehensive database, enabling the LLM to generate more precise and effective network configurations based on natural language user input. This approach not only improves the accuracy of the generated configurations but also simplifies the process of creating and configuring private networks, making it accessible to users without extensive networking or programming experience. The objective of this demonstration is to highlight the potential of combining local LLMs and RAG to deliver secure, efficient, and adaptable 5G network solutions, paving the way for a future where 5G networks are both privacy-conscious and versatile across diverse user profiles.", "AI": {"tldr": "A locally deployed, quantized LLaMA-3 (8B Q-4B) combined with retrieval-augmented generation (RAG) is used to automate 5G network configuration on edge devices to preserve privacy. RAG supplies factual context to compensate for the smaller model, enabling natural-language-driven, accurate, and private network provisioning.", "motivation": "Protect sensitive 5G network data (avoid external APIs), reduce dependency on cloud models, cut latency and cost, and make 5G network setup/configuration accessible to non-expert users by providing natural-language interfaces.", "method": "Run a quantized LLaMA-3 8B model locally/at-edge. Maintain a local knowledge base of 5G configuration documents, policies, templates and telemetry. Use an embedding-based retriever and vector store to surface relevant documents (RAG). Combine retrieved context with prompts to the local LLM to generate configurations and operational directives. Optionally validate outputs locally before applying them.", "result": "Demonstration shows improved configuration quality and usability versus the bare small model; keeps sensitive data on-premises; enables quicker iterative configuration by non-experts. Trade-offs include lower raw model capability versus larger cloud models and challenges around hallucination and validation.", "conclusion": "Local LLM + RAG is a promising, privacy-preserving approach for automating 5G network tasks. It is practical for edge deployment but requires careful retrieval design, validation layers, and evaluation to be production-ready."}}
{"id": "2511.20834", "categories": ["cs.DC", "cs.AR", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.20834", "abs": "https://arxiv.org/abs/2511.20834", "authors": ["Dionysios Adamopoulos", "Anastasia Poulopoulou", "Georgios Goumas", "Christina Giannoula"], "title": "Accelerating Sparse Convolutions in Voxel-Based Point Cloud Networks", "comment": null, "summary": "Sparse Convolution (SpC) powers 3D point cloud networks widely used in autonomous driving and AR/VR. SpC builds a kernel map that stores mappings between input voxel coordinates, output coordinates, and weight offsets, then uses this map to compute feature vectors for output coordinates. Our work identifies three key properties of voxel coordinates: they are integer-valued, bounded within a limited spatial range, and geometrically continuous-neighboring voxels on the same object surface are highly likely to exist at small spatial offsets from each other. Prior SpC engines do not fully exploit these properties and suffer from high pre-processing and post-processing overheads during kernel map construction. To address this, we design Spira, the first voxel-property-aware SpC engine for GPUs. Spira proposes: (i) a high-performance one-shot search algorithm that builds the kernel map with no preprocessing and high memory locality, (ii) an effective packed-native processing scheme that accesses packed voxel coordinates at low cost, (iii) a flexible dual-dataflow execution mechanism that efficiently computes output feature vectors by adapting to layer characteristics, and (iv) a network-wide parallelization strategy that builds kernel maps for all SpC layers concurrently at network start. Our evaluation shows that Spira significantly outperforms prior SpC engines by 1.71x on average and up to 2.31x for end-to-end inference, and by 2.13x on average and up to 3.32x for layer-wise execution across diverse layer configurations.", "AI": {"tldr": "Spira is a GPU-focused sparse convolution engine that exploits three voxel properties (integer coordinates, bounded spatial range, and geometric continuity) to build kernel maps and compute output features more efficiently. It introduces a one-shot search, packed-native processing, dual-dataflow execution, and network-wide parallelization to reduce preprocessing/postprocessing overheads, achieving 1.71x average and up to 2.31x end-to-end speedups (and up to 3.32x layer-wise) over prior SpC engines.", "motivation": "Existing sparse convolution engines do not fully exploit key properties of voxel coordinates, leading to high overhead when constructing kernel maps and computing features for 3D point cloud processing. This limits runtime efficiency on GPUs for applications like autonomous driving and AR/VR.", "method": "Spira implements four main techniques: (i) a high-performance one-shot search algorithm that builds kernel maps without preprocessing and with high memory locality; (ii) packed-native processing to access packed voxel coordinates cheaply; (iii) a dual-dataflow execution that adapts computation to layer characteristics; and (iv) network-wide parallelization to build kernel maps for all sparse convolution layers concurrently at network start.", "result": "On diverse layer configurations, Spira outperforms prior sparse convolution engines by 1.71x on average and up to 2.31x for end-to-end inference. For layer-wise execution it achieves 2.13x average speedup and up to 3.32x for some layers.", "conclusion": "By leveraging voxel integerness, boundedness, and geometric continuity, Spira reduces kernel map overheads and improves GPU memory locality and execution flexibility, yielding substantial inference performance gains and making SpC more efficient across networks and layers."}}
{"id": "2511.20975", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.20975", "abs": "https://arxiv.org/abs/2511.20975", "authors": ["Yinwei Dai", "Zhuofu Chen", "Anand Iyer", "Ravi Netravali"], "title": "Aragog: Just-in-Time Model Routing for Scalable Serving of Agentic Workflows", "comment": null, "summary": "Agentic workflows have emerged as a powerful paradigm for solving complex, multi-stage tasks, but serving them at scale is computationally expensive given the many LLM inferences that each request must pass through. Configuration selection, or the cost-aware assignment of workflow agents to specific LLMs, can reduce these costs, but existing approaches bind configuration decisions before request execution, making them ill-suited for the heterogeneous and lengthy execution of workflows. Specifically, system loads can fluctuate rapidly and substantially during a request's lifetime, causing fixed configurations to quickly become suboptimal. We present Aragog, a system that progressively adapts a request's configuration throughout its execution to match runtime dynamics. To make this practical despite the massive space of workflow configurations, Aragog decouples the problem into two core elements -- a one-time routing step that identifies all accuracy-preserving configurations, and a cheap per-stage scheduler that selects among them using up-to-date system observations -- and introduces novel strategies to accelerate each. Across diverse workflows and model families, Aragog increases maximum serving throughput by 50.0--217.0\\% and reduces median latency by 32.5--78.9\\% at peak request rates, while maintaining accuracy comparable to the most expensive configurations.", "AI": {"tldr": "Aragog progressively adapts agentic-workflow LLM configurations during each request\u2019s execution. It separates a one-time routing pass that enumerates accuracy-preserving configurations from a cheap per-stage scheduler that picks among them using live system observations, enabling dynamic, low-overhead reconfiguration. The system improves peak throughput by 50\u2013217% and cuts median latency 32.5\u201378.9% while keeping accuracy comparable to the costliest configurations.", "motivation": "Agentic workflows require many LLM inferences and are costly to serve; fixed, pre-chosen configurations become suboptimal when system load and latency vary during a request\u2019s long, multi-stage execution. A runtime-adaptive, cost-aware configuration mechanism is needed.", "method": "Decouple configuration selection into (1) a one-time routing step that identifies all configurations that preserve accuracy, and (2) a per-stage scheduler that cheaply picks among those configurations based on current system observations. Introduces algorithmic strategies to accelerate routing and scheduling so the approach is practical over the large configuration space.", "result": "Across multiple workflows and model families, Aragog raises maximum serving throughput by 50.0\u2013217.0% and reduces median latency by 32.5\u201378.9% under peak load, without degrading accuracy compared to the most expensive (highest-accuracy) configurations.", "conclusion": "Progressive, per-stage configuration adaptation\u2014implemented via a decoupled routing-and-scheduling approach\u2014effectively matches runtime dynamics and substantially improves efficiency of serving agentic workflows while preserving accuracy."}}
{"id": "2511.20982", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.20982", "abs": "https://arxiv.org/abs/2511.20982", "authors": ["Junhan Liao", "Minxian Xu", "Wanyi Zheng", "Yan Wang", "Kejiang Ye", "Rajkumar Buyya", "Chengzhong Xu"], "title": "A Dynamic PD-Disaggregation Architecture for Maximizing Goodput in LLM Inference Serving", "comment": "14 pages", "summary": "To meet strict Service-Level Objectives (SLOs),contemporary Large Language Models (LLMs) decouple the prefill and decoding stages and place them on separate GPUs to mitigate the distinct bottlenecks inherent to each phase. However, the heterogeneity of LLM workloads causes producerconsumer imbalance between the two instance types in such disaggregated architecture. To address this problem, we propose DOPD (Dynamic Optimal Prefill/Decoding), a dynamic LLM inference system that adjusts instance allocations to achieve an optimal prefill-to-decoding (P/D) ratio based on real-time load monitoring. Combined with an appropriate request-scheduling policy, DOPD effectively resolves imbalances between prefill and decoding instances and mitigates resource allocation mismatches due to mixed-length requests under high concurrency. Experimental evaluations show that, compared with vLLM and DistServe (representative aggregation-based and disaggregationbased approaches), DOPD improves overall system goodput by up to 1.5X, decreases P90 time-to-first-token (TTFT) by up to 67.5%, and decreases P90 time-per-output-token (TPOT) by up to 22.8%. Furthermore, our dynamic P/D adjustment technique performs proactive reconfiguration based on historical load, achieving over 99% SLOs attainment while using less additional resources.", "AI": {"tldr": "This paper proposes DOPD, a dynamic LLM inference system that adaptively adjusts GPU allocations for prefill and decoding stages to maintain an optimal prefill/decoding ratio under mixed-length, heterogeneous workloads. Using real-time load monitoring and request scheduling, DOPD reduces imbalance and improves resource utilization, leading to up to 1.5\u00d7 goodput, 67.5% reduction in P90 TTFT, and 22.8% reduction in P90 TPOT compared to vLLM and DistServe, while achieving >99% SLO attainment with fewer extra resources.", "motivation": "Disaggregated LLM inference (separating prefill and decoding on different GPUs) helps meet SLOs but suffers from producer\u2013consumer imbalance and allocation mismatch due to heterogeneous, mixed-length requests, causing degraded throughput and latency under high concurrency.", "method": "Introduce DOPD: continuously monitor real-time load and historical patterns; dynamically reallocate instance counts between prefill and decoding to target an optimal P/D ratio; combine with a request-scheduling policy that directs requests to appropriately provisioned instances to handle mixed-length workloads and maintain balance.", "result": "DOPD outperforms baselines vLLM (aggregation-based) and DistServe (disaggregation-based) with up to 1.5\u00d7 higher goodput, up to 67.5% lower P90 TTFT, and up to 22.8% lower P90 TPOT. Proactive reconfiguration using historical load achieves >99% SLO attainment with lower incremental resource overhead.", "conclusion": "Dynamic, load-aware adjustment of prefill and decoding resource allocations, together with scheduling, can effectively resolve imbalances in disaggregated LLM inference, improving throughput and latency while meeting SLOs with fewer resources."}}
{"id": "2511.21413", "categories": ["cs.DC", "cs.AI", "cs.DB", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.21413", "abs": "https://arxiv.org/abs/2511.21413", "authors": ["Tim Trappen", "Robert Ke\u00dfler", "Roland Pabel", "Viktor Achter", "Stefan Wesner"], "title": "Automated Dynamic AI Inference Scaling on HPC-Infrastructure: Integrating Kubernetes, Slurm and vLLM", "comment": "6 pages, 3 figures", "summary": "Due to rising demands for Artificial Inteligence (AI) inference, especially in higher education, novel solutions utilising existing infrastructure are emerging. The utilisation of High-Performance Computing (HPC) has become a prevalent approach for the implementation of such solutions. However, the classical operating model of HPC does not adapt well to the requirements of synchronous, user-facing dynamic AI application workloads. In this paper, we propose our solution that serves LLMs by integrating vLLM, Slurm and Kubernetes on the supercomputer \\textit{RAMSES}. The initial benchmark indicates that the proposed architecture scales efficiently for 100, 500 and 1000 concurrent requests, incurring only an overhead of approximately 500 ms in terms of end-to-end latency.", "AI": {"tldr": "Paper proposes integrating vLLM, Slurm and Kubernetes on the RAMSES supercomputer to serve LLMs for synchronous user-facing workloads; initial benchmarks show efficient scaling to 100, 500, 1000 concurrent requests with around 500 ms overhead in end-to-end latency.", "motivation": "HPC traditionally targets batch and long-running workloads; rising demand for interactive AI inference (especially in higher education) requires synchronous, low-latency, user-facing serving solutions that can leverage existing HPC resources without large infrastructure changes.", "method": "Design and implement an architecture that integrates vLLM (for efficient LLM inference), Slurm (HPC scheduler) and Kubernetes (container orchestration) on the RAMSES supercomputer to route and manage concurrent inference requests. Benchmarks simulate 100, 500 and 1000 concurrent requests to measure end-to-end latency and scalability.", "result": "Initial benchmark results indicate the integrated architecture scales to the tested concurrency levels while adding approximately 500 ms overhead to end-to-end latency compared to baseline expectations. The overhead remains similar across tested loads, suggesting acceptable scalability.", "conclusion": "Integrating vLLM, Slurm and Kubernetes on existing HPC systems is a viable approach to serve interactive LLM workloads with reasonable latency overheads, enabling higher education institutions to reuse HPC infrastructure for AI inference."}}
