<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 5]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [CodeCRDT: Observation-Driven Coordination for Multi-Agent LLM Code Generation](https://arxiv.org/abs/2510.18893)
*Sergey Pugachev*

Main category: cs.DC

TL;DR: CodeCRDT replaces explicit message-passing between stochastic LLM agents with observation-driven coordination using CRDTs to enable lock-free, conflict-free concurrent code generation with eventual consistency. Evaluations (600 trials) show mixed performance: some tasks gain up to 21.1% speedup, others suffer up to 39.4% slowdown, but convergence is always achieved with zero merge failures.


<details>
  <summary>Details</summary>
Motivation: Multi-agent LLM systems aim to parallelize work but are bottlenecked by coordination costs and conflicts; the authors seek a coordination pattern that avoids expensive messaging and merge failures while supporting concurrent code generation.

Method: Introduce 'observation-driven coordination' where agents monitor a shared CRDT-based state with observable updates and deterministic convergence. Design CodeCRDT to allow lock-free updates with strong eventual consistency. Formalize the model for stochastic LLM agents and evaluate empirically across 6 tasks with 50 runs per mode (600 trials) comparing parallel coordination modes and measuring speed, quality, conflict rates, and convergence.

Result: Empirical results show 100% convergence with zero merge failures. Performance outcomes vary by task: up to 21.1% speedup on favorable tasks, but up to 39.4% slowdown on others. Semantic conflict rates are reported at 5–10%. The study characterizes when parallel coordination helps vs. hurts based on task structure and highlights quality-performance tradeoffs.

Conclusion: CodeCRDT is a viable coordination pattern that guarantees deterministic convergence and removes merge failures, but its performance benefits are task-dependent. The formalization and empirical characterization clarify when observation-driven parallel coordination is effective and when it introduces overhead or semantic conflicts.

Abstract: Multi-agent LLM systems fail to realize parallel speedups due to costly
coordination. We present CodeCRDT, an observation-driven coordination pattern
where agents coordinate by monitoring a shared state with observable updates
and deterministic convergence, rather than explicit message passing. Using
Conflict-Free Replicated Data Types (CRDTs), CodeCRDT enables lock-free,
conflict-free concurrent code generation with strong eventual consistency.
Evaluation across 600 trials (6 tasks, 50 runs per mode) shows both benefits
and trade-offs: up to 21.1% speedup on some tasks, up to 39.4% slowdown on
others, and 100% convergence with zero merge failures. The study formalizes
observation-driven coordination for stochastic LLM agents, revealing semantic
conflict rates (5-10%) and quality-performance tradeoffs, and provides
empirical characterization of when parallel coordination succeeds versus fails
based on task structure.

</details>


### [2] [AI for Distributed Systems Design: Scalable Cloud Optimization Through Repeated LLMs Sampling And Simulators](https://arxiv.org/abs/2510.18897)
*Jacopo Tagliabue*

Main category: cs.DC

TL;DR: LLMs generate candidate Python scheduler policies which are evaluated deterministically in a domain-specific simulator; an iterative generate-and-verify loop with structured feedback improves throughput on a FaaS runtime (Bauplan/Eudoxia) while preserving interpretability.


<details>
  <summary>Details</summary>
Motivation: Searching the large space of distributed-systems scheduling policies is hard; using LLMs to propose human-readable policies plus deterministic simulator verification aims to accelerate discovery while keeping designs interpretable and testable.

Method: Stochastic code generation from LLMs (Python policies) -> evaluate each candidate in the open-source Eudoxia simulator on standardized traces -> produce structured feedback from simulator to steer further LLM generations. System architecture ties Bauplan runtime, the simulator, and the feedback loop together.

Result: Preliminary experiments show throughput improvements across multiple models. The approach enables targeted search and preserves interpretability, but gains are early-stage and constrained by current setup.

Conclusion: Combining LLM-driven policy proposals with deterministic simulation is promising. Future work should address limits: scaling, robustness, simulator bootstrapping (where AI may help), security/sandboxing of generated code, and broader evaluation.

Abstract: We explore AI-driven distributed-systems policy design by combining
stochastic code generation from large language models (LLMs) with deterministic
verification in a domain-specific simulator. Using a Function-as-a-Service
runtime (Bauplan) and its open-source simulator (Eudoxia) as a case study, we
frame scheduler design as an iterative generate-and-verify loop: an LLM
proposes a Python policy, the simulator evaluates it on standardized traces,
and structured feedback steers subsequent generations. This setup preserves
interpretability while enabling targeted search over a large design space. We
detail the system architecture and report preliminary results on throughput
improvements across multiple models. Beyond early gains, we discuss the limits
of the current setup and outline next steps; in particular, we conjecture that
AI will be crucial for scaling this methodology by helping to bootstrap new
simulators.

</details>


### [3] [RLBoost: Harvesting Preemptible Resources for Cost-Efficient Reinforcement Learning on LLMs](https://arxiv.org/abs/2510.19225)
*Yongji Wu,Xueshen Liu,Haizhong Zheng,Juncheng Gu,Beidi Chen,Z. Morley Mao,Arvind Krishnamurthy,Ion Stoica*

Main category: cs.DC

TL;DR: RLBoost is a hybrid RL training system that harvests preemptible (spot) GPUs for rollout workloads while keeping training on reserved GPUs. It uses adaptive offload, pull-based weight transfer, and token-level response collection to tolerate frequent preemptions, achieving 1.51–1.97× higher throughput and 28–49% cost savings.


<details>
  <summary>Details</summary>
Motivation: Training RL for LLMs mixes two mismatched stages: stateless, parallel rollouts (cheaply run on many transient GPUs) and tightly-coupled training (needs full-mesh, stable GPUs). Existing co-located and naïvely disaggregated approaches either waste resources or underutilize hardware. Preemptible GPUs are abundant but volatile; harnessing them for rollouts could cut costs and speed up training if handled efficiently.

Method: RLBoost uses a hybrid architecture: (1) adaptive rollout offload to shift workload between on-demand and preemptible clusters based on availability; (2) pull-based weight transfer so newly provisioned instances quickly fetch current model weights; (3) token-level response collection and migration to allow partial responses to be captured and migrated when preemptions occur, enabling continuous load balancing and minimal wasted work.

Result: On the reported experiments, RLBoost improved end-to-end training throughput by 1.51×–1.97× and reduced cost per unit of training by 28%–49% compared to using only on-demand GPUs.

Conclusion: By aligning rollout characteristics with preemptible resources and adding mechanisms to handle preemption and weight synchronization, RLBoost enables cost-effective, higher-throughput RL training for LLMs without changing core RL algorithms.

Abstract: Reinforcement learning (RL) has become essential for unlocking advanced
reasoning capabilities in large language models (LLMs). RL workflows involve
interleaving rollout and training stages with fundamentally different resource
requirements. Rollout typically dominates overall execution time, yet scales
efficiently through multiple independent instances. In contrast, training
requires tightly-coupled GPUs with full-mesh communication. Existing RL
frameworks fall into two categories: co-located and disaggregated
architectures. Co-located ones fail to address this resource tension by forcing
both stages to share the same GPUs. Disaggregated architectures, without
modifications of well-established RL algorithms, suffer from resource
under-utilization. Meanwhile, preemptible GPU resources, i.e., spot instances
on public clouds and spare capacity in production clusters, present significant
cost-saving opportunities for accelerating RL workflows, if efficiently
harvested for rollout.
  In this paper, we present RLBoost, a systematic solution for cost-efficient
RL training that harvests preemptible GPU resources. Our key insight is that
rollout's stateless and embarrassingly parallel nature aligns perfectly with
preemptible and often fragmented resources. To efficiently utilize these
resources despite frequent and unpredictable availability changes, RLBoost
adopts a hybrid architecture with three key techniques: (1) adaptive rollout
offload to dynamically adjust workloads on the reserved (on-demand) cluster,
(2) pull-based weight transfer that quickly provisions newly available
instances, and (3) token-level response collection and migration for efficient
preemption handling and continuous load balancing. Extensive experiments show
RLBoost increases training throughput by 1.51x-1.97x while improving cost
efficiency by 28%-49% compared to using only on-demand GPU resources.

</details>


### [4] [FLASH Viterbi: Fast and Adaptive Viterbi Decoding for Modern Data Systems](https://arxiv.org/abs/2510.19301)
*Ziheng Deng,Xue Liu,Jiantong Jiang,Yankai Li,Qingxu Deng,Xiaochun Yang*

Main category: cs.DC

TL;DR: This paper introduces FLASH Viterbi, a memory- and compute-efficient Viterbi decoding family tailored for edge systems. It uses a non-recursive divide-and-conquer algorithm with pruning and parallelization, plus a memory-efficient beam-search variant (FLASH-BS), and FPGA accelerators. The methods claim superior decoding time and lower memory use while remaining adaptive to system constraints; code is released.


<details>
  <summary>Details</summary>
Motivation: Standard Viterbi decoding is memory-intensive and computationally inflexible for edge/resource-constrained platforms. Existing space-saving approaches trade runtime for memory and lack adaptability to diverse deployment constraints. The paper aims to provide a fast, lightweight, adaptive, and hardware-friendly decoder.

Method: FLASH Viterbi: non-recursive divide-and-conquer strategy combined with pruning and parallelization to reduce memory footprint and improve throughput. FLASH-BS: a dynamic beam-search variant using a compact data structure to decouple memory complexity from hidden-state size. Both algorithms expose tunable internal parameters for adaptivity. FPGA-based hardware accelerators are implemented for practical deployment.

Result: Extensive experiments (details not in abstract) reportedly show consistent improvements over baselines in decoding time and memory efficiency while preserving adaptability and hardware friendliness. FPGA implementations demonstrate high throughput and low resource usage. Code is publicly available.

Conclusion: FLASH Viterbi and FLASH-BS provide practical, adaptive Viterbi decoders for edge deployment, offering favorable trade-offs among time, space, and hardware resource use; the work supports reproducibility via released code.

Abstract: The Viterbi algorithm is a key operator for structured sequence inference in
modern data systems, with applications in trajectory analysis, online
recommendation, and speech recognition. As these workloads increasingly migrate
to resource-constrained edge platforms, standard Viterbi decoding remains
memory-intensive and computationally inflexible. Existing methods typically
trade decoding time for space efficiency, but often incur significant runtime
overhead and lack adaptability to various system constraints. This paper
presents FLASH Viterbi, a Fast, Lightweight, Adaptive, and Hardware-Friendly
Viterbi decoding operator that enhances adaptability and resource efficiency.
FLASH Viterbi combines a non-recursive divide-and-conquer strategy with pruning
and parallelization techniques to enhance both time and memory efficiency,
making it well-suited for resource-constrained data systems.To further decouple
space complexity from the hidden state space size, we present FLASH-BS Viterbi,
a dynamic beam search variant built on a memory-efficient data structure. Both
proposed algorithms exhibit strong adaptivity to diverse deployment scenarios
by dynamically tuning internal parameters.To ensure practical deployment on
edge devices, we also develop FPGA-based hardware accelerators for both
algorithms, demonstrating high throughput and low resource usage. Extensive
experiments show that our algorithms consistently outperform existing baselines
in both decoding time and memory efficiency, while preserving adaptability and
hardware-friendly characteristics essential for modern data systems. All codes
are publicly available at https://github.com/Dzh-16/FLASH-Viterbi.

</details>


### [5] [Serverless GPU Architecture for Enterprise HR Analytics: A Production-Scale BDaaS Implementation](https://arxiv.org/abs/2510.19689)
*Guilin Zhang,Wulan Guo,Ziqi Tan,Srinivas Vippagunta,Suchitra Raman,Shreeshankar Chatterjee,Ju Lin,Shang Liu,Mary Schladenhauffen,Jeffrey Luo,Hailong Jiang*

Main category: cs.DC

TL;DR: Paper proposes a production-oriented BDaaS blueprint combining a single-node serverless GPU runtime with TabNet to deliver low-latency, cost-efficient, and interpretable tabular ML for regulated settings. Benchmarks on HR, Adult, and BLS datasets show large improvements over Spark and CPU baselines (up to 4.5x throughput, 98x lower latency, 90% lower cost per 1K inferences), and compliance/interpretability additions add minimal overhead (~5.7 ms, p99 < 22 ms). The work ships a reproducible Helm package and a decision framework for deployment.


<details>
  <summary>Details</summary>
Motivation: Organizations with regulatory requirements need analytics that balance timeliness, cost, and auditability. Traditional distributed frameworks (e.g., Spark/Flink) introduce coordination and auditing overheads that are poorly matched to moderate-scale, latency-sensitive inference; meanwhile serverless GPUs and interpretable tabular models (TabNet) create an opportunity for a new deployment blueprint tailored to regulated environments.

Method: Design a BDaaS blueprint that integrates a single-node serverless GPU runtime with TabNet. Leverage GPU acceleration for throughput, serverless elasticity for cost savings, and TabNet's feature-mask interpretability to meet IL4/FIPS-style compliance requirements. Implement a Helm-packaged, reproducible runtime and run benchmarks comparing GPU pipeline against Spark and CPU baselines on HR, Adult, and BLS tabular datasets. Measure throughput, latency (including p99), cost per 1K inferences, and interpretability stability under load.

Result: GPU pipelines outperform Spark/CPU baselines: up to 4.5x higher throughput, 98x lower latency, and ~90% lower cost per 1K inferences. Compliance mechanisms introduce only ~5.7 ms extra latency with p99 < 22 ms. Interpretability (feature masks) remains stable even at peak load, supporting auditability claims.

Conclusion: The paper demonstrates a practical, compliance-aware serverless GPU approach for secure, interpretable, and cost-efficient tabular analytics in enterprise/government settings, and provides a benchmark, Helm-packaged blueprint, and decision framework to aid adoption.

Abstract: Industrial and government organizations increasingly depend on data-driven
analytics for workforce, finance, and regulated decision processes, where
timeliness, cost efficiency, and compliance are critical. Distributed
frameworks such as Spark and Flink remain effective for massive-scale batch or
streaming analytics but introduce coordination complexity and auditing
overheads that misalign with moderate-scale, latency-sensitive inference.
Meanwhile, cloud providers now offer serverless GPUs, and models such as TabNet
enable interpretable tabular ML, motivating new deployment blueprints for
regulated environments. In this paper, we present a production-oriented Big
Data as a Service (BDaaS) blueprint that integrates a single-node serverless
GPU runtime with TabNet. The design leverages GPU acceleration for throughput,
serverless elasticity for cost reduction, and feature-mask interpretability for
IL4/FIPS compliance. We conduct benchmarks on the HR, Adult, and BLS datasets,
comparing our approach against Spark and CPU baselines. Our results show that
GPU pipelines achieve up to 4.5x higher throughput, 98x lower latency, and 90%
lower cost per 1K inferences compared to Spark baselines, while compliance
mechanisms add only ~5.7 ms latency with p99 < 22 ms. Interpretability remains
stable under peak load, ensuring reliable auditability. Taken together, these
findings provide a compliance-aware benchmark, a reproducible Helm-packaged
blueprint, and a decision framework that demonstrate the practicality of
secure, interpretable, and cost-efficient serverless GPU analytics for
regulated enterprise and government settings.

</details>
